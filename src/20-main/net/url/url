#!/usr/bin/env zsh
# --------------------( LICENSE                            )--------------------
# Copyright 2007-2013 by Cecil Curry.
# See "COPYING" for additional details.

declare_parcel_as_script_with_stdin <<'/---'
Handle *URLs* (i.e., Uniform Resource Locators).
/---

# ....................{ CANONICALIZERS                     }....................
#FIXME: This is also implementable via 'curl --head --show-error --silent "${url}"'. Do so.
#FIXME: Documentation could certainly use sprucing up. Add a proper example.

declare_function_with_stdin <<'/---'
void :canonicalize_url(string string_name)

Canonicalize the value of the passed string variable as a URL. If such URL
points to an HTTP redirection rather than a page, such canonicalization
attempts to iteratively resolve such redirection until the resulting URL either
resolves to a page or exceeds a sensible timeout for doing so. In the latter
case, an exception is thrown: e.g.,

.:canonicalize_url()
==========================================
[source]
------------------------------------------
# If http://dieoff.com redirects to http://holoceneextinction.com
# redirects to http://raiazome.com, then...
>>> :string url='http://dieoff.com'
>>> :canonicalize_url url
>>> output_string "${url}"
http://raiazome.com
------------------------------------------
==========================================
/---
# If:
#
# * "curl" is pathable, prefer such command, which (unsurprisingly) provides a
#   considerably simpler method of canonicalizing URLs than "wget".
# * "wget" is pathable, fallback to such command.
:define_func_if_pathable_or_noop :canonicalize_url\
   'die_unless_arg "Expected one URL name."
    :string url_name__cu="${1}"
    die_unless_string "${url_name__cu}"' ''\
   'curl'\
       ':set_string_to_string "${url_name__cu}" "$(command curl\
            --head\
            --location\
            --show-error\
            --silent\
            --output /dev/null\
            --write-out ''%{url_effective}''\
            "${(P)url_name__cu}")"'\
    'wget'\
        ':string page_headers__cu

        # Capture all pager headers sent by the webservers serving such URL and
        # all . Dismantled, this is:
        #
        # * "--max-redirect=...", resolving nearly arbitrarily many redirections
        #   (rather than failing at the 21st redirection).
        # * "--output-document=-", outputting to standard output (to be captured
        #   by such process substitution).
        # * "--spider", retrieving only page headers (rather than page content).
        # * "--tries=1", failing if the first attempt fails (rather than
        #   retrying 20 times).
        # * "to_stdout:", redirecting standard error to output. Nonsensically,
        #   "wget" always prints such headers to standard error -- and provides
        #   no options for modifying such behavior. *sigh*
        page_headers__cu="$({
            command wget\
                --max-redirect=999999\
                --output-document=-\
                --spider\
                --tries=1\
                "${(P)url_name__cu}"
        } to_stdout:)"

        #FIXME: Actually implement
        #:set_string_to_string_line_last_matching_pcre_group_if_found().
        #Wonder how we''ll do that, incidentally? Two methods, as we see it:
        #
        #1. Split such string on newlines into a list and search such list from
        #   the *END* for the first matching line.
        #2. Iteratively apply such PCRE to such list from the *BEGINNING*. The
        #   last captured group if any is the desired group.
        #
        #O.K.; clearly, on arbitrarily large strings, the former solution is
        #(probably) the most efficient. Although the storage costs are
        #significantly higher, we don''t expect to ever apply such function to
        #grossly corpulescent strings. In such case, iteratively searching from
        #the beginning of a large string merely to ignore most such matches
        #seems particularly inefficient. Besides, the former solution is
        #modestly clever... modestly.

        # If such URL redirects to another URL, set such string to the last URL
        # such URL transitively redirects to; else, leave such string unaltered.
        :set_string_to_string_line_last_matching_pcre_group_if_found\
            "${url_name__cu}" "${page_headers__cu}" ''^Location: ([^ ]++) '' or
            ignore_failure'

# --------------------( WASTELAND                          )--------------------
        #     :set_string_to_string\
        #         "${url_name__cu}" "${url_resolved__cu}"
        # # Else, leave such URL unmodified.
        # }'

# if { is_pathable curl } {
#     function canonicalize_url:() {
#         # Validate sanity.
#         die_unless_arg 'Expected one URL.'
#         string url="${1}"
# 
#         # Canonicalize such URL. Happily, this is quite simple.
#         command curl\
#             --head\
#             --location\
#             --show-error\
#             --silent\
#             --output /dev/null\
#             --write-out '%{url_effective}'\
#             "${url}"
#     }
# # If "wget" is installed, fallback to such command.
# } elif { is_pathable wget } {
#     function canonicalize_url:() {
#         # Validate sanity.
#         die_unless_arg 'Expected one URL.'
#         string url="${1}" page_headers
# 
#         # Capture all pager headers sent by the webservers serving such URL and
#         # all . Dismantled, this is:
#         #
#         # * "--max-redirect=...", resolving nearly arbitrarily many redirections
#         #   (rather than failing at the 21st redirection).
#         # * "--output-document=-", outputting to standard output (to be captured
#         #   by such process substitution).
#         # * "--spider", retrieving only page headers (rather than page content).
#         # * "--tries=1", failing if the first attempt fails (rather than
#         #   retrying 20 times).
#         # * "to_stdout:", redirecting standard error to output. Nonsensically,
#         #   "wget" always prints such headers to standard error -- and provides
#         #   no options for modifying such behavior. *sigh*
#         page_headers="$({
#             command wget\
#                 --max-redirect=999999\
#                 --output-document=-\
#                 --spider\
#                 --tries=1\
#                 "${url}"
#         } to_stdout:)"
# 
#         #FIXME: Actually implement
#         #:set_string_to_string_last_matching_pcre_multiline_group_if_found:().
#         #Wonder how we'll do that, incidentally? Two methods, as we see it:
#         #
#         #1. Split such string on newlines into a list and search such list from
#         #   the *END* for the first matching line.
#         #2. Iteratively apply such PCRE to such list from the *BEGINNING*. The
#         #   last captured group if any is the desired group.
#         #
#         #O.K.; clearly, on arbitrarily large strings, the former solution is
#         #(probably) the most efficient. Although the storage costs are
#         #significantly higher, we don't expect to ever apply such function to
#         #grossly corpulescent strings. In such case, iteratively searching from
#         #the beginning of a large string merely to ignore most such matches
#         #seems particularly inefficient. Besides, the former solution is
#         #modestly clever... modestly.
# 
#         # If such URL redirects to another URL, get the last URL such URL
#         # transitively redirects to.
#         if { :set_string_to_string_last_matching_pcre_multiline_group_if_found:\
#             url_resolved "${page_headers}" '^Location: ([^ ]++) ' } {
#             output_string "${url_resolved}"
#         # Else, get such URL as is.
#         } else {
#             output_string "${url}"
#         }
#     }
# }

#FUXME: After doing so, rename to canonicalize_url() and shift to a more
#general-purpose parcel -- say, "always/net/url".

        #FUXME: We need to match in a line-oriented manner and hence use PCREs,
        #instead. In particular, match as follows:
        #   if { :set_string_to_string_text_matching_glob_group_if_found:\
        #       url_resolved "${page_headers}" '^Location: ([^ ]++) ' } {
        #FUXME: Note that "curl" may or may not require the slightly different PCRE:
        #'^Location: (.*)$', as reported by stackexchange. Honestly, one or the
        #other is probably wrong, so verify! Sample URL for verification purposes:
        #
        #* http://raspberrypi.stackexchange.com/a/1521/86 should resolve to
        #  http://raspberrypi.stackexchange.com/questions/1508/how-do-i-access-the-distributions-name-on-the-command-line/1521#1521.

# If "wget" is installed, prefer such command. While "curl" resolves only the
# first such redirection, "wget" recursively resolves all possible redirections.

#Handle `wget`, a CLI utility for mirroring remote websites to local directories.
#. For
#example, if http://dieoff.com redirected to http://holoceneextinction.com
#redirected to http://raiazome.com, then this function prints the final URL,
#"http://raiazome.com".

#Transitively resolve the passed URL of all intermediary redirects

    # Avoid running under "try" as that currently fails to interpret shell
    # sensitive characters properly (e.g., "&"), and redirect stderr to stdout
    #FUXME: This is broken. What did we replace match_multiline_first_group()
    #with, again? Given the simplicity of the PCRE, shift to using a glob instead.
