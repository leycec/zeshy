#!/usr/bin/env zeshy
# --------------------( LICENSE                            )--------------------
# Copyright 2007-2014 by Cecil Curry.
# See "LICENSE" for additional details.

#FIXME: "-n ${ZPCRE_OP[(w)2]}" isn't quite right. We absolutely need to
#record byte indices for macros expanded within block arguments passed to
#top-level macros: e.g.,
#
#    :func :void :outer() {
#        :func :void :inner() {
#            print "Me! It's me!"
#        }
#    }
#
#While we *COULD* force each and every macro to explicitly call this
#function with each and every passed block argument, that seems exceedingly
#ungainly. Instead, since macros are line-bounded, simply start the next
#match at any byte index inclusively between the first and last characters
#of such line. For efficiency, we'd might as well choose the byte index of
#the last character of such macro's name, which we have access to below.
#FIXME: O.K.! The above change is both simple and correct. Unfortunately,
#we need to do more. Alot more. The reason why, of course, is that
#expanding an inner macro invalidates the last (but not first) byte indices
#of all outer macros containing such inner macro. Consider the following
#fairly minimal example:
#
#    :func :void :outer1() {
#         :func :void :inner1() {
#         }
#    }
#
#    :func :void :outer2() {
#         :func :void :mid1() {
#             :func :void :inner2() {
#             }
#         } {
#             :func :void :inner3() {
#             }
#         } <<'/---
#    /---
#         :func :void :mid2() {
#             :func :void :inner4() {
#             }
#         } {
#             :func :void :inner5() {
#             }
#         } <<'/---
#    /---
#    } <<'/---
#    /---
#
#In the third prepreprocessor phase, the order of macro expansion will be
#as follows:
#
#1. :inner2(), invalidating the last byte indices (of both the macro and
#   block arguments) of :mid() and :outer(). For each such macro, note that
#   the last byte indices of such macro and their block arguments differ.
#2. :inner1(), invalidating the same byte indices.
#3. :mid().
#4. :outer(), invalidating the last byte indices (of both the macro and
#   block arguments) of :outer().
#
#Solving such issue (hopefully) only requires changes to the third
#preprocessor phase. To generalize our way into a solution, let's begin at
#the simplest possible example:
#
#    :func :void :outer() {
#        :func :void :inner() {
#            print "Me! It's me!"
#        }
#    } <<'/---
#    /---
#
#If all macros followed such form, the following would suffice:
#
#* Declare two new integer locals:
#  * ${macro_byte_prior_last}, the byte index of the previously expanded
#    macro.
#  * ${macro_byte_offset}, the number of bytes by which to offset all
#    lexically subsequent byte indices of outer macros.
#* After replacing *ANY* macro, unconditionally:
#  * Set ${macro_byte_prior_last} to the original last byte index of the
#    current macro.
#  * Increment ${macro_byte_offset} by the difference of such macro's prior
#    length (as provided by its recorded first and last byte indices) and
#    replacement length.
#* Before converting *ANY* block argument into a string:
#  * If such argument's first byte index is strictly greater than
#    ${macro_byte_prior_last}, increment both such argument's first *AND*
#    last byte indices by ${macro_byte_offset}.
#  * Else if such argument's last byte index is strictly greater than
#    ${macro_byte_prior_last}, increment only such argument's last byte
#    index by ${macro_byte_offset}.
#* After converting all block arguments for the current macro:
#  * If such macro's last byte index is strictly greater than
#    ${macro_byte_prior_last}, increment only such macro's last byte index
#    by ${macro_byte_offset}.
#
#Unfortunately, a quandry remains. (We knew the above was simply too
#simple!) We need to discover when the current macro cannot have embedded
#the prior macro, as, in such case, ${macro_byte_offset} must be reset to
#0. We cannot simply leave such integer as is, as subsequent incrementation
#of such offset must start from 0.
#
#O.K.; so, we probably need two LIFO integer stacks. How about this:
#
#* Declare new integer locals:
#  * ${macro_byte_stack_index}, the current index into both of the two
#    stacks below. Initially set to 1, implying the default index.
#* Declare new list locals:
#  * ${macro_byte_stack_prior_last}, listing the last byte indices of all
#    (?) previously expanded macros.
#  * ${macro_byte_stack_offset}, listing the number of bytes by which to
#    offset all lexically subsequent byte indices of outer macros (defined
#    as macros with byte indices exceeding the corresponding list item of
#    ${macro_byte_stack_prior_last}).
#
#O.K.; it's clear that, while we can extend the current approach to get
#this working, the interleaving of inner macros with multiple block
#arguments passed to an outer macro complicates things. Fortunately, there
#appears to be a simple solution: refactor everything. Just kidding! Well,
#only sort of. We *WILL* need to refactor the current approach we take to
#constructing the macro function call. Rather than appending the arguments
#to such function to a string local, we *REALLY* instead want to convert
#block arguments into single-quoted strings *IN-PLACE* in the code to be
#preprocessed -- in effect, using such code as a temporary storage buffer.
#What's sweet about this approach is that the macro function to be called
#needn't be programmatically constructed! After converting all block
#arguments for such function (and adjusting byte indices accordingly, of
#course), we simply evaluate the substring of such code identified by such
#delimiting byte indices.
#
#That said, is "adjusting byte indices accordingly" after converting a
#block argument as simple as we make it out to be? Probably, but consider.
#
#In any case, this approach requires that we *INTERLEAVE* byte indices for
#both entire macros and block arguments into a single list. For obvious
#reasons, byte indices for all block arguments passed to a macro will need
#to appear *BEFORE* byte indices for such macro in such list. To
#differentiate between byte indices for macros and byte indices for block
#arguments in such list, we'll need to suffix the former (of which there
#will presumably be fewer) with an identifying character -- say, ".".
#
#So, what does this give us? Well, since we've interleaved the two, we no
#probably longer need the excrutiatingly convoluted map-based data
#structures we were contemplating. Minor relief, anyway. We *WILL* still
#need the list-based stacks we were contemplating above, but such is life.
#
#Wait. We'll need to make additional changes to the second preprocessor
#phase to ensure that the byte indices for inner macros are interleaved
#with those of outer block arguments in a strictly increasing sorted
#manner. Ignoring "." suffixes (treat such strings as floats, perhaps?),
#the easiest means of achieving such ordering might be to simply sort such
#list afterwords with a zsh builtin. Although, that strikes us as fairly
#hacky. Or perhaps not? Yeah, probably not. After all, how the heck could
#we manually ensure such sorting during PCRE-based iteration?
#
#Additionally, one nice aspect of sorting after-the-fact is that we can
#retain all of our existing data structures and logic for the first two
#phases (well, don't forget to suffix macro indices by "."!) and simply
#define a new list local ${macro_bytes} into which we sort the contents of
#the existing two lists of byte indices. (Alternately, we could tryp to
#keep a second list of byte indices mostly-sorted by ...
#
#Wait. Conventional sorting doesn't work, as we need to group last and
#first byte indices in a balanced manner. That said, we *COULD* split last
#and first byte indices into two discrete lists and then sort *THOSE*
#lists. We think, anyway. Contemplate further.
#
#Right. We should note that the two lists will be ordered in opposite
#directions. Rather than explicitly reverse one of them, however, simply
#index one with the negation of the indices used to index the other. Done!
#
#Actually, not done. We can't both sorting such lists and expect them to
#remain in sync. Clearly, only the following alternatives then apply:
#
#* Manually sort inner macro indices into outer macro and block argument
#  indices. This would certainly work, but clearly incurs non-negligible
#  time costs -- particularly as we begin employing more macros.
#* Sort all indices after the fact by combining each pair of first and last
#  indices into a single integer using fixed-point math. Somewhat
#  astonishingly, zsh supports the full gamut of bit operations in
#  arithmetic expansions. Moreover, there appears to be no meaningful upper
#  limit on integer values. Sadly, we can't recall if there's a means of
#  determining the maximum integer value programmatically. On my machine,
#  here's what we find:
#
#  >>> :int j; (( j = (10*(10*10000000000000000)) )); print $(( [#10] j ))
#  1000000000000000000
#  >>> :int j; (( j = (10*(10*100000000000000000)) )); print $(( [#10] j ))
#  -8446744073709551616
#
#We've clearly bisected the limit. This implies, however, that we can
#divide the range. Ah; right. So, we're dealing with 64-bit integers (i.e.,
#longs): e.g.,
#
#  >>> :int j; (( j = 1 << 63 )); print $(( [#16] j ))
#  -0x8000000000000000
#  >>> :int j; (( j = 1 << 64 )); print $(( [#16] j ))
#  0x1
#
#This implies that we should reserve the upper 32 bits for the first byte
#index and the lower 32 bits for the last byte index, further implying that
#the maximum line number the zeshy preprocessor can support is:
#
#    4,294,967,295
#
#Clearly, such maximum will effectively guaranteeably *NEVER* be reached,
#so we're actually fine here. (Still, that should be checked.)
#
#Oh, bother. We forgot about the terminating constant. (Ugh.) We'd somehow
#need to ignore such constants for the purposes of sorting -- which isn't
#going to happen, clearly. Alternatively, rather than embedding terminating
#constants into such list, we *COULD* record the byte index pair of the
#last block argument for each macro in a new map local for efficient
#subsequent lookup. That should incur approximately the same costs as the
#terminating integer approach, but allow us to sort the byte indices list.
#
#Oh, wait; that's silly. Simply embed the terminating constant as a boolean
#bit in *EVERY* such integer.
#
#Oh, wait; under the new conception of aggregating macros and block
#arguments together, we don't care *AT ALL* whether an argument is the last
#or not. So, this whole issue goes up in fruitless smoke. Yay!
#
#Right. After contemplation, it's clear that we no longer care *AT ALL*
#where the last block argument is. Instead, what we care about is
#differentiating byte indice pairs for macros from block arguments. We
#can't simply used floating point numbers for one, as floating point has
#"holes" at larger integer ranges and hence behaves only as an
#approximation. Instead, what we need to do is divide the 64-bit integer
#space into an absurdly compact bit representation. While doing so, we must
#take care to ensure that the resulting structure will remain sortable -- a
#delicate balancing act. Can we do it? We believe we can. Consider:
#
#            First         Last        Boolean bits:
#            byte index    byte index  01 if macro; 00 if block arg
# 64 bit    /-----^-----\/-----^-----\/---^----\
# integer =  High 31 bits Mid 31 bits Low 2 bits
#
#For sorting purposes, the first byte index must be allocated the high
#bits. For orthogonality, the last byte index is allocated the middle
#bits. The boolean bits are allocated all remaining space (despite
#technically only requiring one bit of storage).
#
#Wait! Do we actually need boolean bits? Technically, no. Since the first
#and last bytes of *ALL* block arguments are guaranteed to be "{" and "}",
#testing such bytes suffices to distinguish macros from block arguments.
#For simplicity, our first implementation should probably do exactly that.
#Forego the boolean bits, for now! Going forward, however, it might be
#prudent to optimize such testing with boolean bits. Contemplate later.
#
#Ah. Actually, we believe we *DO* need boolean bits -- but for hard
#algorithmic purposes rather than merely optimization. In the algorithm
#below, integer ${macro_byte_prior} records not merely the byte index pair
#of the prior object but such object's *TYPE*, which must be conditionally
#checked at certain points. So, there we have it: boolean bits to go.
#
#Ugh. Actually, we do appear to need at least three boolean values,
#justifying the above allocation of two bits for such values. What's the
#third such value? Here's a full list of all currently required bits:
#
#* "00", signifying a macro. The upper bits signify the byte index pair.
#* "01", signifying a block argument. The upper bits signify the byte
#  index pair.
#* "10", signifying a terminating block argument. The upper bits signify
#  only the byte index of such terminating block argument -- in whichever
#  of the upper or middle 31 bits is more efficient to access. Oh! Right.
#  For sorting purposes, we *MUST* use the upper 31 bits for this. 
#
#Yay! We no longer require boolean bits, as we've excised ${macro_byte_prior}
#entirely. Given that, it's (more or less) trivial to simply test string bytes
#instead to determine object types.
#
#Yes, we definitively need to record the positions of block arguments as a
#"{"- and "}"-delimited conglomerate pair *AND* of only the latter such
#delimiter. We require the former for expansion purposes, as we need to
#know what to convert into a string. We require the latter for detection
#purposes, because we otherwise have no means of detecting the depth to
#which such objects are nested within other objects.
#
#Naturally, this implies that the second parse phase must now also record
#the byte indices of individual terminating "}" delimiters. Simple.
#
#O.K.; integer precision depends on the machine but should always either
#be 64 or 32 bits. As documented under "man zshmisc":
#
#"For integers, the shell is usually compiled to use 8-byte precision
# where this is available, otherwise precision is 4 bytes. This can be
# tested, for example, by giving the command
# `print - $(( 12345678901 ))'; if the number appears unchanged, the
# precision is at least 8 bytes."
#
#This can thus be tested as follows:
#
#    #FIXME: Define new function :int.set_system_bit_width() in the main
#    #codebase implementing such logic. (Yay!)
#    integer int_is_64_bit=$(( (1 << 31) > 0 ))
#
#Now, finally, the generalized solution to be applied in the third phase:
#
#* Declare new integer locals:
#  * ${macro_byte_offset_stack_index}, the current index into the stack
#    below. Initially set to 1, implying the default index. Actually, just
#    name such integer ${macro_depth} -- as that's exactly what it is.
#* Declare new list locals:
#  * ${macro_byte_offset_stack}, listing the number of bytes by which to
#    offset all lexically subsequent byte indices of outer macros (defined
#    as macros with last byte indices exceeding that of
#    ${macro_byte_prior}). Actually, perhaps name such list
#    ${macro_depth_to_byte_offset}. It's *NOT* a stack under current usage.
#    It does exactly what the new name reflects: it maps from deptch
#    indices to byte offsets. Simple.
#* On iterating to the next object:
#  * If such object is a terminating "}" delimiter, then:
#    * Increment ${macro_byte_offset_stack_index} by 1. (This is basically
#      the only purpose of recording such delimiters.)
#    * Immediately continue to the next object. Do *NOT* set
#      ${macro_byte_prior} to such object's byte index pair, please.
#  * Else if such object is a block argument, then (in order):
#    . Set ${macro_byte_offset_stack[
#          $(( macro_byte_offset_stack_index - 1 ))]} to
#          ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}.
#    . Set ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]} to
#      0.
#    . Increment ${macro_byte_offset_stack_index} by -1. (For very minor
#      efficiency, do this afterwards.)
#    . Continue onward to expansion and glory!
#* To expand either a macro *OR* block argument (in order):
#  . Increment such object's *LAST* byte index by
#    ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}.
#    Its first byte index remains unchanged.
#  . Expand such object in a type-specific manner, using such byte indices.
#  . Increment ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}
#    by the difference of such object's prior length (as provided by its
#    recorded byte index pair) and replacement length.
#
#Hmm. Do we honestly still require ${macro_byte_prior}? That seems
#increasingly superfluous; we only test it in one place above.
#
#O.K.; great! We've excised ${macro_byte_prior} and all (or most, anyway)
#redundancies above, producing what would appear to be a very concise,
#self-consistent algorithm. Let's give her a go, shall we?
#
#That's it. Hurrah, hurrah! The above algorithm appears to smoothly scale to
#*ALL* nesting depths, which is certainly a relief.

#FIXME: The long-term solution to function prototype optimization, though a bit
#balls-crazy, is as follows:
#
#* Merge the principal PCRE (i.e., ${ZESHY_CALLABLE_PROTOTYPE_PCRE}) produced
#  by ={*-soil/*-declare/*-pcre} into the principal PCRE produced by ={pcre}.
#* Refactor ={*-soil/*-declare/*-func/func} accordingly. (Actually, any
#  refactoring should be quite minimal, as we already reference match indices
#  by name rather than hard-coded integer).
#
#What's indolently pleasant about this approach is that it avoids all of the
#repetitous PCRE compilation we currently perform. Consider it: on each and
#every function declaration, we currently recompile
#${ZESHY_CALLABLE_PROTOTYPE_PCRE}! This is absolute insanity. Happily, the
#above approach does away with that entirely. It also pushes the limits of
#PCRE-based parsing, but hopefully in a tractable, feasible way.
#
#Only one way to find out whether "libpcre" will actually support a PCRE that
#monstrously complex *AND* whether we can actually reliably debug that PCRE,
#right? So, let's do this.

# ....................{ GLOBALS ~ int                      }....................
#FIXME: Document me.

# Bit masks for obtaining the first and last byte indices from a single integer
# into which such indices have been bit-packed.
integer -g\
    ZESHY__MACRO_BYTE_INDICES_FIRST_MASK\
    ZESHY__MACRO_BYTE_INDICES_LAST_MASK

# Number of bits by which to shift first byte indices right by to bit-pack such
# indices into a single integer.
integer -g ZESHY__MACRO_BYTE_INDICES_FIRST_SHIFT_COUNT

# ....................{ PREPROCESSORS                      }....................
#FIXME: Arguably shift to a new parcel ={code} devoted to such operation.
#FIXME: Optimize by globalizing locals. (You know the code-cadenced drill.)
#FIXME: Document me. In particular, note that the caller must define string
#local ${ZESHY__CODE_PREPROCESS}.

# :void ::code.preprocess()
function ::code.preprocess() {
    # Validate sanity.
    (( # == 0 )) || :die 'Expected no arguments.'

    # If such variable is undefined or not a string, throw an exception. See
    # :var.die_unless_type_string().
    [[ ${(t)ZESHY__CODE_PREPROCESS-} == 'scalar'('-'*|) ]] || :die\
        'String ${ZESHY__CODE_PREPROCESS} undefined or not a string.'

    # Canonical string global ${ZPCRE_OP}, localized to avoid overwriting
    # either global or caller-specific versions of such variable.
    local ZPCRE_OP

    # Currently matched macro name.
    local macro_name

    # Function call replacing the current macro in the passed code by its
    # expansion. The first shell word of this whitespace-delimited string is
    # the name of such function (and hence such macro); all remaining shell
    # words are the arguments to be passed to such function.
    local ZESHY__MACRO_FUNC_CALL

    # Byte indices of the first and last characters of such macro's name.
    integer macro_byte_name_first macro_byte_name_last

    # Byte index of the last non-whitespace character of the current macro.
    integer macro_byte_last

    # Byte indices of the first and last characters of the currently matched
    # block argument passed to such macro.
    integer macro_byte_arg_block_first macro_byte_arg_block_last

    # Byte index of the last character of the previously matched block argument
    # passed to such macro if any or the first character following the last
    # character of such macro's name otherwise.
    integer macro_byte_arg_block_prior_last

    # List of positive integer triples, each a byte index into the passed code
    # such that:
    #
    # * The first list item of each such triple is the byte index of the first
    #   non-whitespace character of such macro (e.g., the ":" prefixing such
    #   macro's name).
    # * The second list item of each such triple is the byte index of the last
    #   non-whitespace character of such macro name (e.g., the "c" in ":func").
    # * The third list item of each such triple is the byte index of the last
    #   non-whitespace character of such macro (e.g., the "}" suffixing such
    #   macro's last block argument).
    #
    # Such triples are listed in reverse lexical order (i.e., in the reverse
    # order such macros appear in such code). The reason why is fairly subtle,
    # as reasons go. To prevent the expansion of each macro from invalidating
    # byte indices of subsequent macros, the third preprocessor phase must
    # expand and hence iterate macros in reverse order. While there exist
    # various means of ensuring this, persisting such macros in such order by
    # prepending such indices to the head of this list with list operator "+="
    # is arguably the simplest and most efficient. However, such operator
    # requires a valid 1-based list index and hence cannot be used to prepend
    # directly into the first list index (which would require "+=" to support
    # appending to an imaginary list index 0). We circumvent this oddity by
    # retaining the empty string as a "dummy" first list index to which we
    # append all subsequent triples.
    #
    # Such list circumvents zsh's current inability to nest PCRE-based
    # iteration, permitting matches in the first preprocessor phase to be
    # accessed in the second preprocessor phase. (It's all rather droll.)
    local -a macro_bytes_first_name_last
             macro_bytes_first_name_last=( '' )

    # List of ";"-terminated sublists of positive integer pairs, each a byte
    # index into the passed code such that:
    #
    # * The first list item of each such pair is the byte index of the "{"
    #   prefixing the next block argument passed to such macro.
    # * The second list item of each such pair is the byte index of the "}"
    #   suffixing the same block argument.
    #
    # Since zsh does *NOT* currently support type composition, terminate each
    # such sublist by the arbitrary delimiter ";". (This is horrible.)
    #
    # Such list circumvents zsh's current inability to nest PCRE-based
    # iteration, permitting matches in the second preprocessor phase to be
    # accessed in the third preprocessor phase. (Angel of death, take us now.)
    local -a macro_bytes_arg_block_first_last

    # 1-based index into list ${ZESHY__MACRO_DEPTH_TO_BYTE_OFFSET}, signifying
    # the current block argument depth (i.e., number of block arguments in
    # which the currently parsed construct is nested).
    integer ZESHY__MACRO_DEPTH=1

    # Map from block argument depth to parent byte offset (i.e., number of
    # bytes by which the last byte index of constructs containing the current
    # construct should be offset). Since such depth is guaranteed to be
    # 1-based, such map is implemented as a list for efficiency.
    local -a ZESHY__MACRO_DEPTH_TO_BYTE_OFFSET

    # If preprocessor PCREs have yet to be generated or require regeneration
    # (e.g., due to the declaration of new macros), do so.
    ::preprocessor_pcres.make_if_needed

    # Locally disable default zsh option "multibyte", permitting substrings
    # matched by pcre_match() to be indexed by the " "-delimited byte indices
    # written to ${ZPCRE_OP}.
    #
    # Unfortunately, pcre_match() fails to set integer globals ${MBEGIN} and
    # ${MEND} to the character indices of the start and end of the matched
    # substring. Fortunately, it *DOES* set string global ${ZPCRE_OP} to the
    # " "-delimited byte indices of the start and end of such substring.
    # Unfortunately, multibyte strings (e.g., UTF8-encoded) are indexed by
    # character rather than byte indices under default zsh option "multibyte".
    # Fortunately, locally disabling such option permits all strings (multibyte
    # or not) and hence such substring to be indexed by such byte indices.
    setopt -- local_options no_multibyte

    # ..................{ PHASE ~ macro                      }..................
    # Due to zsh inadequacies, partition preprocessing into multiple phases.
    # Since "zsh" currently only permits one PCRE to be compiled at a time,
    # PCRE-based iteration cannot be nested. Since the first two phases perform
    # such iteration, such phases cannot be combined. (Q.E.D., sadly.)

    # Compile and optimize such PCRE. See for_string_text_matching_pcre:().
    pcre_compile -- "${ZESHY_MACRO_PCRE}"
    pcre_study

    # For each macro matched from such code, record matched byte indices for
    # the next preprocessor phase.
    ZPCRE_OP='0 0'
    while {
        pcre_match -b -n ${ZPCRE_OP[(w)2]} -- "${ZESHY__CODE_PREPROCESS}"
    } {
        # Such macro's name.
        macro_name=${match[${ZESHY_MACRO_MATCH_INDEX_NAME}]}

        # If such name does *NOT* correspond to an existing function, throw an
        # exception. (Macros are implemented as functions of the same name.)
        (( ${+functions[${macro_name}]} )) || :die\
            'Macro function '${macro_name}'() undefined.'

        #FIXME: Report this bug! This is currently affecting *EVERYONE*
        #leveraging zsh module "zsh/pcre", which is probably *EVERYONE*.
        # Byte indices recorded for the next preprocessor phase. Note that
        # "man zshmodules" documents ${ZPCRE_OP} to be a " "-delimited integer
        # pair such that:
        #
        # * The first such integer is the byte index of the first character of
        #   the matched substring.
        # * The second such integer is the byte index of the first character 
        #   following such substring.
        #
        # Unfortunately, this is *NOT* the case. The documentation is "off by
        # one" in both cases, a bug in either the current zsh implementation or
        # documentation. Regardless of whether option "-n" is passed to
        # pcre_match():
        #
        # * The first such integer is the byte index of the last character
        #   preceding such substring.
        # * The second such integer is the byte index of the last character of
        #   such substring.
        #
        # However, the subtle entanglements don't end there. The byte index of
        # the first character of such substring is the byte index of the next
        # character following the first such integer. If such character is
        # unibyte (i.e., encoded as a single byte), incrementing the latter by
        # 1 suffices to calculate the former. If such character is multibyte
        # (i.e., encoded as two or more bytes), however, this overly simplistic
        # calculation fails.
        #
        # By PCRE design, the character preceding a matched macro name is
        # guaranteed to be whitespace. Technically, UTF-8-encoded multibyte
        # whitespace characters *DO* exist. Since no official scripts precedes
        # a macro expansion by such characters *AND* since it appears
        # vanishingly unlikely that third-party scripts would ever do so,
        # assume such character to be unibyte for simplicity. In such case,
        # simplistically incrementing the first such integer by 1 suffices.
        macro_byte_name_first=$(( ${ZPCRE_OP[(w)1]} + 1 ))
        macro_byte_last=${ZPCRE_OP[(w)2]}
        macro_byte_name_last=$(( macro_byte_name_first + ${#macro_name} - 1 ))

        #FIXME: Insufficient, as this fails to distinguish syntactically valid
        #macros accepting no arguments from syntactically invalid macros
        #accepting arguments. Can we actually distinguish between the two? The
        #answer is a resounding "yup." By fortuitous design, PCRE actually
        #captures groups matched in positive lookahead! This means that we can
        #amend the current PCRE with positive lookahead immediately following
        #the matched macro name. Such lookahead must be made optional, and
        #should attempt to look ahead for one or more horizontal whitespace
        #characters followed by exactly one non-whitespace character: e.g.,
        #    local pcre_='(?=('${pcre_spaces}'\S)?)'
        #If such captured group is nonempty *AND* the empty group signifying
        #such macro to be syntactically valid was *NOT* matched, then and only
        #then have we positively identified such macro to be invalid. Sweet!
        #FIXME: Shift parcel ={*-get} here and explicitly source above.

        # If the empty group signifying such macro to be syntactically valid
        # was *NOT* matched, such macro is syntactically invalid. In such case,
        # throw an exception.
        (( ${#match} >= ZESHY_MACRO_MATCH_INDEX_IS_VALID )) || {
            # Line number of the current macro in such code, calculated by
            # removing all non-newline characters preceding such macro and
            # counting the number of newline characters that remain.
            integer line_number="${#ZESHY__CODE_PREPROCESS[
                1,${macro_byte_name_first}]//[^$'\n']}"

            # Throw such exception.
            :die $(::parcel.get_current)'macro '${macro_name}'() on line '${line_number}' passed syntactically invalid arguments (e.g., due to unmatched braces or quotes).'
        }

        # Record matched byte indices for the next preprocessor phase.
        macro_bytes_first_name_last[1]+=(
            ${macro_byte_name_first}
            ${macro_byte_name_last}
            ${macro_byte_last}
        )
    }

    # For simplicity, remove the placeholder empty string prefixing such list.
    macro_bytes_first_name_last[1]=()

    # ..................{ PHASE ~ block argument             }..................
    # Compile and optimize such PCRE. See for_string_text_matching_pcre:().
    pcre_compile -- "${ZESHY_MACRO_ARG_BLOCK_PCRE}"
    pcre_study

    # For each previously matched macro, match each block argument passed to
    # such macro.
    for macro_byte_name_first macro_byte_name_last macro_byte_last (
        "${macro_bytes_first_name_last[@]}") {
        # Begin searching for block arguments passed to the current macro
        # immediately after the last character of such macro's name.
        ZPCRE_OP='0 '${macro_byte_name_last}

        # For each block argument matched from such code, record matched byte
        # indices for the next preprocessor phase.
        while {
            # Byte index of the last character of the prior match if any or the
            # first character following the last character of such macro's name
            # otherwise.
            macro_byte_arg_block_prior_last=${ZPCRE_OP[(w)2]}

            # While such index precedes the last byte index of such macro,
            # attempt to match the next block argument passed to such macro.
            (( macro_byte_arg_block_prior_last < macro_byte_last )) &&
                pcre_match -b -n ${macro_byte_arg_block_prior_last} --\
                "${ZESHY__CODE_PREPROCESS}"
        } {
            #FIXME: Actually, macro names can contain UTF-8-encoded characters.
            #We should probably prohibit this, for the moment -- that, or
            #reimplement this in a multibyte-aware manner. *sigh*

            # Record matched byte indices for the next preprocessor phase.
            #
            # The last character preceding such match is either the last
            # character of such macro's name *OR* the "}" delimiting the prior
            # block argument and hence guaranteed to be ASCII. Given that,
            # incrementing the byte index of such character by 1 gives the byte
            # index of the first whitespace character of such match;
            # incrementing such index by the byte length of the possibly empty
            # substring of standard arguments preceding such block arguments
            # gives the byte index of the "{" delimiting such block argument.
            macro_bytes_arg_block_first_last+=(
                $(( ${ZPCRE_OP[(w)1]} + ${match[1]} + 1 ))
                    ${ZPCRE_OP[(w)2]}
            )
        }

        # Append an arbitrary non-positive integer constant terminating the
        # sublist of block argument byte indices for such macro. (Hacks "R"
        # Us.)
        macro_bytes_arg_block_first_last+=0
    }

    # ..................{ PHASE ~ expand                     }..................
    #FIXME: Don't forget to strip suffixing line continuations (i.e.,
    #"\"-prefixed newlines) from macro arguments before calling such macros.
    #Efficiently straightforward using a simple global glob replacement. 
    #FIXME: Actually, we pretty much *HAVE* to call eval() to ensure proper
    #expansion of standard arguments. (Consider process substitutions embedded
    #in such arguments, for example.) Simplifies things, anyway. *shrug*
    #FIXME: Actually, couldn't we apply the presumably less expensive parameter
    #expansion flag "(e)" *AFTER* stripping line continuations and performing
    #shell word splitting with parameter expansion flag "(z)"? Although, at
    #that point, simply calling eval() is probably more efficient.
    #
    #Note that, if we do take the eval() route, we'll need to apply at least
    #parameter expansion flag "(q)" to each block argument. (Probably "(qq)").

    #FIXME: Shift above.
    integer macro_bytes_arg_block_first_last_index
    macro_bytes_arg_block_first_last_index=1

    # For each previously matched macro, replace such macro by its expansion.
    for macro_byte_name_first macro_byte_name_last macro_byte_last (
        "${macro_bytes_first_name_last[@]}") {
        # Function call replacing such macro in such code by its expansion.
        # Since such function's name is such macro's name, begin such call with
        # the latter.
        ZESHY__MACRO_FUNC_CALL=${ZESHY__CODE_PREPROCESS[
            ${macro_byte_name_first},${macro_byte_name_last}]}

        # Set the byte index of the last character of the prior block argument
        # to the byte index of the last character of such macro's name,
        # ensuring standard arguments following such name and preceding the
        # first block argument are passed to such call in the customary way.
        macro_byte_arg_block_prior_last=${macro_byte_name_last}

        # For each previously matched block argument passed to such macro,
        # convert such argument into a standard single-quoted argument.
        # Since a variadic number of block arguments are supported, the prior
        # phase terminates the last block argument passed to such macro with a
        # character constant.
        while {
            # Byte index of the first character of such block argument if any
            # or such character constant otherwise.
            macro_byte_arg_block_first=${macro_bytes_arg_block_first_last[
                ${macro_bytes_arg_block_first_last_index}]}

            # If such index is *NOT* the terminating constant (i.e., zero),
            # proceed.
            (( macro_byte_arg_block_first ))
        } {
            # Pass such call all standard arguments (i.e., non-block arguments)
            # following the prior block argument and preceding the current
            # block argument if any. (If no such arguments exist, this simply
            # appends the empty string and hence reduces to a noop.)
            #
            # By PCRE design, such arguments are guaranteed to be both preceded
            # and followed by non-empty whitespace.
            ZESHY__MACRO_FUNC_CALL+=${ZESHY__CODE_PREPROCESS[
                $(( macro_byte_arg_block_prior_last + 1 )),
                $(( macro_byte_arg_block_first - 1 ))]}

            # Preserve the byte index of the last character of such block
            # argument for subsequent examination *AFTER* expanding the prior
            # such index above.
            macro_byte_arg_block_prior_last=${macro_bytes_arg_block_first_last[
                $(( macro_bytes_arg_block_first_last_index + 1 ))]}

            # Pass such call such block argument converted to a standard
            # argument (i.e., as a single-quoted string, thus implicitly
            # quote-protecting all shell-reserved characters in such block).
            ZESHY__MACRO_FUNC_CALL+=${(qq)ZESHY__CODE_PREPROCESS[
                ${macro_byte_arg_block_first},
                ${macro_byte_arg_block_prior_last}]}
        }

        #FIXME: Append all remaining standard arguments preceding the optional
        #suffixing here-document or -string.
        if [[ ]] {
            ZESHY__MACRO_FUNC_CALL+=
        }
    }
}

# ....................{ MAIN                               }....................
# Define system-specific globals required by the above preprocessor.
() {
    # If the current system supports 64-bit integers, set such globals
    # accordingly. See :int.set_system_bit_width().
    if (( (1 << 31) > 0 )) {
        ZESHY__MACRO_BYTE_INDICES_LAST_MASK=0xFFFFFFFF
        ZESHY__MACRO_BYTE_INDICES_FIRST_SHIFT_COUNT=32
    # Else, zsh guarantees the current system to support 32-bit integers.
    # Again, set such globals accordingly.
    } else {
        ZESHY__MACRO_BYTE_INDICES_LAST_MASK=0xFFFF
        ZESHY__MACRO_BYTE_INDICES_FIRST_SHIFT_COUNT=16
    }

    # Implicitly compute rather than explicitly specify such mask (e.g.,
    # "0xFFFFFFFF00000000"). Unfortunately, zsh truncates explicitly
    # specified 16-digit hexadecimal integers to 15 digits with an error
    # resembling:
    #
    #     >>> ZESHY__MACRO_BYTE_INDICES_FIRST_MASK=0xFFFFFFFF00000000
    #       zsh: number truncated after 15 digits: 0xFFFFFFFF0000000
    #
    # Fortunately, such questionable behaviour is circumventable by
    # implicitly computing such values.
    ZESHY__MACRO_BYTE_INDICES_FIRST_MASK=$((
        ZESHY__MACRO_BYTE_INDICES_LAST_MASK <<\
        ZESHY__MACRO_BYTE_INDICES_FIRST_SHIFT_COUNT ))
}

# --------------------( WASTELANDS                         )--------------------
#    :func :void :outer() {
#    } {
#        :func :void :inner() {
#            print "Me! It's me!"
#        }
#    } <<'/---
#    /---
#
#In this case, such inner macro invalidates only the last byte index of the
#last block argument of the outer macro. As generalization:
#
#* 
#
#Specifically:
#
#* Declare a new list local ${stack} serving as a LIFO stack.

    #* After converting all block arguments for the current macro:
    #  * If such macro's last byte index is strictly greater than
    #    ${macro_byte_prior_last}, increment only such macro's last byte index
    #    by ${macro_byte_offset}.
    #    . Increment such object's *LAST* byte index by
    #      ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}.
    #  . Add ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]} to
    #    such object's *LAST* byte index to obtain its new last byte index.
    #  . Set ${macro_byte_prior} to such object's original byte index pair.
    #  * Else if such object's *LAST* byte index is strictly greater than the
    #    last byte index of ${macro_byte_prior} (which, in consideration,
    #    should imply such object to be a block argument), then (in order):
    #    . Increment such object's *LAST* byte index by
    #      ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}.

    #  * ${macro_byte_prior}, the byte index pair of the most recently expanded
    #    macro preceding the current macro. A stack should *NOT* be required
    #    here, if our meager analysis is correct.

# Before expanding either a macro *OR* block argument (...although, :
    #So, we make tests of whether a byte index pair refers to a block argument
    #or not tests against 0 and hence more efficient than

#    . Prefix *ALL* such contents by the following line:
#      "${general_alias_definitions}
#      "func ::script.code__${random_integer}() {"
#    . Suffix *ALL* such contents by the following lines:
#      "}
#      ::script.code__${random_integer}", where:
#      * ${random_integer} is a random integer guaranteeing the uniqueness of
#        such

#FUXME: Or perhaps not? The prior solution appears to work quite well, now.

 # Since the former is a non-integer
    # character constant on iterating past the last block argument passed to
    # such macro, declare such indices to be strings rather than integers.
    #Solving such issue can be split into two parts:
    #
    #1. Detecting such 

            # macro_byte_arg_block_last=${macro_bytes_arg_block_first_last[$((
            #     macro_bytes_arg_block_first_last_index + 1 ))]}

        # for macro_byte_arg_block_first macro_byte_arg_block_last (
        #     "${macro_bytes_arg_block_first_last[,-1]}") {

        #FUXME: Implement me.

        # If such macro was passed a here-document or -string on standard
        # input, pass such input to such call as a single-quoted here-string.
        #FUXME: Correct me. We probably want to embed new byte indices
        #corresponding to the first and last byte indices of such here string.
        #The latter is simply ${ZPCRE_OP[(w)2]}; the former is the latter minus
        #the byte length of such match.
        #FUXME: Alternatively, perhaps we should simply copy such match into a
        #string list item of the same list. Since empty standard input is
        #indistinguishable from no standard input, perhaps this is the ideal
        #approach. It certainly seems simpler, if slightly less efficient. (Or
        #perhaps not, since we'll be taking a string slice and hence copy
        #anyway later if not here?)
        #FUXME: This necessitates modifying ${macro_byte_last} above to
        #strictly precede such byte indices, implying such integer should be
        #renamed to ${macro_byte_args_last}.

        # If such macro was passed a here-document or -string on standard
        # input, record such string for the third preprocessor phase.
        # if [[ -n ${ZESHY__CODE_PREPROCESS[
        #     ${ZESHY_MACRO_MATCH_INDEX_HERE_DOC_OR_STRING}]} ]] {
        #     macro_byte_here_first=
        #     macro_byte_here_last=
        # }

        # If such macro was passed a here-document or -string on standard
        # input, record such string for the third preprocessor phase.
        # if [[ -n ${ZESHY__CODE_PREPROCESS[
        #     ${ZESHY_MACRO_MATCH_INDEX_HERE_DOC_OR_STRING}]} ]] {
        #     macro_bytes_first_name_last[1]+=( )
        # }

  # delimited by
            # "'" characters and properly
            # internally escaped).
# Hence, we need not add such
            # whitespace manually.
    # Byte indices of the first and last characters of the previously matched block argument
    # passed to such macro  if any or the first character following the last
    # character of such macro's name otherwise.

    #FUXME: Still require this? Excise us!

    # Byte index into such code.
    # integer macro_byte

            # Since the character preceding such match is guaranteed to be a non-UTF-8 whitespace character, incrementing such byte
            # index by 1 is safe.

            # Since the character preceding such block argument is guaranteed
            # to be a non-UTF-8 whitespace character, incrementing such byte
            # index by 1 is safe.

        # to the last character of the last argument passed to such macro.
#FUXME: O.K.; so, implementing a zsh-capable in pure-PCRE is rather difficult
#if not infeasible. For example, consider the complexity of merely
#differentiating the following edge cases:
#
#:func ':void yum()' {
#    print ${:-
#    :func
#    }
#    { print $(:stdin.get) } <<'o''k'
#:func
#o'k
#}
#
#Things get *VERY* tricky, *VERY* rapidly. Note the fact, for example, that
#quoted here-doc words may contain quote escapes that then need to be matched
#as their unescaped equivalents, which isn't really feasible to accomplish with
#simplistic PCRE-based group capturing and back referencing.
#
#So, here's what we're going to do for now (and possibly always): we're going
#to implement a genuine preprocessor in the sense that preprocessing will be a
#distinct phase performed strictly *BEFORE* zsh interpretation. In the above
#examples, such preprocessor would treat all macros ":func" regardless of zsh
#context the same. While non-ideal, there's just no feasible means of
#reimplementing robust zsh parsing with PCRE-based iteration.

#FUXME: Uh-oh! How do we perform nested PCRE-based matching? We need to
#perform iterative PCRE matching here to reliably match macros and, for each
#such macro, call such macro's preprocessor -- which itself could (and at
#least in the case of :func() absolutely will) also perform iterative PCRE
#matching. In a conventional language, this would be no problem. Here, however,
#we have a problem.
#
#While we *COULD* probably mitigate this by performing glob-based iteration
#here instead, globs are unwieldy, inefficient, and ultimately infeasible.
#Consider matching here-documents without back references, for example.
#
#Technically, we could simply call pcre_compile() *AFTER* calling each and
#every macro preprocessor.
#FUXME: O.K.; the solution, clearly, is to split preprocessing into two phases:
#
#1. In the first phase, PCRE-based iteration finds *ALL* macros to be replaced
#   in the second phase. Specifically, for each macro, such iteration appends
#   the byte indices of (in order):
#   * The first character of such macro (e.g., the ":" prefixing such macro).
#   * For each block argument passed to such macro (i.e., "{"- and
#     "}"-delimited block of user-defined code):
#     * The first character of such argument (i.e., such "{" delimiter).
#     * The last character of such argument (i.e., such "}" delimiter).
#   * The last character of such macro (e.g., the "}" suffixing such macro),
#     prefixed by "e". Since the number of block arguments is variable, the
#     second phase needs to differentiate this index from block
#     argument-specific indices. This seems a simple way.
#2. In the second phase, list-based iteration iterates over the previously
#   appended byte indices in *REVERSE* order, ensuring macros appearing later
#   in such code will be replaced before macros appearing earlier. Rather than
#   explicitly reverse such list (which introduces other complications),
#   implicitly reverse such list by ensuring the first item of such list is
#   always the empty string and only appending immediately after such item.
#   Assuming a linked list implementation, this is presumably efficient.

    # Phase 2: call and replace all such macros by their output in such code.

    # If such list size is *NOT* a factor of three, such list does *NOT*
    # consist of byte index triples. In such case, throw an exception. (While
    # this should never be the case, an ounce of prevention...)
    # (( ${#macro_bytes_first_name_last} % 3 == 0 )) || :die
    #     'List ${macro_bytes_first_name_last} size '${#macro_bytes_first_name_last}' indivisible by 3.'

    #FUXME: Ugh. We really need the macro order of both
    #${macro_bytes_first_name_last} and
    #${macro_bytes_arg_block_first_last} to be synchronized, implying we
    #should probably implement such reversal above where we append to the
    #former. (Fairly clear, in hindsight.)

    # For each previously matched macro, match each block argument passed to
    # such macro. The next preprocessor phase replaces such macros by their
    # expansions "in place." To prevent these expansions from invalidating the
    # byte indices of block arguments passed to subsequent macros, the next
    # preprocessor phase must iterate macros in reverse order. To ensure this,
    # iterate such macros in reverse order here.
    # for (( macro_byte=$(( ${#macro_bytes_first_name_last} - 2 ));
    #        macro_byte > 1;
    #        macro_byte+=-3 )) {

    #FUXME: Not quite right. We should note that zsh's inability to nest PCRE
    #compilations is an excrutiating handicap. Basically, we really need to
    #explicitly match a sufficiently large finite number of block arguments --
    #say, 16? (This can always be artificially increased as needed.) The reason
    #why, of course, is that we have no way of efficiently matching individual
    #block macro arguments with the ideal "((?:...)*)" approach permitting a
    #countably infinite number of such arguments. Hmm; actually, we *COULD* do
    #it, and probably should. To do so, however, we'll need a new phase 1.75.
    #
    #In phase 1, simply append the following integers to a list local for each
    #macro match:
    #
    #* The first byte of such macro name.
    #* The last byte of the last argument passed to such macro.
    #
    #Since the current PCRE structure readily provides such bytes, no
    #additional work should be required here.
    #
    #In phase 1.75, we'll need to iteratively find the first and last bytes of
    #each block argument passed to such macro (and append such bytes to yet
    #another list local in a manner also detailed elsewhere).
    #
    #It's hardly ideal, of course, but there is no ideal solution here. This
    #has the benefit of scaling to an arbitrary number of arguments, which is
    #blatantly preferable to the alternative -- even if it does incur minor
    #inefficiencies and code complexities along the way.
    #FUXME: Unfortunately, the above approach is complicated by the current
    #need to match macro-specific indentation prefixing the "}" delimiting such
    #block arguments, clearly infeasible with a single universal PCRE. Given
    #that, it's fairly clear that we either need to:
    #
    #1. Only match a finite number of block arguments at the same time as
    #   macros themselves are matched.
    #2. Match a countably infinite number of block arguments after macros are
    #   matched by matching the former with genuine context-free recursion.
    #
    #Obviously, one of these approaches has a bounded shelf-life and the other
    #does not. Given that, we'd might as well implement it right the first
    #time. (You know what to do.)

    #FUXME: We arguably need an intermediary validation phase -- call it
    #"Phase 1.5", perhaps. This phase's sole purpose is to validate the passed
    #code with respect to preprocessing -- that is, to ensure that all macros
    #present in such code were syntactically valid and hence preprocessed.
    #Naturally, such phase should be optimized away under optimized builds.
    #
    #To support such phase, phase 1 should append the starting byte index of
    #each syntactically valid macro to a new list local -- say,
    #${macro_valid_bytes_start}.
    #
    #In all non-optimized builds, phase 1.5 should then compile
    #${ZESHY__MACRO_NAMES_PCRE} and iterate over all substrings matching such
    #PCRE in such code. For each such match, such iteration should compare the
    #starting byte index of such match with the next list item in
    #${macro_valid_bytes_start}. If the two differ, the current macro must
    #necessarily be syntactically invalid. In such case, throw an exception and
    #immediately terminate preprocessing.
    #
    #Such validation does unavoidably introduce inefficiencies but must
    #nonetheless be performed. Due to the complexities of such PCREs, such
    #validation will be essential even for core debugging.
    #FUXME: Actually, we can combine phases 1 and 1.5 via clever PCRE
    #construction. How? By simply appending an alternative to the main
    #macro-matching PCRE matching syntactically invalid macros as any substring
    #beginning with a valid macro name. By listing such alternative *AFTER* the
    #initial PCRE matching only syntactically valid macros, we effectively
    #guarantee the former to only match invalid macros. (At least, that's the
    #theory.) Well, this puts a brighter spin on validation.

        # Begin searching for block arguments passed to the current macro from
        # the first character following the last character of such macro's name
        # to the last character of the last argument passed to such macro.
        # ZPCRE_OP=$((
        #     ${macro_bytes_first_name_last[$(( ${macro_byte} + 1 ))]} + 1
        # ))' '$((
        #     ${macro_bytes_first_name_last[$(( ${macro_byte} + 2 ))]}
        # ))

           # macro_byte < ({${#macro_bytes_first_name_last}..1}) {

    # triple of
    # previously recorderd byte indices, in
    # reverse order.

    # In the first phase, iteratively match all preprocessor macros in the
    # passed code and append the corresponding match indices to list local
    # ${macro_bytes_first_name_last}.

        # If either the byte indices of the last character of such macro and
        # such macro's name are equal, such macro is either a syntactically
        # valid macro accepting no arguments *OR* . the character following the
        # latter
        # is a newline, such macro  *AND* 
        # (( macro_byte_last == macro_byte_name_last )) ||
        # ${ZESHY__CODE_PREPROCESS} ]] || 

        #FUXME: Perform crude validation here.
#of the first and last non-whitespace macro characters.
        # Macro name.
        # macro_name=${}

        #FUXME: We probably also want to append the byte index of the last
        #character of such macro name, to prevent having to match macro names
        #in the next phase as well.

    # For each such match, append the following integers to list local
    # ${macro_bytes_first_name_last}:
    #
    # * The index of the first non-whitespace byte of such macro (e.g., the ":"
    #   prefixing such macro's name).
    # * The index of the last non-whitespace byte of such macro (e.g., the "}"
    #   suffixing such macro's last block argument).

#and
    # last bytes of such macro to a list local
    # Fortunately, locally disabling such option permits all strings (multibyte
    # or not) and hence such substring to be indexed by such byte indices.

    #   * The first two phases *ALWAYS* perform PCRE-based iteration.
    #   * Phase 2 *MAY* perform PCRE-based iteration, conditionally depending
    #     on whether the macros such phase calls do so. (Notably, the :func()
    #     macro does so.)
    #
    # Why? Subtleties! Consider the following causal chain of logic.
    #
    # * Since the first two phases *ALWAYS* perform PCRE-based iteration.
    #   * zsh currently only permits one PCRE to be compiled at a time,
    #     directly implying PCRE-based iteration cannot be nested.
    # * Such phases cannot be combined into a single phase.

# :void :code.preprocess(^:string code_name)
    # (( # == 1 )) || :die 'Expected one string name.'
    # local code_name__cp=${1} code__cp

    # For efficiency, cache such code.
    # code__cp=${(P)code_name__cp}

    # Set such code.
    # :string.set "${code_name__cp}" "${code__cp}"

#FUXME: Ugh. Shift string setters and type checkers here we suppose. Let's
#assume we've done that, for now.

    #FUXME: For readability, each line of such replacement should be manually
    #by the exact amount of indentation prefixing each such macro. This will
    #probably require matching such indentation into a new list local -- say,
    #${macros_indentation}.
    #FUXME: Wait. Forget this. For what "readability"? zsh forcefully reindents
    #all internal code, anyway.
