#!/usr/bin/env zsh
# --------------------( LICENSE                            )--------------------
# Copyright 2007-2014 by Cecil Curry.
# See "COPYING" for additional details.
#
#FIXME: Convert into parcel documentation.
# --------------------( SYNOPSIS                           )--------------------
# Preprocess zeshy-specific code into zsh-specific code.

#FIXME: O.K.; that's *EXACTLY* what we're going to do, but only as an interim
#solution. The long-term solution, though a bit balls-crazy, is as follows:
#
#* Merge the principal PCRE (i.e., ${ZESHY_CALLABLE_PROTOTYPE_PCRE}) produced
#  by ={*-soil/*-declare/*-pcre} into the principal PCRE produced by ={pcre}.
#* Refactor ={*-soil/*-declare/*-func/func} accordingly. (Actually, any
#  refactoring should be quite minimal, as we already reference match indices
#  by name rather than hard-coded integer).
#
#What's indolently pleasant about this approach is that it avoids all of the
#repetitous PCRE compilation we currently perform. Consider it: on each and
#every function declaration, we currently recompile
#${ZESHY_CALLABLE_PROTOTYPE_PCRE}! This is absolute insanity. Happily, the
#above approach does away with that entirely. It also pushes the limits of
#PCRE-based parsing, but hopefully in a tractable, feasible way.
#
#Only one way to find out whether "libpcre" will actually support a PCRE that
#monstrously complex *AND* whether we can actually reliably debug that PCRE,
#right? So, let's do this.
#FIXME: Or perhaps not? The prior solution appears to work quite well, now.

# ....................{ DEPENDENCIES                       }....................
::script_zsh.source_or_unwind_call_stack pcre

# ....................{ GLOBALS                            }....................
#FIXME: Document me.
# Listset of all registered macros. 
typeset -gaU ZESHY__MACRO_NAMES

# ....................{ DECLARERS                          }....................
#FIXME: Implement macro documentation support. This implies, of course, that
#we'll need to shift early-time documentation functionality here. *sigh*
#FIXME: Document me.

# :void :macro[
#     args =  (:string macro_name),
#     stdin = (:string macro_asciidoc)]
#
# Declare the passed macro, documented by
# http://asciidoc.org[AsciiDoc]-formatted standard input if passed or
# undocumented otherwise. For convenience, consider passing such documentation
# as a single- or double-quoted here-document: e.g.,
#
# == Macro Name Constraints ==
#
# For simplicity, macro names are constrained as follows: 
function :macro() {
    # Validate sanity.
    (( # == 1 )) || :die 'Expected one macro name.'
    local macro_name=${1}
    [[ -n ${macro_name} ]] || :die 'Macro name empty.'

    # If such name is *NOT* a valid macro name, throw an exception.
    # Technically, we *COULD* permit *ANY* nonempty strings to be valid macro
    # names (as is currently the case with alias and function names). Doing so,
    # however, would substantially complicate subsequent:
    #
    # * PCRE generation. All "libpcre"-reserved characters in such name would
    #   need to be explicitly escaped.
    # * Code preprocessing. Since macro users would need to explicitly escape
    #   all shell-reserved characters in such name *AND* since there exist
    #   multiple means of escape such characters in zsh (e.g., "\"-prefixed
    #   escapes and single- and double-quoted strings), matching all
    #   permissible uses of such macro would require either:
    #   * Matching all possible means of escape such characters.
    #   * Matching only one such means, requiring macro users to only escape
    #     such characters by such means.
    #   * Matching *NO* such means, requiring macro users to *NEVER* escape
    #     such characters. Since such preprocessing is *NOT* a zsh-compliant
    #     parser, such requirement is implementable. Unfortunately, it also
    #     introduces syntactic non-orthogonalities between the zsh parser and
    #     zeshy preprocessor. In other words, it's highly problematic.
    #
    # The latter is, arguably, the greater concern. While correcting the former
    # issue is certainly (albeit annoyingly) feasible, correcting the latter
    # issue is considerably less feasible if not infeasible altogether. For the
    # moment, sidestep such issues by prohibiting such characters.
    [[ ${macro_name} == [a-zA-Z0-9-_:.]## ]] || :die\
        'Macro name "'${macro_name}'" invalid. Consider renaming such macro to only contain letters and "_", "-", ".", and ":" characters.'

    # If such macro has already been declared, throw an exception.
    (( ${+ZESHY__MACRO_NAMES[${macro_name}]} == 0 )) || :die\
        'Macro name "'${macro_name}'" already declared.'

    # Declare such macro.
    ZESHY__MACRO_NAMES+=${macro_name}

    # Clear all previously generated preprocessor PCREs, forcing their
    # regeneration on the next preprocessor call. Since such PCREs match and
    # hence depend on the set of all macro names, declaring such macro
    # invalidates such PCREs. (While we *COULD* do so here, deferring such
    # regeneration prevents redundant regeneration in the common case of
    # multiple adjacent macro declarations in a parcel.)
    ::preprocessor_pcres.clear
}

# ....................{ SOURCERS                           }....................
# :void :script.source(:string filename)
function :script.source() {
    # Validate sanity.
    (( # == 1 )) || :die 'Expected one filename.'
    local script_filename=${1} script_code
    [[ -f ${script_filename} ]] || :die\
        'Script "'${script_filename}'" not found or not a file.'

    # To avoid modifying such script, localize such script's contents. For
    # efficiency, do so via zsh module "mapfile" rather than customary
    # alternatives (e.g., process substitution).
    {
        zmodload zsh/mapfile
        script_code=${mapfile[${script_filename}]}
    # Since such module is inherently unsafe, guarantee such module to be
    # unloaded immediately after retrieving such code -- even in the event such
    # retrieval induces an exception.
    } always {
        zmodload -ui zsh/mapfile
    }

    #FIXME: Nontrivial. We'll need to refactor
    #get_shell_script_shebang_command_name_if_found() into a setter and shift
    #the resulting function into a new parcel at this level. *sigh*
    #FIXME: Probably, no. Just inline the corresponding code here.
    #FIXME: Strip the prefixing shebang line from such code. Such line does
    #*NOT* constitute interpretable zeshy code and must not be treated as such.

    # If the first line of such code is *NOT* a shebang referencing the "zeshy"
    # interpreter, throw an exception. Preprocessing and evaluating arbitrary
    # bytes as valid zeshy code could have painful consequences and must be
    # avoided at all costs.

    # Preprocess such code.
    local ZESHY__CODE_PREPROCESS=${script_code}
    ::code.preprocess

    # Run such preprocessed code.
    eval "${script_code}"
}

# ....................{ PREPROCESSORS                      }....................
#FIXME: Arguably shift to a new parcel ={code} devoted to such operation.
#FIXME: Optimize by globalizing locals. (You know the code-cadenced drill.)
#FIXME: Document me. In particular, note that the caller must define string
#local ${ZESHY__CODE_PREPROCESS}.

# :void ::code.preprocess()
function ::code.preprocess() {
    # Validate sanity.
    (( # == 0 )) || :die 'Expected no arguments.'

    # If such variable is undefined or not a string, throw an exception. See
    # :var.die_unless_type_string().
    [[ ${(t)ZESHY__CODE_PREPROCESS-} == 'scalar'('-'*|) ]] || :die\
        'String ${ZESHY__CODE_PREPROCESS} undefined or not a string.'

    # Canonical string global ${ZPCRE_OP}, localized to avoid overwriting
    # either global or caller-specific versions of such variable.
    local ZPCRE_OP

    # Currently matched macro name.
    local macro_name

    # Function call replacing the current macro in the passed code by its
    # expansion. The first shell word of this whitespace-delimited string is
    # the name of such function (and hence such macro); all remaining shell
    # words are the arguments to be passed to such function.
    local ZESHY__MACRO_FUNC_CALL

    # Byte indices of the first and last characters of such macro's name.
    integer macro_byte_name_first macro_byte_name_last

    # Byte index of the last non-whitespace character of the current macro.
    integer macro_byte_last

    # Byte indices of the first and last characters of the currently matched
    # block argument passed to such macro. Since the former is a non-integer
    # character constant on iterating past the last block argument passed to
    # such macro, declare such indices to be strings rather than integers.
    local macro_byte_arg_block_first macro_byte_arg_block_last

    # Byte index of the last character of the previously matched block argument
    # passed to such macro if any or the first character following the last
    # character of such macro's name otherwise.
    integer macro_byte_arg_block_prior_last

    # List of positive integer triples, each a byte index into the passed code
    # such that:
    #
    # * The first list item of each such triple is the byte index of the first
    #   non-whitespace character of such macro (e.g., the ":" prefixing such
    #   macro's name).
    # * The second list item of each such triple is the byte index of the last
    #   non-whitespace character of such macro name (e.g., the "c" in ":func").
    # * The third list item of each such triple is the byte index of the last
    #   non-whitespace character of such macro (e.g., the "}" suffixing such
    #   macro's last block argument).
    #
    # Such triples are listed in reverse lexical order (i.e., in the reverse
    # order such macros appear in such code). The reason why is fairly subtle,
    # as reasons go. To prevent the expansion of each macro from invalidating
    # byte indices of subsequent macros, the third preprocessor phase must
    # expand and hence iterate macros in reverse order. While there exist
    # various means of ensuring this, persisting such macros in such order by
    # prepending such indices to the head of this list with list operator "+="
    # is arguably the simplest and most efficient. However, such operator
    # requires a valid 1-based list index and hence cannot be used to prepend
    # directly into the first list index (which would require "+=" to support
    # appending to an imaginary list index 0). We circumvent this oddity by
    # retaining the empty string as a "dummy" first list index to which we
    # append all subsequent triples.
    #
    # Such list circumvents zsh's current inability to nest PCRE-based
    # iteration, permitting matches in the first preprocessor phase to be
    # accessed in the second preprocessor phase. (It's all rather droll.)
    local -a macro_bytes_first_name_last
             macro_bytes_first_name_last=( '' )

    # List of ";"-terminated sublists of positive integer pairs, each a byte
    # index into the passed code such that:
    #
    # * The first list item of each such pair is the byte index of the "{"
    #   prefixing the next block argument passed to such macro.
    # * The second list item of each such pair is the byte index of the "}"
    #   suffixing the same block argument.
    #
    # Since zsh does *NOT* currently support type composition, terminate each
    # such sublist by the arbitrary delimiter ";". (This is horrible.)
    #
    # Such list circumvents zsh's current inability to nest PCRE-based
    # iteration, permitting matches in the second preprocessor phase to be
    # accessed in the third preprocessor phase. (Angel of death, take us now.)
    local -a macro_bytes_arg_block_first_last

    # If preprocessor PCREs have yet to be generated or require regeneration
    # (e.g., due to the declaration of new macros), do so.
    ::preprocessor_pcres.make_if_needed

    # Locally disable default zsh option "multibyte", permitting substrings
    # matched by pcre_match() to be indexed by the " "-delimited byte indices
    # written to ${ZPCRE_OP}.
    #
    # Unfortunately, pcre_match() fails to set integer globals ${MBEGIN} and
    # ${MEND} to the character indices of the start and end of the matched
    # substring. Fortunately, it *DOES* set string global ${ZPCRE_OP} to the
    # " "-delimited byte indices of the start and end of such substring.
    # Unfortunately, multibyte strings (e.g., UTF8-encoded) are indexed by
    # character rather than byte indices under default zsh option "multibyte".
    # Fortunately, locally disabling such option permits all strings (multibyte
    # or not) and hence such substring to be indexed by such byte indices.
    setopt -- local_options no_multibyte

    # ..................{ PHASE ~ macro                      }..................
    # Due to zsh inadequacies, partition preprocessing into multiple phases.
    # Since "zsh" currently only permits one PCRE to be compiled at a time,
    # PCRE-based iteration cannot be nested. Since the first two phases perform
    # such iteration, such phases cannot be combined. (Q.E.D., sadly.)

    # Compile and optimize such PCRE. See for_string_text_matching_pcre:().
    pcre_compile -- "${ZESHY_MACRO_PCRE}"
    pcre_study

    #FIXME: "-n ${ZPCRE_OP[(w)2]}" isn't quite right. We absolutely need to
    #record byte indices for macros expanded within block arguments passed to
    #top-level macros: e.g.,
    #
    #    :func :void :outer() {
    #        :func :void :inner() {
    #            print "Me! It's me!"
    #        }
    #    }
    #
    #While we *COULD* force each and every macro to explicitly call this
    #function with each and every passed block argument, that seems exceedingly
    #ungainly. Instead, since macros are line-bounded, simply start the next
    #match at any byte index inclusively between the first and last characters
    #of such line. For efficiency, we'd might as well choose the byte index of
    #the last character of such macro's name, which we have access to below.
    #FIXME: O.K.! The above change is both simple and correct. Unfortunately,
    #we need to do more. Alot more. The reason why, of course, is that
    #expanding an inner macro invalidates the last (but not first) byte indices
    #of all outer macros containing such inner macro. Consider the following
    #fairly minimal example:
    #
    #    :func :void :outer1() {
    #         :func :void :inner1() {
    #         }
    #    }
    #
    #    :func :void :outer2() {
    #         :func :void :mid1() {
    #             :func :void :inner2() {
    #             }
    #         } {
    #             :func :void :inner3() {
    #             }
    #         } <<'/---
    #    /---
    #         :func :void :mid2() {
    #             :func :void :inner4() {
    #             }
    #         } {
    #             :func :void :inner5() {
    #             }
    #         } <<'/---
    #    /---
    #    } <<'/---
    #    /---
    #
    #In the third prepreprocessor phase, the order of macro expansion will be
    #as follows:
    #
    #1. :inner2(), invalidating the last byte indices (of both the macro and
    #   block arguments) of :mid() and :outer(). For each such macro, note that
    #   the last byte indices of such macro and their block arguments differ.
    #2. :inner1(), invalidating the same byte indices.
    #3. :mid().
    #4. :outer(), invalidating the last byte indices (of both the macro and
    #   block arguments) of :outer().
    #
    #Solving such issue (hopefully) only requires changes to the third
    #preprocessor phase. To generalize our way into a solution, let's begin at
    #the simplest possible example:
    #
    #    :func :void :outer() {
    #        :func :void :inner() {
    #            print "Me! It's me!"
    #        }
    #    } <<'/---
    #    /---
    #
    #If all macros followed such form, the following would suffice:
    #
    #* Declare two new integer locals:
    #  * ${macro_byte_prior_last}, the byte index of the previously expanded
    #    macro.
    #  * ${macro_byte_offset}, the number of bytes by which to offset all
    #    lexically subsequent byte indices of outer macros.
    #* After replacing *ANY* macro, unconditionally:
    #  * Set ${macro_byte_prior_last} to the original last byte index of the
    #    current macro.
    #  * Increment ${macro_byte_offset} by the difference of such macro's prior
    #    length (as provided by its recorded first and last byte indices) and
    #    replacement length.
    #* Before converting *ANY* block argument into a string:
    #  * If such argument's first byte index is strictly greater than
    #    ${macro_byte_prior_last}, increment both such argument's first *AND*
    #    last byte indices by ${macro_byte_offset}.
    #  * Else if such argument's last byte index is strictly greater than
    #    ${macro_byte_prior_last}, increment only such argument's last byte
    #    index by ${macro_byte_offset}.
    #* After converting all block arguments for the current macro:
    #  * If such macro's last byte index is strictly greater than
    #    ${macro_byte_prior_last}, increment only such macro's last byte index
    #    by ${macro_byte_offset}.
    #
    #Unfortunately, a quandry remains. (We knew the above was simply too
    #simple!) We need to discover when the current macro cannot have embedded
    #the prior macro, as, in such case, ${macro_byte_offset} must be reset to
    #0. We cannot simply leave such integer as is, as subsequent incrementation
    #of such offset must start from 0.
    #
    #O.K.; so, we probably need two LIFO integer stacks. How about this:
    #
    #* Declare new integer locals:
    #  * ${macro_byte_stack_index}, the current index into both of the two
    #    stacks below. Initially set to 1, implying the default index.
    #* Declare new list locals:
    #  * ${macro_byte_stack_prior_last}, listing the last byte indices of all
    #    (?) previously expanded macros.
    #  * ${macro_byte_stack_offset}, listing the number of bytes by which to
    #    offset all lexically subsequent byte indices of outer macros (defined
    #    as macros with byte indices exceeding the corresponding list item of
    #    ${macro_byte_stack_prior_last}).
    #
    #O.K.; it's clear that, while we can extend the current approach to get
    #this working, the interleaving of inner macros with multiple block
    #arguments passed to an outer macro complicates things. Fortunately, there
    #appears to be a simple solution: refactor everything. Just kidding! Well,
    #only sort of. We *WILL* need to refactor the current approach we take to
    #constructing the macro function call. Rather than appending the arguments
    #to such function to a string local, we *REALLY* instead want to convert
    #block arguments into single-quoted strings *IN-PLACE* in the code to be
    #preprocessed -- in effect, using such code as a temporary storage buffer.
    #What's sweet about this approach is that the macro function to be called
    #needn't be programmatically constructed! After converting all block
    #arguments for such function (and adjusting byte indices accordingly, of
    #course), we simply evaluate the substring of such code identified by such
    #delimiting byte indices.
    #
    #That said, is "adjusting byte indices accordingly" after converting a
    #block argument as simple as we make it out to be? Probably, but consider.
    #
    #In any case, this approach requires that we *INTERLEAVE* byte indices for
    #both entire macros and block arguments into a single list. For obvious
    #reasons, byte indices for all block arguments passed to a macro will need
    #to appear *BEFORE* byte indices for such macro in such list. To
    #differentiate between byte indices for macros and byte indices for block
    #arguments in such list, we'll need to suffix the former (of which there
    #will presumably be fewer) with an identifying character -- say, ".".
    #
    #So, what does this give us? Well, since we've interleaved the two, we no
    #probably longer need the excrutiatingly convoluted map-based data
    #structures we were contemplating. Minor relief, anyway. We *WILL* still
    #need the list-based stacks we were contemplating above, but such is life.
    #
    #Wait. We'll need to make additional changes to the second preprocessor
    #phase to ensure that the byte indices for inner macros are interleaved
    #with those of outer block arguments in a strictly increasing sorted
    #manner. Ignoring "." suffixes (treat such strings as floats, perhaps?),
    #the easiest means of achieving such ordering might be to simply sort such
    #list afterwords with a zsh builtin. Although, that strikes us as fairly
    #hacky. Or perhaps not? Yeah, probably not. After all, how the heck could
    #we manually ensure such sorting during PCRE-based iteration?
    #
    #Additionally, one nice aspect of sorting after-the-fact is that we can
    #retain all of our existing data structures and logic for the first two
    #phases (well, don't forget to suffix macro indices by "."!) and simply
    #define a new list local ${macro_bytes} into which we sort the contents of
    #the existing two lists of byte indices. (Alternately, we could tryp to
    #keep a second list of byte indices mostly-sorted by ...
    #
    #Wait. Conventional sorting doesn't work, as we need to group last and
    #first byte indices in a balanced manner. That said, we *COULD* split last
    #and first byte indices into two discrete lists and then sort *THOSE*
    #lists. We think, anyway. Contemplate further.
    #
    #Right. We should note that the two lists will be ordered in opposite
    #directions. Rather than explicitly reverse one of them, however, simply
    #index one with the negation of the indices used to index the other. Done!.
    #
    #That's it. Hurrah, hurrah!
    #
    #The above algorithm appears to smoothly scale to *ALL* nesting depths,
    #which is certainly a relief. Consider:
    #
    #    :func :void :outer() {
    #    } {
    #        :func :void :inner() {
    #            print "Me! It's me!"
    #        }
    #    } <<'/---
    #    /---
    #
    #In this case, such inner macro invalidates only the last byte index of the
    #last block argument of the outer macro. As generalization:
    #
    #* 
    #
    #Specifically:
    #
    #* Declare a new list local ${stack} serving as a LIFO stack.

    # For each macro matched from such code, record matched byte indices for
    # the next preprocessor phase.
    ZPCRE_OP='0 0'
    while {
        pcre_match -b -n ${ZPCRE_OP[(w)2]} -- "${ZESHY__CODE_PREPROCESS}"
    } {
        # Such macro's name.
        macro_name=${match[${ZESHY_MACRO_MATCH_INDEX_NAME}]}

        # If such name does *NOT* correspond to an existing function, throw an
        # exception. (Macros are implemented as functions of the same name.)
        (( ${+functions[${macro_name}]} )) || :die\
            'Macro function '${macro_name}'() undefined.'

        #FIXME: Report this bug! This is currently affecting *EVERYONE*
        #leveraging zsh module "zsh/pcre", which is probably *EVERYONE*.
        # Byte indices recorded for the next preprocessor phase. Note that
        # "man zshmodules" documents ${ZPCRE_OP} to be a " "-delimited integer
        # pair such that:
        #
        # * The first such integer is the byte index of the first character of
        #   the matched substring.
        # * The second such integer is the byte index of the first character 
        #   following such substring.
        #
        # Unfortunately, this is *NOT* the case. The documentation is "off by
        # one" in both cases, a bug in either the current zsh implementation or
        # documentation. Regardless of whether option "-n" is passed to
        # pcre_match():
        #
        # * The first such integer is the byte index of the last character
        #   preceding such substring.
        # * The second such integer is the byte index of the last character of
        #   such substring.
        #
        # However, the subtle entanglements don't end there. The byte index of
        # the first character of such substring is the byte index of the next
        # character following the first such integer. If such character is
        # unibyte (i.e., encoded as a single byte), incrementing the latter by
        # 1 suffices to calculate the former. If such character is multibyte
        # (i.e., encoded as two or more bytes), however, this overly simplistic
        # calculation fails.
        #
        # By PCRE design, the character preceding a matched macro name is
        # guaranteed to be whitespace. Technically, UTF-8-encoded multibyte
        # whitespace characters *DO* exist. Since no official scripts precedes
        # a macro expansion by such characters *AND* since it appears
        # vanishingly unlikely that third-party scripts would ever do so,
        # assume such character to be unibyte for simplicity. In such case,
        # simplistically incrementing the first such integer by 1 suffices.
        macro_byte_name_first=$(( ${ZPCRE_OP[(w)1]} + 1 ))
        macro_byte_last=${ZPCRE_OP[(w)2]}
        macro_byte_name_last=$(( macro_byte_name_first + ${#macro_name} - 1 ))

        #FIXME: Insufficient, as this fails to distinguish syntactically valid
        #macros accepting no arguments from syntactically invalid macros
        #accepting arguments. Can we actually distinguish between the two? The
        #answer is a resounding "yup." By fortuitous design, PCRE actually
        #captures groups matched in positive lookahead! This means that we can
        #amend the current PCRE with positive lookahead immediately following
        #the matched macro name. Such lookahead must be made optional, and
        #should attempt to look ahead for one or more horizontal whitespace
        #characters followed by exactly one non-whitespace character: e.g.,
        #    local pcre_='(?=('${pcre_spaces}'\S)?)'
        #If such captured group is nonempty *AND* the empty group signifying
        #such macro to be syntactically valid was *NOT* matched, then and only
        #then have we positively identified such macro to be invalid. Sweet!
        #FIXME: Shift parcel ={*-get} here and explicitly source above.

        # If the empty group signifying such macro to be syntactically valid
        # was *NOT* matched, such macro is syntactically invalid. In such case,
        # throw an exception.
        (( ${#match} >= ZESHY_MACRO_MATCH_INDEX_IS_VALID )) || {
            # Line number of the current macro in such code, calculated by
            # removing all non-newline characters preceding such macro and
            # counting the number of newline characters that remain.
            integer line_number="${#ZESHY__CODE_PREPROCESS[
                1,${macro_byte_name_first}]//[^$'\n']}"

            # Throw such exception.
            :die $(::parcel.get_current)'macro '${macro_name}'() on line '${line_number}' passed syntactically invalid arguments (e.g., due to unmatched braces or quotes).'
        }

        # Record matched byte indices for the next preprocessor phase.
        macro_bytes_first_name_last[1]+=(
            ${macro_byte_name_first}
            ${macro_byte_name_last}
            ${macro_byte_last}
        )
    }

    # For simplicity, remove the placeholder empty string prefixing such list.
    macro_bytes_first_name_last[1]=()

    # ..................{ PHASE ~ block argument             }..................
    # Compile and optimize such PCRE. See for_string_text_matching_pcre:().
    pcre_compile -- "${ZESHY_MACRO_ARG_BLOCK_PCRE}"
    pcre_study

    # For each previously matched macro, match each block argument passed to
    # such macro.
    for macro_byte_name_first macro_byte_name_last macro_byte_last (
        "${macro_bytes_first_name_last[@]}") {
        # Begin searching for block arguments passed to the current macro
        # immediately after the last character of such macro's name.
        ZPCRE_OP='0 '${macro_byte_name_last}

        # For each block argument matched from such code, record matched byte
        # indices for the next preprocessor phase.
        while {
            # Byte index of the last character of the prior match if any or the
            # first character following the last character of such macro's name
            # otherwise.
            macro_byte_arg_block_prior_last=${ZPCRE_OP[(w)2]}

            # While such index precedes the last byte index of such macro,
            # attempt to match the next block argument passed to such macro.
            (( macro_byte_arg_block_prior_last < macro_byte_last )) &&
                pcre_match -b -n ${macro_byte_arg_block_prior_last} --\
                "${ZESHY__CODE_PREPROCESS}"
        } {
            #FIXME: Actually, macro names can contain UTF-8-encoded characters.
            #We should probably prohibit this, for the moment -- that, or
            #reimplement this in a multibyte-aware manner. *sigh*

            # Record matched byte indices for the next preprocessor phase.
            #
            # The last character preceding such match is either the last
            # character of such macro's name *OR* the "}" delimiting the prior
            # block argument and hence guaranteed to be ASCII. Given that,
            # incrementing the byte index of such character by 1 gives the byte
            # index of the first whitespace character of such match;
            # incrementing such index by the byte length of the possibly empty
            # substring of standard arguments preceding such block arguments
            # gives the byte index of the "{" delimiting such block argument.
            macro_bytes_arg_block_first_last+=(
                $(( ${ZPCRE_OP[(w)1]} + ${match[1]} + 1 ))
                    ${ZPCRE_OP[(w)2]}
            )
        }

        # Append an arbitrary character constant terminating the sublist of
        # argument block byte indices for such macro. (Hacks "R" Us.)
        macro_bytes_arg_block_first_last+=';'
    }

    # ..................{ PHASE ~ expand                     }..................
    #FIXME: Don't forget to strip suffixing line continuations (i.e.,
    #"\"-prefixed newlines) from macro arguments before calling such macros.
    #Efficiently straightforward using a simple global glob replacement. 
    #FIXME: Actually, we pretty much *HAVE* to call eval() to ensure proper
    #expansion of standard arguments. (Consider process substitutions embedded
    #in such arguments, for example.) Simplifies things, anyway. *shrug*
    #FIXME: Actually, couldn't we apply the presumably less expensive parameter
    #expansion flag "(e)" *AFTER* stripping line continuations and performing
    #shell word splitting with parameter expansion flag "(z)"? Although, at
    #that point, simply calling eval() is probably more efficient.
    #
    #Note that, if we do take the eval() route, we'll need to apply at least
    #parameter expansion flag "(q)" to each block argument. (Probably "(qq)").

    #FIXME: Shift above.
    integer macro_bytes_arg_block_first_last_index
    macro_bytes_arg_block_first_last_index=1

    # For each previously matched macro, replace such macro by its expansion.
    for macro_byte_name_first macro_byte_name_last macro_byte_last (
        "${macro_bytes_first_name_last[@]}") {
        # Function call replacing such macro in such code by its expansion.
        # Since such function's name is such macro's name, begin such call with
        # the latter.
        ZESHY__MACRO_FUNC_CALL=${ZESHY__CODE_PREPROCESS[
            ${macro_byte_name_first},${macro_byte_name_last}]}

        # Set the byte index of the last character of the prior block argument
        # to the byte index of the last character of such macro's name,
        # ensuring standard arguments following such name and preceding the
        # first block argument are passed to such call in the customary way.
        macro_byte_arg_block_prior_last=${macro_byte_name_last}

        # For each previously matched block argument passed to such macro,
        # convert such argument into a standard single-quoted argument.
        # Since a variadic number of block arguments are supported, the prior
        # phase terminates the last block argument passed to such macro with a
        # character constant.
        while {
            # Byte index of the first character of such block argument if any
            # or such character constant otherwise.
            macro_byte_arg_block_first=${macro_bytes_arg_block_first_last[
                ${macro_bytes_arg_block_first_last_index}]}

            # If such index is *NOT* the terminating character, proceed.
            [[ ${macro_byte_arg_block_first} != ';' ]]
        } {
            # Pass such call all standard arguments (i.e., non-block arguments)
            # following the prior block argument and preceding the current
            # block argument if any. (If no such arguments exist, this simply
            # appends the empty string and hence reduces to a noop.)
            #
            # By PCRE design, such arguments are guaranteed to be both preceded
            # and followed by non-empty whitespace.
            ZESHY__MACRO_FUNC_CALL+=${ZESHY__CODE_PREPROCESS[
                $(( macro_byte_arg_block_prior_last + 1 )),
                $(( macro_byte_arg_block_first - 1 ))]}

            # Preserve the byte index of the last character of such block
            # argument for subsequent examination *AFTER* expanding the prior
            # such index above.
            macro_byte_arg_block_prior_last=${macro_bytes_arg_block_first_last[
                $(( macro_bytes_arg_block_first_last_index + 1 ))]}

            # Pass such call such block argument converted to a standard
            # argument (i.e., as a single-quoted string, thus implicitly
            # quote-protecting all shell-reserved characters in such block).
            ZESHY__MACRO_FUNC_CALL+=${(qq)ZESHY__CODE_PREPROCESS[
                ${macro_byte_arg_block_first},
                ${macro_byte_arg_block_prior_last}]}
        }

        #FIXME: Append all remaining standard arguments preceding the optional
        #suffixing here-document or -string.
        if [[ ]] {
            ZESHY__MACRO_FUNC_CALL+=
        }
    }
}

# --------------------( WASTELANDS                         )--------------------
    #Solving such issue can be split into two parts:
    #
    #1. Detecting such 

            # macro_byte_arg_block_last=${macro_bytes_arg_block_first_last[$((
            #     macro_bytes_arg_block_first_last_index + 1 ))]}

        # for macro_byte_arg_block_first macro_byte_arg_block_last (
        #     "${macro_bytes_arg_block_first_last[,-1]}") {

        #FUXME: Implement me.

        # If such macro was passed a here-document or -string on standard
        # input, pass such input to such call as a single-quoted here-string.
        #FUXME: Correct me. We probably want to embed new byte indices
        #corresponding to the first and last byte indices of such here string.
        #The latter is simply ${ZPCRE_OP[(w)2]}; the former is the latter minus
        #the byte length of such match.
        #FUXME: Alternatively, perhaps we should simply copy such match into a
        #string list item of the same list. Since empty standard input is
        #indistinguishable from no standard input, perhaps this is the ideal
        #approach. It certainly seems simpler, if slightly less efficient. (Or
        #perhaps not, since we'll be taking a string slice and hence copy
        #anyway later if not here?)
        #FUXME: This necessitates modifying ${macro_byte_last} above to
        #strictly precede such byte indices, implying such integer should be
        #renamed to ${macro_byte_args_last}.

        # If such macro was passed a here-document or -string on standard
        # input, record such string for the third preprocessor phase.
        # if [[ -n ${ZESHY__CODE_PREPROCESS[
        #     ${ZESHY_MACRO_MATCH_INDEX_HERE_DOC_OR_STRING}]} ]] {
        #     macro_byte_here_first=
        #     macro_byte_here_last=
        # }

        # If such macro was passed a here-document or -string on standard
        # input, record such string for the third preprocessor phase.
        # if [[ -n ${ZESHY__CODE_PREPROCESS[
        #     ${ZESHY_MACRO_MATCH_INDEX_HERE_DOC_OR_STRING}]} ]] {
        #     macro_bytes_first_name_last[1]+=( )
        # }

  # delimited by
            # "'" characters and properly
            # internally escaped).
# Hence, we need not add such
            # whitespace manually.
    # Byte indices of the first and last characters of the previously matched block argument
    # passed to such macro  if any or the first character following the last
    # character of such macro's name otherwise.

    #FUXME: Still require this? Excise us!

    # Byte index into such code.
    # integer macro_byte

            # Since the character preceding such match is guaranteed to be a non-UTF-8 whitespace character, incrementing such byte
            # index by 1 is safe.

            # Since the character preceding such block argument is guaranteed
            # to be a non-UTF-8 whitespace character, incrementing such byte
            # index by 1 is safe.

        # to the last character of the last argument passed to such macro.
#FUXME: O.K.; so, implementing a zsh-capable in pure-PCRE is rather difficult
#if not infeasible. For example, consider the complexity of merely
#differentiating the following edge cases:
#
#:func ':void yum()' {
#    print ${:-
#    :func
#    }
#    { print $(:stdin.get) } <<'o''k'
#:func
#o'k
#}
#
#Things get *VERY* tricky, *VERY* rapidly. Note the fact, for example, that
#quoted here-doc words may contain quote escapes that then need to be matched
#as their unescaped equivalents, which isn't really feasible to accomplish with
#simplistic PCRE-based group capturing and back referencing.
#
#So, here's what we're going to do for now (and possibly always): we're going
#to implement a genuine preprocessor in the sense that preprocessing will be a
#distinct phase performed strictly *BEFORE* zsh interpretation. In the above
#examples, such preprocessor would treat all macros ":func" regardless of zsh
#context the same. While non-ideal, there's just no feasible means of
#reimplementing robust zsh parsing with PCRE-based iteration.

#FUXME: Uh-oh! How do we perform nested PCRE-based matching? We need to
#perform iterative PCRE matching here to reliably match macros and, for each
#such macro, call such macro's preprocessor -- which itself could (and at
#least in the case of :func() absolutely will) also perform iterative PCRE
#matching. In a conventional language, this would be no problem. Here, however,
#we have a problem.
#
#While we *COULD* probably mitigate this by performing glob-based iteration
#here instead, globs are unwieldy, inefficient, and ultimately infeasible.
#Consider matching here-documents without back references, for example.
#
#Technically, we could simply call pcre_compile() *AFTER* calling each and
#every macro preprocessor.
#FUXME: O.K.; the solution, clearly, is to split preprocessing into two phases:
#
#1. In the first phase, PCRE-based iteration finds *ALL* macros to be replaced
#   in the second phase. Specifically, for each macro, such iteration appends
#   the byte indices of (in order):
#   * The first character of such macro (e.g., the ":" prefixing such macro).
#   * For each block argument passed to such macro (i.e., "{"- and
#     "}"-delimited block of user-defined code):
#     * The first character of such argument (i.e., such "{" delimiter).
#     * The last character of such argument (i.e., such "}" delimiter).
#   * The last character of such macro (e.g., the "}" suffixing such macro),
#     prefixed by "e". Since the number of block arguments is variable, the
#     second phase needs to differentiate this index from block
#     argument-specific indices. This seems a simple way.
#2. In the second phase, list-based iteration iterates over the previously
#   appended byte indices in *REVERSE* order, ensuring macros appearing later
#   in such code will be replaced before macros appearing earlier. Rather than
#   explicitly reverse such list (which introduces other complications),
#   implicitly reverse such list by ensuring the first item of such list is
#   always the empty string and only appending immediately after such item.
#   Assuming a linked list implementation, this is presumably efficient.

    # Phase 2: call and replace all such macros by their output in such code.

    # If such list size is *NOT* a factor of three, such list does *NOT*
    # consist of byte index triples. In such case, throw an exception. (While
    # this should never be the case, an ounce of prevention...)
    # (( ${#macro_bytes_first_name_last} % 3 == 0 )) || :die
    #     'List ${macro_bytes_first_name_last} size '${#macro_bytes_first_name_last}' indivisible by 3.'

    #FUXME: Ugh. We really need the macro order of both
    #${macro_bytes_first_name_last} and
    #${macro_bytes_arg_block_first_last} to be synchronized, implying we
    #should probably implement such reversal above where we append to the
    #former. (Fairly clear, in hindsight.)

    # For each previously matched macro, match each block argument passed to
    # such macro. The next preprocessor phase replaces such macros by their
    # expansions "in place." To prevent these expansions from invalidating the
    # byte indices of block arguments passed to subsequent macros, the next
    # preprocessor phase must iterate macros in reverse order. To ensure this,
    # iterate such macros in reverse order here.
    # for (( macro_byte=$(( ${#macro_bytes_first_name_last} - 2 ));
    #        macro_byte > 1;
    #        macro_byte+=-3 )) {

    #FUXME: Not quite right. We should note that zsh's inability to nest PCRE
    #compilations is an excrutiating handicap. Basically, we really need to
    #explicitly match a sufficiently large finite number of block arguments --
    #say, 16? (This can always be artificially increased as needed.) The reason
    #why, of course, is that we have no way of efficiently matching individual
    #block macro arguments with the ideal "((?:...)*)" approach permitting a
    #countably infinite number of such arguments. Hmm; actually, we *COULD* do
    #it, and probably should. To do so, however, we'll need a new phase 1.75.
    #
    #In phase 1, simply append the following integers to a list local for each
    #macro match:
    #
    #* The first byte of such macro name.
    #* The last byte of the last argument passed to such macro.
    #
    #Since the current PCRE structure readily provides such bytes, no
    #additional work should be required here.
    #
    #In phase 1.75, we'll need to iteratively find the first and last bytes of
    #each block argument passed to such macro (and append such bytes to yet
    #another list local in a manner also detailed elsewhere).
    #
    #It's hardly ideal, of course, but there is no ideal solution here. This
    #has the benefit of scaling to an arbitrary number of arguments, which is
    #blatantly preferable to the alternative -- even if it does incur minor
    #inefficiencies and code complexities along the way.
    #FUXME: Unfortunately, the above approach is complicated by the current
    #need to match macro-specific indentation prefixing the "}" delimiting such
    #block arguments, clearly infeasible with a single universal PCRE. Given
    #that, it's fairly clear that we either need to:
    #
    #1. Only match a finite number of block arguments at the same time as
    #   macros themselves are matched.
    #2. Match a countably infinite number of block arguments after macros are
    #   matched by matching the former with genuine context-free recursion.
    #
    #Obviously, one of these approaches has a bounded shelf-life and the other
    #does not. Given that, we'd might as well implement it right the first
    #time. (You know what to do.)

    #FUXME: We arguably need an intermediary validation phase -- call it
    #"Phase 1.5", perhaps. This phase's sole purpose is to validate the passed
    #code with respect to preprocessing -- that is, to ensure that all macros
    #present in such code were syntactically valid and hence preprocessed.
    #Naturally, such phase should be optimized away under optimized builds.
    #
    #To support such phase, phase 1 should append the starting byte index of
    #each syntactically valid macro to a new list local -- say,
    #${macro_valid_bytes_start}.
    #
    #In all non-optimized builds, phase 1.5 should then compile
    #${ZESHY__MACRO_NAMES_PCRE} and iterate over all substrings matching such
    #PCRE in such code. For each such match, such iteration should compare the
    #starting byte index of such match with the next list item in
    #${macro_valid_bytes_start}. If the two differ, the current macro must
    #necessarily be syntactically invalid. In such case, throw an exception and
    #immediately terminate preprocessing.
    #
    #Such validation does unavoidably introduce inefficiencies but must
    #nonetheless be performed. Due to the complexities of such PCREs, such
    #validation will be essential even for core debugging.
    #FUXME: Actually, we can combine phases 1 and 1.5 via clever PCRE
    #construction. How? By simply appending an alternative to the main
    #macro-matching PCRE matching syntactically invalid macros as any substring
    #beginning with a valid macro name. By listing such alternative *AFTER* the
    #initial PCRE matching only syntactically valid macros, we effectively
    #guarantee the former to only match invalid macros. (At least, that's the
    #theory.) Well, this puts a brighter spin on validation.

        # Begin searching for block arguments passed to the current macro from
        # the first character following the last character of such macro's name
        # to the last character of the last argument passed to such macro.
        # ZPCRE_OP=$((
        #     ${macro_bytes_first_name_last[$(( ${macro_byte} + 1 ))]} + 1
        # ))' '$((
        #     ${macro_bytes_first_name_last[$(( ${macro_byte} + 2 ))]}
        # ))

           # macro_byte < ({${#macro_bytes_first_name_last}..1}) {

    # triple of
    # previously recorderd byte indices, in
    # reverse order.

    # In the first phase, iteratively match all preprocessor macros in the
    # passed code and append the corresponding match indices to list local
    # ${macro_bytes_first_name_last}.

        # If either the byte indices of the last character of such macro and
        # such macro's name are equal, such macro is either a syntactically
        # valid macro accepting no arguments *OR* . the character following the
        # latter
        # is a newline, such macro  *AND* 
        # (( macro_byte_last == macro_byte_name_last )) ||
        # ${ZESHY__CODE_PREPROCESS} ]] || 

        #FUXME: Perform crude validation here.
#of the first and last non-whitespace macro characters.
        # Macro name.
        # macro_name=${}

        #FUXME: We probably also want to append the byte index of the last
        #character of such macro name, to prevent having to match macro names
        #in the next phase as well.

    # For each such match, append the following integers to list local
    # ${macro_bytes_first_name_last}:
    #
    # * The index of the first non-whitespace byte of such macro (e.g., the ":"
    #   prefixing such macro's name).
    # * The index of the last non-whitespace byte of such macro (e.g., the "}"
    #   suffixing such macro's last block argument).

#and
    # last bytes of such macro to a list local
    # Fortunately, locally disabling such option permits all strings (multibyte
    # or not) and hence such substring to be indexed by such byte indices.

    #   * The first two phases *ALWAYS* perform PCRE-based iteration.
    #   * Phase 2 *MAY* perform PCRE-based iteration, conditionally depending
    #     on whether the macros such phase calls do so. (Notably, the :func()
    #     macro does so.)
    #
    # Why? Subtleties! Consider the following causal chain of logic.
    #
    # * Since the first two phases *ALWAYS* perform PCRE-based iteration.
    #   * zsh currently only permits one PCRE to be compiled at a time,
    #     directly implying PCRE-based iteration cannot be nested.
    # * Such phases cannot be combined into a single phase.

# :void :code.preprocess(^:string code_name)
    # (( # == 1 )) || :die 'Expected one string name.'
    # local code_name__cp=${1} code__cp

    # For efficiency, cache such code.
    # code__cp=${(P)code_name__cp}

    # Set such code.
    # :string.set "${code_name__cp}" "${code__cp}"

#FUXME: Ugh. Shift string setters and type checkers here we suppose. Let's
#assume we've done that, for now.

    #FUXME: For readability, each line of such replacement should be manually
    #by the exact amount of indentation prefixing each such macro. This will
    #probably require matching such indentation into a new list local -- say,
    #${macros_indentation}.
    #FUXME: Wait. Forget this. For what "readability"? zsh forcefully reindents
    #all internal code, anyway.
