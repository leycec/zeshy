#!/usr/bin/env zeshy
# --------------------( LICENSE                            )--------------------
# Copyright 2007-2014 by Cecil Curry.
# See "LICENSE" for additional details.

#FIXME: We appear to have thought that suffixing "[" delimiters by newlines in 
#variable expansions constituted valid syntax, which it clearly doesn't: e.g.,
#
#    # This is horribly invalid syntax.
#    ZESHY__MACRO_COMMAND+=${ZESHY__PREPROCESS_CODE[
#        $(( ZESHY__MACRO_BYTE_ARG_BLOCK_PRIOR_LAST + 1 )),
#        $(( ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST - 1 ))]}
#
#Whatever the initial reasoning, such syntax is blatantly wrong. Since newlines
#are (unfortunately) significant, such extraneous newlines *MUST* be excised.

#FIXME: The long-term solution to function prototype optimization, though a bit
#balls-crazy, is as follows:
#
#* Merge the principal PCRE (i.e., ${ZESHY_CALLABLE_PROTOTYPE_PCRE}) produced
#  by ={*-soil/*-declare/*-pcre} into the principal PCRE produced by ={pcre}.
#* Refactor ={*-soil/*-declare/*-func/func} accordingly. (Actually, any
#  refactoring should be quite minimal, as we already reference match indices
#  by name rather than hard-coded integer).
#
#What's indolently pleasant about this approach is that it avoids all of the
#repetitous PCRE compilation we currently perform. Consider it: on each and
#every function declaration, we currently recompile
#${ZESHY_CALLABLE_PROTOTYPE_PCRE}! This is absolute insanity. Happily, the
#above approach does away with that entirely. It also pushes the limits of
#PCRE-based parsing, but hopefully in a tractable, feasible way.
#
#Only one way to find out whether "libpcre" will actually support a PCRE that
#monstrously complex *AND* whether we can actually reliably debug that PCRE,
#right? So, let's do this.

# ....................{ GLOBALS ~ public : string          }....................
# Current macro's expansion to be set by macro functions and hence public.
typeset -g ZESHY_MACRO_EXPANSION

# ....................{ GLOBALS ~ private : string         }....................
# Current macro's name.
typeset -g ZESHY__MACRO_NAME

# Command setting the current macro's expansion, whose:
#
# * First shell word is the name of the function expanding such macro.
# * Subsequent shell words are the same words passed to such macro such that:
#   * Scalar arguments are unmodified.
#   * Block arguments are converted to single-quoted scalar arguments.
typeset -g ZESHY__MACRO_COMMAND

# ....................{ GLOBALS ~ private : int            }....................
# Byte indices of the first and last non-whitespace bytes of the currently
# preprocessed code to be preprocessed by the current preprocessor iteration.
typeset -gi ZESHY__PREPROCESS_CODE_BYTE_FIRST ZESHY__PREPROCESS_CODE_BYTE_LAST

# Currently iterated index of ${ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST_INDEX}.
typeset -gi ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST_INDEX

# Byte indices of the first and last non-whitespace bytes of the name of the
# current macro.
typeset -gi ZESHY__MACRO_BYTE_NAME_FIRST ZESHY__MACRO_BYTE_NAME_LAST

#FIXME: Still need ${ZESHY__MACRO_BYTE_FIRST}? Excise otherwise.

# Byte indices of the first and last non-whitespace bytes of the current macro.
typeset -gi ZESHY__MACRO_BYTE_FIRST ZESHY__MACRO_BYTE_LAST

# Byte indices of the first and last characters of the currently matched
# block argument passed to such macro.
typeset -gi ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST ZESHY__MACRO_BYTE_ARG_BLOCK_LAST

#FIXME: Still used? Excise otherwise.

# Byte index of the last character of the previously matched block argument
# passed to such macro if any or the first character following the last
# character of such macro's name otherwise.
# typeset -gi ZESHY__MACRO_BYTE_ARG_BLOCK_PRIOR_LAST

#FIXME: Appears to be obselete now. Excise after due consideration.

# Current 1-based index of list ${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST}.
# typeset -gi ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX

# ....................{ GLOBALS ~ private : list           }....................
# List of pairs of byte indices into the preprocessed code whose:
#
# * First item is the byte index of the first non-whitespace character of such
#   macro and hence the first character of such macro's name. For all official
#   macros, such character is guaranteed to be ":".
# * Second item is the byte index of the last non-whitespace character of such
#   macro and hence the last character of either:
#   * Such macro's name if such macro was passed no arguments.
#   * Such macro's last argument otherwise.
#
# The preprocessor iterates and appends to such list as a FIFO queue.
typeset -ga ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST

# List of triples of byte indices into the preprocessed code whose:
#
# * First item is the byte index of the first non-whitespace character of such
#   macro (e.g., the ":" prefixing :func()).
# * Second item is the byte index of the last non-whitespace character of such
#   macro name (e.g., the "c" in :func()).
# * Third item is the byte index of the last non-whitespace character of such
#   macro (e.g., the "}" suffixing such macro's last block argument).
#
# Such triples are listed in reverse lexical order (i.e., in the reverse order
# such macros appear in such code). The reason why is fairly subtle, as reasons
# go. To prevent the expansion of each macro from invalidating byte indices of
# subsequent macros, the third preprocessor phase must expand and hence iterate
# macros in reverse order. While there exist various means of ensuring this,
# persisting such macros in such order by prepending such indices to the head
# of this list with list operator "+=" is arguably the simplest and most
# efficient. However, such operator requires a valid 1-based list index and
# hence cannot be used to prepend directly into the first list index (which
# would require "+=" to support appending to an imaginary list index 0). We
# circumvent this oddity by retaining the empty string as a "dummy" first list
# index to which we append all subsequent triples.
#
# Such list circumvents zsh's current inability to nest PCRE-based iteration,
# permitting matches in the first preprocessor phase to be accessed in the
# second preprocessor phase. (It's all rather droll.)
typeset -ga ZESHY__MACRO_BYTES_NAME_FIRST_LAST

# List of ";"-terminated sublists of positive integer pairs, each a byte
# index into the passed code such that:
#
# * The first list item of each such pair is the byte index of the "{"
#   prefixing the next block argument passed to such macro.
# * The second list item of each such pair is the byte index of the "}"
#   suffixing the same block argument.
#
# Since zsh does *NOT* currently support type composition, terminate each
# such sublist by the arbitrary delimiter ";". (This is horrible.)
#
# Such list circumvents zsh's current inability to nest PCRE-based
# iteration, permitting matches in the second preprocessor phase to be
# accessed in the third preprocessor phase. (Angel of death, take us now.)

#FIXME: Appears to be obselete now. Excise after due consideration.

# typeset -ga ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST

# ....................{ PREPROCESSORS                      }....................
#FIXME: Optimize by globalizing locals. (You know the code-cadenced drill.)
#FIXME: Document me. In particular, note that the caller must define
#${ZESHY__PREPROCESS_CODE} prior to calling such function.

# :void ::code.preprocess()
function ::code.preprocess() {
    # Validate sanity.
    (( # == 0 )) || :die 'Expected no arguments.'

    # If this function call is recursive, throw an exception. For both
    # efficiency and implementation sanity, this function assumes macro
    # expansions to *NOT* call this function. See TRAPZERR().
    (( ${funcstack[(I)::code.preprocess]-1} == 1 )) ||
    [[ ${funcsourcetrace[${funcstack[(I)::code.preprocess]}]-} == *':0' ]] ||
        :die 'Non-recursive function ::code.preprocess() called recursively.'

    # If such variable is undefined or not a string, throw an exception. See
    # :var.die_unless_type_string().
    [[ ${(t)ZESHY__PREPROCESS_CODE-} == 'scalar'('-'*|) ]] || :die\
        'String ${ZESHY__PREPROCESS_CODE} undefined or not a string.'

    # If such code is empty, report success immediately. (That was easy.)
    [[ -n ${ZESHY__PREPROCESS_CODE} ]] || return 0

    # ..................{ DEPENDENCIES                       }..................
    # If preprocessor PCREs have yet to be generated or require regeneration
    # (e.g., due to the declaration of new macros), do so.
    ::preprocessor_pcres.make_if_needed

    # ..................{ OPTIONS                            }..................
    # Locally disable default zsh option "multibyte", permitting substrings
    # matched by pcre_match() to be indexed by the " "-delimited byte indices
    # written to ${ZPCRE_OP}.
    #
    # Unfortunately, pcre_match() fails to set integer globals ${MBEGIN} and
    # ${MEND} to the character indices of the start and end of the matched
    # substring. Fortunately, it *DOES* set string global ${ZPCRE_OP} to the
    # " "-delimited byte indices of the start and end of such substring.
    # Unfortunately, multibyte strings (e.g., UTF8-encoded) are indexed by
    # character rather than byte indices under default zsh option "multibyte".
    # Fortunately, locally disabling such option permits all strings (multibyte
    # or not) and hence such substring to be indexed by such byte indices.
    setopt local_options no_multibyte

    # ..................{ INITIALIZATION                     }..................
    # Canonical string global ${ZPCRE_OP}, localized to avoid overwriting
    # either global or caller-specific versions of such variable.
    local ZPCRE_OP

    # Initialize the first preprocessor iteration such that all top-level
    # (i.e., non-nested) macros in such code will be unconditionally expanded.
    ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST=( 1 ${#ZESHY__PREPROCESS_CODE} )

    #FIXME: Document us.
    #FIXME: Do we still require this?

    ZESHY__MACRO_BYTES_NAME_FIRST_LAST=( '' )

    # ..................{ PHASE                              }..................
    # Expand all top-level (i.e., non-nested) macros between the first and last
    # byte indices into such code corresponding to the current preprocessor
    # iteration, until *ALL* macros in such code have either been explicitly
    # expanded or implicitly deleted (e.g., due to being nested in an unused
    # block argument of an expanded parent macro). Since
    # ${ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST} is a FIFO queue, this
    # preprocessing strategy is effectively (...and efficiently) recursive.
    for ((
        ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST_INDEX=1;
        ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST_INDEX <\
            ${#ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST_INDEX};
        ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST_INDEX+=2
    )) {
        ZESHY__PREPROCESS_CODE_BYTE_FIRST=${ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST[${ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST_INDEX}]}
         ZESHY__PREPROCESS_CODE_BYTE_LAST=${ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST[$(( ${ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST_INDEX} + 1 ))]}

        #FIXME: Inject the three existing preprocessor phases here...

        # ................{ PHASE ~ macro                      }................
        # Due to zsh inadequacies, preprocessing is partitioned into multiple
        # phases. Since "zsh" permits only one PCRE to be compiled at a time,
        # PCRE-based iteration cannot be nested. Since the first two phases perform
        # such iteration, such phases cannot be combined. (Q.E.D., sadly.)

        #FIXME: Recompiling such PCRE on each preprocessor iteration strikes us
        #as as *HORRIFICALLY* inefficient. There simply *MUST* be a better way.
        #Anything would be better than this. But there probably isn't any
        #alternative. How could there be? zsh's overly parsimonious PCRE API
        #bites us hard on this one, offering yet further stimulus for a
        #disparate zeshy interpreter. For the moment, however, we have little
        #choice but to abide by such dictates.
        #
        #Hmm. We *REALLY* need to obviate such horribleness in at least the
        #general case of :func() and :alias_{command,maximal,suffix}()
        #expansions. Can we? Not really. But we can slightly lessen the blow.
        #Since we *KNOW* the signature of such macros, we can avoid both the
        #PCRE compilation *AND* iteration performed by the second phase. We
        #still have to perform such phase's interior logic, of course. But
        #every little bit helps. To effect this, we'll need to:
        #
        #* Hard-code the matching of :func() and
        #  :alias_{command,maximal,suffix}() into +{pcre/macro} by *ALWAYS*
        #  capturing exactly one and only one block argument following such
        #  macro names into a new group index unique to such argument.
        #* In phase 2 below:
        #  * If the current macro name is either :func() or
        #    :alias_{command,maximal,suffix}(), hard-code logic setting the
        #    first and last byte indices of such macro's single block argument
        #    using such captured match substring. (Yay!)
        #  * Else, perform the current PCRE-based search to get such indices.
        #FIXME: The above is great. But we can do significantly better. The
        #perscipacious reader may not necessarily believe this, but we can
        #engineer the codebase such that "${ZESHY__MACRO_PCRE}" need only be
        #compiled *ONCE* for the entire codebase. How? Read on, for zeshy
        #Shangri-la beckons:
        #
        #* Declare a new boolean (integer) global ${ZESHY__PCRE_IS_RECOMPILED},
        #  initialized to ***1***. Such global should be explicitly set to 1
        #  *EVERYWHERE* else in the codebase that pcre_compiled() is called,
        #  implying all calls to pcre_compiled() should be wrapped in a zeshy
        #  helper function also setting ${ZESHY__PCRE_IS_RECOMPILED} to 1.
        #* Refactor the following code to as follows:
        #
        #    # If such PCRE has yet to be compiled *OR* was compiled but has
        #    # since been replaced in the zsh cache by a more recently compiled
        #    # PCRE, compile and optimize such PCRE.
        #    if (( ZESHY__PCRE_IS_RECOMPILED )) {
        #        pcre_compile -- "${ZESHY__MACRO_PCRE}"
        #        pcre_study
        #
        #        # Avoid unnecessary recompilation on subsequent iterations.
        #        ZESHY__PCRE_IS_RECOMPILED=0
        #    }
        #* We currently employ PCRE-based iteration in two other prominent
        #  locations at this time: (a) parsing apart prototype lists (e.g.,
        #  attributes, names, arguments) and (b) capturing block arguments. In
        #  both cases, an unexpected alternative exists: "=~"-style PCRE-based
        #  iteration. While requiring PCRE recompilation on each call to "=~"
        #  and hence mildly inefficient, such approach has the profound benefit
        #  of *NOT* overwriting the internally cached PCRE resulting from calls
        #  to pcre_compile(). (Yes, we have manually verified this at the CLI.)
        #  That means we may iteratively call "=~" as much as we like while
        #  preserving the compilation of "${ZESHY__MACRO_PCRE}".
        #
        #  The question remains, however: is iterating with "=~" feasible? Yes,
        #  if somewhat cumbersomely. Consider:
        #
        #     >>> :string digits='091827' digit_pair
        #     >>> :string pcre_pair='^(\d\d)'
        #     >>> :while :is -n ${digits} :and ${digits} =~ ${pcre_pair} :si {
        #     ...     digit_pair=${match[1]}
        #     ...     :string.print "${digit_pair}"
        #     ...     digits=${digits[$(( ${#digit_pair} + 1 )),-1]}
        #     ... }
        #     09
        #     18
        #     27
        #
        #  This works. Of course, it destroys the source string in the process.
        #  But, hey... Let's see you do better on only twelve hours of sleep.
        #  It's somewhat clumsy, but it works. And that's all that really
        #  matters. The mild inefficiencies introduced by such approach are
        #  dramatically overshadowed by the massive efficiency gain of *NOT*
        #  recompiling the core PCRE every preprocessor iteration.
        #
        #  For safety, PCREs iterated in this way should be anchored to the
        #  string with "^" or prior match start with "\G".
        #
        #Awesome, eh?
        #FIXME: It may be possible to reuse the compiled "${ZESHY__MACRO_PCRE}"
        #PCRE to also parse and capture block arguments. This is absurdly
        #clever, and more than a little crazy -- but we believe it *COULD*
        #work. *COULD*. We should note that we're getting into wacky-hacky
        #territory with this one, but... Heck, it's worth a short shot in the
        #darkling dark. Consider; what if we:
        #
        #* Refactor ${ZESHY__MACRO_PCRE} to also capture block arguments.
        #* Iterate block arguments by calling pcre_match() with the exact byte
        #  indices of such macro in ${ZESHY__PREPROCESS_CODE}. Since such
        #  substring *WILL* begin with a macro name, such substring will be
        #  matched as a macro. If at least one block argument was passed to
        #  such macro, the last such block argument will be captured. If you
        #  consider it, however, this is *EXACTLY* what we want: to iterate
        #  block arguments in reverse order. (Insanity.)
        #* Convert such argument to a single-quoted string and hence to a
        #  scalar argument. Since such argument is no longer a block argument,
        #  iteration is guaranteed to terminate after converting all block
        #  arguments to single-quoted strings.
        #* Excise both ${ZESHY__MACRO_ARG_BLOCK_NEXT_PCRE} and
        #  ${pcre_words_passable_optional_spaced_grouped} from ${+pcre/macro},
        #  neither of which will be required anymore.
        #
        #That's inarguably the single-most clever code hackery I've ever
        #tunneled into. Man; I love it. It's so fundamentally insane, it simply
        #*HAS* to work.
        #FIXME: Indeed, such approach yields several other unexpected
        #efficiency gains. Since ${ZESHY__MACRO_PCRE} will be refactored to
        #also capture block arguments:
        #
        #* We need *NOT* explicitly match :func() and :alias_*() block
        #  arguments in ${ZESHY__MACRO_PCRE} -- which is great, as that sounded
        #  fairly terrible. (Dodged another code bullet on that one.)
        #* Instead, in a general-purpose manner applicable to *ALL* macros,
        #  perform phase 2 logic on the last block argument passed to the
        #  current macro matched by the prior pcre_match() call *BEFORE*
        #  subsequently iterating all other block arguments with additional
        #  pcre_match() calls. Since we're capturing one block argument "for
        #  free" already, we'd might as well efficiently leverage that. Yay!

        # Compile and optimize such PCRE.
        pcre_compile -- "${ZESHY__MACRO_PCRE}"
        pcre_study

        # For each macro matched from such code, record matched byte indices for
        # the next preprocessor phase.
        ZPCRE_OP='0 0'
        while {
            pcre_match -b -n ${ZPCRE_OP[(w)2]} -- "${ZESHY__PREPROCESS_CODE}"
        } {
            #FIXME: Despite zsh and libpcre constraints, we believe we've uncovered
            #a means of reliability discerning the starting and ending byte indices
            #of arbitrarily nested syntactically invalid syntactic cnstructs. How?
            #For each syntactic construct PCRE:
            #
            #* If such PCRE identifies a syntactic invalidity (e.g., by hetting the
            #  end of the string), such PCRE should report an error by backtracking
            #  to the character immediately preceding the offending syntactic
            #  construct and (mildly ironically) reporting success (with no
            #  additional capture groups). This ensures that ${ZPCRE_OP[(w)2]} will
            #  provide the byte index of such construct. Yay!
            #
            #  *WAIT*. To differentiate invalid from valid macros, we do
            #  arbitrarily require an additional non-empty capture group -- say,
            #  capturing the character immediately preceding the invalid syntactic
            #  construct via positive lookbehind into a trailing capture group.
            #  Call such global
            #  ${ZESHY__MACRO_GROUP_INDEX_INVALIDITY_CHAR_LAST}.
            #
            #  *WAIT*. If a macro argument is syntactically invalid, that should
            #  imply that... Ah-ha! O.K.; we should actually instead capture the
            #  last character (or perhaps all characters) of the set of all
            #  arguments passed to a macro into a trailing capture group.  Call
            #  such global ${ZESHY__MACRO_GROUP_INDEX_ARGS_CHAR_LAST}.
            #
            #  Since macros are *NEVER* recursive, such group is guaranteed to be
            #  non-empty if such macro is valid.
            #* ={backend} may then reliably detect such invalidity by performing
            #  the following logic:
            #  * If match group ${ZESHY__MACRO_GROUP_INDEX_NAME} is nonempty, a
            #    macro was matched. In such case, if:
            #    * If match group ${ZESHY__MACRO_GROUP_INDEX_ARGS_CHAR_LAST} is
            #      nonempty, such macro is valid; else, such macro is invalid.
            #  * Else, a macro was not matched. In such case, a syntactically
            #    invalid construct was matched if and only if the following:
            #    (( ${ZPCRE_OP[(w)2]} < ${#ZESHY__PREPROCESS_CODE} ))
            #
            #In either case, if an invalidity is matched:
            #
            #* The first byte index of the invalid syntactic construct is given by
            #  $(( ${ZPCRE_OP[(w)2]} + 1 )).
            #* The last byte index of such construct is, of course,
            #  ${#ZESHY__PREPROCESS_CODE}.
            #FIXME: Backtracking on syntactically invalid constructs to the
            #character immediately preceding such construct may prove somewhat
            #non-trivial. For example, (*ACCEPT) only returns from subroutine calls
            #(rather than halting matching) when in such calls, possibly preventing
            #us from responding in a sane fashion to invalid embedded syntactic
            #constructs. Hmm; by our PCRE design, this could still work. Possibly.
            #If we've meticulously matched everything exactly correctly, embedding
            #(*ACCEPT) in a subroutine call *SHOULD* induce the following behavour:
            #
            #* That subroutine call matching that syntactic construct immediately
            #  returns to the call matching its parent construct. Due to
            #  backtracking, the current position will be immediately before the
            #  offending child construct. Assuming proper PCRE construction, the
            #  call matching the parent construct will fail to match at this point,
            #  having exhausted all other options, in which case such call reports
            #  failure to and hence inducing backtracking in its parent and so on
            #  recursively up the line, iinevitably resulting in backtracking batk
            #  to the beginning and hence match failure. Hmm; that's bad.
            #
            #What we need to do is recursively propagate (*ACCEPT) up the
            #subroutine call hierarchy. Oh, boy. Perhaps that's simpler than it
            #sounds? Firstly, how would a parent call detect that a child call has
            #prematurely returned via (*ACCEPT)? Well...
            #
            #O.K.; we believe we may have found the way: "\K". Happily (and
            #contrarily to all other PCRE behaviour), subroutine calls do *NOT*
            #isolate the effect of "\K". That is, "\K" operates in a global manner
            #and hence permits us a subversive means of breaking through the
            #otherwise rigid barrier PCRE imposes about such calls.
            #
            #The idea here is that, on detecting a syntactic invalidity, the PCRE
            #responsible for matching that construct instead:
            #
            #* Backtracks to the first character of such construct. Since
            #  "(*COMMIT)", "(*SKIP)", and "(*PRUNE)" do nothing meaningful in
            #  subroutine calls, this implies we'll need to leverage implicit
            #  backtracking via alternation. Unfortunately, to backtrack all the
            #  way *BEFORE* the first character of such construct, we'll need to
            #  also leverage positive lookahead -- which shouldn't be much of an
            #  imposition: e.g.,
            #    local pcre_quotes_single="(?=')(?:'(?>[^']++|'')*+'"'|\K.*+)'
            #  Alternately, if such positive lookahead is *TOO* horrible (...it's
            #  not, really), we could backtrack to the first character following
            #  such prefix: e.g.,
            #    local pcre_quotes_single="'(?:(?>[^']++|'')*+'"'|\K.*+)'
            #  Unfortunately, that then requires a manual search backwards in
            #  ={backend} to attempt to find the start of such construct.
            #  Error-prone and fugly, needless to say the least. Let's just take
            #  the efficiency hit on this one, yes?
            #* All recursive subroutine calls (e.g., ${pcre_embeddable}) should be
            #  suffixed by something resembling
            #  '(?:'${pcre_string_end_report_success}')?+'.
            #* In ={backend}, detect the error condition as follows:
            #  * If a macro name was matched *AND* the first character of such name
            #    is *NOT* equal to the character at
            #    ${ZESHY__CODE_PEPROCESS[$(( ${ZPCRE_OP[(w)1]} + 1 ))]},
            #    then such macro is invalid.
            #  * Else, a macro was not matched. Assuming that we've recorded the
            #    last byte index of the prior match (trivial), then if such index
            #    is *NOT* one less than the first byte index of the current match,
            #    an invalid construct must have been matched. Oh, yes; this would
            #    happily work for the prior case as well. Yay! That tidies things.
            #
            #In summation:
            #
            #* In syntactic construct PCREs, refactor to the form:
            #    local pcre_quotes_single="(?=')(?:'(?>[^']++|'')*+'"'|\K.*+)'
            #* All recursive subroutine calls (e.g., ${pcre_embeddable}) should be
            #  suffixed by ${pcre_string_end_report_success_optional}, defined as:
            #    local pcre_string_end_report_success_optional='(?:'${pcre_string_end_report_success}')?+'
            #* In ={backend}, assuming that we've recorded the last byte index of
            #  the prior match (trivial), then if such index is *NOT* one less than
            #  the first byte index of the current match, an invalid construct must
            #  have been matched.
            #
            #Surprisingly simple, actually.
            #FIXME: For disambiguity in exception messages, unsupported
            #here-documents (i.e., here-documents with non-literal delimiters)
            #should be differentiated from syntactically invalid here-documents:
            #the former are *NOT* invalid in general-purpose zsh code, merely
            #unsupported in zeshy code. Happily, doing so should be more or less
            #trivial. In fact, we've already done so; see
            #${ZESHY__MACRO_HERE_DOC_UNSUPPORTED_PCRE}. Yay!

            # If a macro was matched, handle such macro.
            if [[ -n ${match[${ZESHY__MACRO_GROUP_INDEX_MACRO_NAME}]-} ]] {
                #FIXME: Consider everything below. Probably bollocks, at the moment.

                # Such macro's name.
                ZESHY__MACRO_NAME=${match[${ZESHY__MACRO_GROUP_INDEX_NAME}]}

                # Byte indices recorded for the next preprocessor phase. Note that
                # "man zshmodules" documents ${ZPCRE_OP} to be a " "-delimited integer
                # pair such that:
                #
                # * The first such integer is the byte index of the first character of
                #   the matched substring.
                # * The second such integer is the byte index of the first character 
                #   following such substring.
                #
                # Unfortunately, this is *NOT* the case. The documentation is "off by
                # one" in both cases, a bug in either the current zsh implementation or
                # documentation. Instead:
                #
                # * The first such integer is the byte index of the last character
                #   preceding such substring.
                # * The second such integer is the byte index of the last character of
                #   such substring.
                #
                # However, the subtle entanglements don't end there. The byte index of
                # the first character of such substring is the byte index of the next
                # character following the first such integer. If such character is
                # unibyte (i.e., encoded as a single byte), incrementing the latter by
                # 1 suffices to calculate the former. If such character is multibyte
                # (i.e., encoded as two or more bytes), however, this overly simplistic
                # calculation fails.
                #
                # By PCRE design, the character preceding a matched macro name is
                # guaranteed to be whitespace. Technically, UTF-8-encoded multibyte
                # whitespace characters *DO* exist. Since no official scripts precedes
                # a macro expansion by such characters *AND* since it appears
                # vanishingly unlikely that third-party scripts would ever do so,
                # assume such character to be unibyte for simplicity. In such case,
                # simplistically incrementing the first such integer by 1 suffices.
                ZESHY__MACRO_BYTE_NAME_FIRST=$(( ${ZPCRE_OP[(w)1]} + 1 ))
                ZESHY__MACRO_BYTE_NAME_LAST=$((
                    ZESHY__MACRO_BYTE_NAME_FIRST + ${#ZESHY__MACRO_NAME} - 1 ))
                ZESHY__MACRO_BYTE_LAST=${ZPCRE_OP[(w)2]}

                #FIXME: Shift parcel ={*-get} here and explicitly source above.
                #FIXME: Testing for syntactic invalidity now reduces to testing whether
                #a macro was matched or not and, if not, whether the last capture group
                #exists: e.g.,
                #
                #    if [[ -z ${match[${ZESHY__MACRO_GROUP_INDEX_NAME}]} ]] &&
                #        (( ${#match} < ZESHY__MACRO_GROUP_INDEX_STRING_IS_VALID )) {
                #        :die "Uh oh! Syntactic invalidity found."
                #    }

                # If the empty group signifying such macro to be syntactically valid
                # was *NOT* matched, such macro is syntactically invalid. In such case,
                # throw an exception.
                (( ${#match} >= ZESHY__MACRO_GROUP_INDEX_IS_VALID )) || {
                    # Line number of the current macro in such code, calculated by
                    # removing all non-newline characters preceding such macro and
                    # counting the number of newline characters that remain.
                    integer line_number="${#ZESHY__PREPROCESS_CODE[
                        1,${ZESHY__MACRO_BYTE_NAME_FIRST}]//[^$'\n']}"

                    # Throw such exception.
                    :die $(::parcel.get_current)'macro '${ZESHY__MACRO_NAME}'() on line '${line_number}' passed syntactically invalid arguments (e.g., due to unmatched braces or quotes).'
                }

                # Record matched byte indices for the next preprocessor phase.
                ZESHY__MACRO_BYTES_NAME_FIRST_LAST[1]+=(
                    ${ZESHY__MACRO_BYTE_NAME_FIRST}
                    ${ZESHY__MACRO_BYTE_NAME_LAST}
                    ${ZESHY__MACRO_BYTE_LAST}
                )
            # Else, a halting condition was matched.
            } else {
                #FIXME: Implement me.
            }
        }

        # For simplicity, remove the placeholder empty string prefixing such list.
        ZESHY__MACRO_BYTES_NAME_FIRST_LAST[1]=()

        # For sanity, ensure such list is of the expected size.
        (( ZESHY__MACRO_BYTES_NAME_FIRST_LAST % 3 == 0 )) || :die\
            'List ${ZESHY__MACRO_BYTES_NAME_FIRST_LAST} size '${#ZESHY__MACRO_BYTES_NAME_FIRST_LAST}' not divisible by 3.'

        # ................{ PHASE ~ arg                        }................
    #FIXME: Great! We believe there may exist a critical optimization, however.
    #Given the iterative queue-based approach above, is it *REALLY* essential that
    #we convert block arguments to strings in the same phase that we expand macros?
    #Intuition suggests, "Nope." Ideally, block arguments should probably be
    #converted to strings in phase 2. Given that, however, there clearly exists no
    #demonstrable reason to do much at all in phase 3 except expand macros in place.
    #That can't be right. Can it? If that *IS* right, we can eliminate
    #${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST} and associated logic in both
    #phases 2 and 3. Contemplate further before doing anything courageously wild.
    #
    #So. We've been mulling it over and over and... honestly, we can't see the
    #implicit catch-22 that must be lurking somewhere. So, let's go cautiously
    #forward, commenting out rather than explicitly eliminating all existing logic
    #that would appear to be obseleted by such an optimization. Careful here...
    #FIXME: O.K.; so, phase 2 will need to perform the following:
    #
    #* Excise ${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST} and associated logic.
    #* For passed block arguments, replace such arguments by a single-quoted
    #  string argument containing exactly such block.
    #* For the prototype passed to prototyped macros, replace such prototype by a
    #  single-quoted string argument containing exactly such prototype (e.g., via
    #  "${(qq)...}"). Alternately, such prototype could also be replaced by n
    #  number of string arguments, each expanding to a specific captured substring
    #  of such prototype (e.g., attributes, names). However, the latter approach is
    #  complicated by the disparity between singular and plural lists and the need
    #  to manually parse apart the latter. However, we may have an equitable
    #  solution to even that. Consider delimiting each list of elements by "\0";
    #  since an unquoted "\0" is *NOT* a valid return type, callable name, or
    #  argument name, such delimiter uniquely identifies a list terminator. An
    #  example is probably requisite here:
    #
    #      # The preprocessor should replace this original macro expansion...
    #      :some_macro [yum = "2", "yo!"] :void {:ug, ":uh!"}(:int o, :int k) "t i"
    #
    #      # ...by this:
    #      :some_macro \0\0\0 'yum' '2' 'yo!' '1' \0 ':void' ':void' ':void' \0 ':ug' ':uh!' \0 ':int o' ':int k' \0 \0 "t i"
    #
    #  Hence:
    #  * The first argument passed to a prototyped macro function should *ALWAYS*
    #    be some sufficiently unique constant unlikely to ever be passed otherwise
    #    (e.g., "\0\0\0").
    #  * Valueless attributes should be assigned the default value "1" (...assuming
    #    that's the current logic, anyway), enabling prototyped macro functions to
    #    copy such attributes directly into a local map (if desired).
    #  * Return types should *NOT* be prefixed by the name of their channel (e.g.,
    #    "stdout"). Since there only exist three possible types with no possibility
    #    of additional types (given the hardwired nature of return channels), all
    #    three channels should *ALWAYS* be passed. Naturally, single return types:
    #    * ":void" should be expanded to three arguments ":void :void ;void".
    #    * ":full" should be expanded to three arguments ":string :string ;int".
    #  * Argument lists should *NOT* be prefixed by the name of their channel
    #    (e.g., "stdin"). Again, simply pass (in order):
    #    * All standard arguments as string arguments, suffixed by "\0".
    #    * All stdin arguments as string arguments, suffixed by "\0".
    #
    #For efficiency, it's absolutely essential that prototype substrings be
    #captured via the same core PCRE capturing macros. Given that requirement, such
    #prototypes *MUST* necessarily be replaced by such captured groups in phase 2.
    #That said, the above approach may not necessarily be ideal. It imposes
    #inefficiencies and complexities on macro functions, which could be subverted
    #by the following alternative approach:
    #
    #      # The preprocessor should replace this original macro expansion...
    #      :some_macro [yum = "2", "yo!"] :void {:ug, ":uh!"}(:int o, :int k) "t i"
    #
    #      # ...by this:
    #      :some_macro \0\0\0 'yum = "2", "yo!"' ':void' '' '' ':ug, ":uh!"' ':int o, :int k' '' "t i"
    #
    #The empty strings are necessary to differentiate singular from plural capture
    #groups (e.g., single return types vs. return type channels). Right. So, the
    #prior example is pretty clearly the way to go. While prefixing such prototype
    #for validation purposes by "\0\0\0" still strikes us as a sensible idea,
    #there's no need for other eccentric usage of "\0" or variadic arguments.

        # Compile and optimize such PCRE.
        pcre_compile -- "${ZESHY__MACRO_ARG_BLOCK_NEXT_PCRE}"
        pcre_study

        #FIXME: Localize above.
        local ZESHY__MACRO_ARG_BLOCK
        local ZESHY__MACRO_ARG_BLOCK_PRIOR_LAST

        # For each previously matched macro, match each block argument passed to
        # such macro.
        for ZESHY__MACRO_BYTE_NAME_FIRST\
            ZESHY__MACRO_BYTE_NAME_LAST ZESHY__MACRO_BYTE_LAST (
            "${ZESHY__MACRO_BYTES_NAME_FIRST_LAST[@]}") {
            # Begin searching for block arguments passed to the current macro
            # immediately after the last character of such macro's name.
            ZPCRE_OP='0 '${ZESHY__MACRO_BYTE_NAME_LAST}

            # Byte index of the last character of the prior match if any or the
            # first character following the last character of such macro's name
            # otherwise.
            ZESHY__MACRO_BYTE_ARG_BLOCK_LAST=${ZESHY__MACRO_BYTE_NAME_LAST}

            # For each block argument matched from such code, record matched byte
            # indices for the next preprocessor phase.
            while {
                # While such index precedes the last byte index of such macro,
                # attempt to match the next block argument passed to such macro.
                #FIXME: We reckon the first test ain't quite right, anymore.
                (( ZESHY__MACRO_BYTE_ARG_BLOCK_LAST <\
                   ZESHY__MACRO_BYTE_LAST )) &&
                    pcre_match -b -n ${ZESHY__MACRO_BYTE_ARG_BLOCK_PRIOR_LAST} --\
                    "${ZESHY__PREPROCESS_CODE}"
            } {
                #FIXME: Explain me. See below.
                # First and last byte indices of such block argument.
                ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST=$((
                    ${ZPCRE_OP[(w)1]} + ${#match[1]} ))
                ZESHY__MACRO_BYTE_ARG_BLOCK_LAST=${ZPCRE_OP[(w)2]}

                #FIXME: Explain me. We'll have to explain that:
                #
                #* ${ZPCRE_OP[(w)1]} is the byte index of the character *PRECEDING*
                #  the first character of such matched substring, which in the case
                #  of the first block argument will be such macro's name.
                #* Macro names can contain UTF-8-encoded characters, meaning that
                #  we can't just increment by 1. So, implement this and *ONLY* this
                #  in a multibyte-aware manner. All other code remains unibyte.
                #* We don't need to explicitly localize such option, as we already
                #  did so above.
                #* Technically, we don't *NEED* to add ${#match[1]} here.
                #
                # See ={var/scalar/scalar}.
                #
                # Note also that the following isn't right:
                #
                #    setopt multibyte
                #    ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST+=${#ZESHY__PREPROCESS_CODE[
                #        ${ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST}]}
                #    setopt no_multibyte
                #
                # We need the number of bytes of such character. The above simply
                # returns "1". Instead, we do the following:
                #
                #* In multibyte mode, capture the desired character to a string.
                #* In unibyte mode, test the length of such string.
                #
                #Lame, but there you are. *shrug*

                #FIXME: Wait. This works, but it's inefficient. Instead, fix the
                #PCRE to ensure that, if there exist *ANY* arguments passed to a
                #macro, the first substring of such match is a horizontal
                #whitespace character.
                #FIXME: O.K.; the solution is, clearly, to revise the PCRE such
                #that it no longer groups the matched name but instead positive
                #lookahead capturing such name *PLUS* the following ASCII character
                #if any (i.e., if not at the end of the string): e.g., 
                #
                #    local pcre_macro_name_grouped='(?=(['${ZESHY__MACRO_NAME_CHAR_CLASS}']++.?))'${(kj:|:)ZESHY__MACRO_TO_FUNC_NAME}
                #
                #Clever, eh? The use of "++" guarantees that the ".?" matches only
                #either a non-macro *ASCII* character subject to the constraints of
                #the following actual match (e.g., horizontal whitespace,
                #backslash, newline) *OR* the end of the string. Note that while
                #capturing positive lookahead *SHOULD* work, we'll probably want to
                #verify this externally at the CLI.
                #
                #Since the last character of such match is guaranteed to be ASCII
                #by PCRE design, the byte index of the last character of such
                #macro's name is obtainable simply by subtracting one from the byte
                #length of such captured substring. Yay!

                setopt multibyte
                ZESHY__MACRO_ARG_BLOCK_PRIOR_LAST=${ZESHY__PREPROCESS_CODE[
                    ${ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST}]}

                setopt no_multibyte
                ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST+=${#ZESHY__MACRO_ARG_BLOCK_PRIOR_LAST}

                #FIXME: In accordance with the final optimization discussed above,
                #let's attempt to convert block arguments to strings in-place here.

                #FIXME: We need to offset such macro by the number of bytes changed
                #by such quote operation, implying we need to store the result of
                #such operation in a string local for testing. Make it so!

                # Convert such block argument to a standard argument formatted as a
                # single-quoted string, implicitly quote-protecting all shell-
                # reserved characters in such block.
                ZESHY__MACRO_ARG_BLOCK=${(qq)ZESHY__PREPROCESS_CODE[
                    ${ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST},
                    ${ZESHY__MACRO_BYTE_ARG_BLOCK_LAST}]}

                #FIXME: Actually, macro names can contain UTF-8-encoded characters.
                #We should probably prohibit this, for the moment -- that, or
                #reimplement this in a multibyte-aware manner. Something like:
                #    setopt -- local_options multibyte

                # Record matched byte indices for the next preprocessor phase.
                #
                # The last character preceding such match is either the last
                # character of such macro's name *OR* the "}" delimiting the last
                # block argument passed to such macro. Given that, incrementing the
                # byte index of such character by 1 gives the byte index of the
                # first whitespace character of such match; incrementing such index
                # by the byte length of the possibly empty substring of standard
                # arguments preceding such block arguments gives the byte index of
                # the "{" delimiting such block argument.
                # ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST+=(
                #     $(( ${ZPCRE_OP[(w)1]} + ${#match[1]} + 1 ))
                #         ${ZPCRE_OP[(w)2]}
                # )
            }
        }

        #FIXME: Excise after due consideration.
        # For sanity, ensure such list is of the expected size.
        # (( ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST % 2 == 0 )) || :die\
        #     'List ${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST} size '${#ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST}' not even.'

        # ................{ PHASE ~ expand                     }................
        #FIXME: By eval-ing macro arguments, we're effectively guaranteeing that
        #redirections and process substitutions embedded in double-quoted arguments
        #*WILL* be evaluated at macro expansion time -- which seems fairly bad. Or
        #perhaps not? We probably want to simply *STOP* eval-ing macro arguments.
        #
        #Hmm; for sane macro expansion, we really need to require that all unquoted
        #process substitutions and redirections be the *LAST* arguments to a macro
        #expansion. (Honestly, that's pretty much how it should be for all
        #conventional commands anyway.) Happily, the current approach should
        #already ensure this. We'll simply want to document (probably in this
        #section) that, by design, the current PCRE definition prohibits macro
        #redirections from being interspersed with standard macro arguments
        #(unlike standard commands). (Since macro functions never output anything, 
        #it makes no rational sense to try to redirect such functions anyway.)
        #
        #However, that still fails to take into account process substitutions
        #(either embedded in double-quoted strings or not). And you know what?
        #Perhaps we don't care terribly much about that. Or perhaps we do. It does
        #seem silly to expand such substitutions at macro expansion time,
        #suggesting that we'll need to either:
        #
        #* Prohibit such substitutions in macro arguments, which is sensible.
        #* Quote-protect all standard macro arguments. This would probably need to
        #  happen in phase 2 of ={backend}. Happily, we've verified that this can
        #  be accomplished reasonably efficiently: e.g.,
        #
        #    >>> :string yaz='"yum oh" test(.) $(yumyum) ''ug uh uk'''
        #    >>> print -r "${(qq@)${(z)yaz}}"
        #    '"yum oh"' 'test(.)' '$(yumyum)' '''ug uh uk'''
        #FIXME: Right. If you consider it, the latter approach is what we need to
        #be doing *ANYWAY*. Why? Because the eval() approach effectively strips
        #significant quoting from passed arguments. For example, a macro expansion
        #call of ":macro '/this is/a dir'" would ensure that the first argument to
        #such macro function would be "/this is/a dir" without the quotes,
        #requiring subsequent use of "(qq)" *ANYWAY* when producing an expansion
        #string. The above approach offloads all such quoting work to the parser,
        #which is where it should be anyway. So, let's definitely go with the
        #"${(qq@)${(z)args}}" approach in preprocessor phase 2 (i.e., here). This
        #ensures that we preserve the original quoting used, which could actually
        #prove significant. (Well, probably not. But who knows?)
        #FIXME: O.K.; we need to strip line continuations as well. Yes, literally
        #*STRIP* all line continuations with a simple global glob-based search and
        #replace: e.g., "${args//\\$'\n'}". Efficiency for the mild win.
        #FIXME: Wait. No. We need strip line continuations only when in unquoted
        #substrings of shell words *OR* between shell words. Line continuations
        #embedded in syntactic constructs should *NOT* be stripped, for fairly
        #obvious reasons. What's unclear, however, is just how we go about doing
        #that. We'll obviously need to do so prior to "(z)"-based word splitting
        #and "(qq)"-based word quoting. Given that, the only reasonable approach is
        #PCRE-based substitution and replacement (with the empty string).
        #
        #Clearly, this is a non-trivial issue to solve. Semantically, however, it's
        #largely trivial, implying that a brute-force, simplistic approach could be
        #preferable. How, though?
        #FIXME: Oh. Oh. Oh. Sooooo lucky! It turns out that "(z)" is doing quite a
        #bit more than mere word splitting. At the least, it also appears to
        #perform syntactically correct line continuation removal: e.g.,
        #
        #    >>> :string yum='yum "ok"\'$'\n''hm \'$'\n'' ugh'
        #    >>> print -r "${(qq@)${(z)yum}}"
        #    'yum' '"ok"hm' 'ugh'
        #
        #So, surprisingly, we're actually good here. "${(qq@)${(z)yum}}" approach
        #is *CLEARLY* the correct way forward, from a variety of perspectives.

        ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX=1

        # For each previously matched macro, replace such macro by its expansion.
        for ZESHY__MACRO_BYTE_NAME_FIRST\
            ZESHY__MACRO_BYTE_NAME_LAST\
            ZESHY__MACRO_BYTE_LAST (
            "${ZESHY__MACRO_BYTES_NAME_FIRST_LAST[@]}") {
            # Such macro's name.
            ZESHY__MACRO_NAME=${ZESHY__PREPROCESS_CODE[
                ${ZESHY__MACRO_BYTE_NAME_FIRST},${ZESHY__MACRO_BYTE_NAME_LAST}]}

            #FIXME: O.K.; so, *EVERY* macro unconditionally "returns" a single
            #string comprising the code replacing such macro. And how do we
            #"return" values in zsh? Right. Setters. This implies that *ALL* macros
            #must necessarily be setters. The question is: of what form? Clearly,
            #macros must accept a string variable name as an argument. (Corollary:
            #all macros accept at least one argument.) The question is: which
            #argument? Clearly, either the first or last, right? Given zeshy
            #nomenclature, we plainly choose the first. This implies that macro
            #functions should follow a nomenclature resembling:
            #
            #    :void :macro_name(^:string expansion_name, ...)
            #
            #Given that, implement the following:
            #
            #* Define a string global ${ZESHY_MACRO_EXPANSION} above.
            #* Pass the string "ZESHY_MACRO_EXPANSION" as the first unconditional
            #  argument following the macro name immediately below. (Yay!)
            #* After calling such macro function, replace such macro in such code
            #  by expanding ${ZESHY_MACRO_EXPANSION}.
            #FIXME: Ah! O.K.; for both efficiency and implementation simplicity, we
            #now require macro functions to *ALWAYS* set string global
            #${ZESHY_MACRO_EXPANSION}, implying that "ZESHY_MACRO_EXPANSION" should
            #*NOT* be explicitly passed as the first argument to macro functions.
            #Such functions simply know by design contract to set such global. Yay!

            # Command setting a passed string variable to such macro's expansion.
            ZESHY__MACRO_COMMAND=${ZESHY__MACRO_TO_FUNC_NAME[${ZESHY__MACRO_NAME}]}

            # If such name does *NOT* correspond to an existing function, throw an
            # exception. (Macros are implemented as functions of the same name.)
            (( ${+functions[${ZESHY__MACRO_COMMAND}]} )) || :die\
                'Macro '${ZESHY__MACRO_NAME}'() function '${ZESHY__MACRO_COMMAND}'() undefined.'

            # Set the byte index of the last character of the prior block argument
            # to the byte index of the last character of such macro's name,
            # ensuring standard arguments following such name and preceding the
            # first block argument are passed to such call in the customary way.
            ZESHY__MACRO_BYTE_ARG_BLOCK_PRIOR_LAST=${ZESHY__MACRO_BYTE_NAME_LAST}

            # For each previously matched block argument passed to such macro,
            # convert such argument into a standard single-quoted argument.
            # Since a variadic number of block arguments are supported, the prior
            # phase terminates the last block argument passed to such macro with a
            # character constant.
            while {
                # While there exists at least one more block argument...
                ((     ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX <\
                    ${#ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST} )) && {
                    # Byte index of the first character of the next block argument.
                    ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST=${
                        ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST[
                      ${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX}]}

                    # ...and while such index precedes the last byte index of such
                    # macro, such block argument was passed to such macro. In such
                    # case, convert such block argument.
                    (( ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST <\
                       ZESHY__MACRO_BYTE_LAST ))
                }
            } {
                # Byte index of the last character of such block argument.
                # Concurrently increment to the next block argument in preparation
                # for the next iteration.
                ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX+=1
                ZESHY__MACRO_BYTE_ARG_BLOCK_LAST=${
                    ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST[
                  ${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX}]}
                ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX+=1

                # Pass such call all standard arguments (i.e., non-block arguments)
                # following the prior block argument and preceding the current
                # block argument if any. (If no such arguments exist, this simply
                # appends the empty string and hence reduces to a noop.)
                #
                # By PCRE design, such arguments are guaranteed to be both preceded
                # and followed by non-empty whitespace.
                ZESHY__MACRO_COMMAND+=${ZESHY__PREPROCESS_CODE[
                    $(( ZESHY__MACRO_BYTE_ARG_BLOCK_PRIOR_LAST + 1 )),
                    $(( ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST - 1 ))]}

                # Preserve the byte index of the last character of such block
                # argument for subsequent examination *AFTER* expanding the prior
                # such index above.
                ZESHY__MACRO_BYTE_ARG_BLOCK_PRIOR_LAST=${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST[
                    $(( ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX + 1 ))]}

                #FIXME: We need to offset such macro by the number of bytes changed
                #by such quote operation, implying we need to store the result of
                #such operation in a string local for testing. Make it so!

                # Pass such call such block argument converted to a standard
                # argument (i.e., as a single-quoted string, thus implicitly
                # quote-protecting all shell-reserved characters in such block).
                ZESHY__MACRO_COMMAND+=${(qq)ZESHY__PREPROCESS_CODE[
                    ${ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST},
                    ${ZESHY__MACRO_BYTE_ARG_BLOCK_PRIOR_LAST}]}
            }

            #FIXME: Append all remaining standard arguments preceding the optional
            #suffixing here-document or -string.
            if [[ ]] {
                ZESHY__MACRO_COMMAND+=
            }

            #FIXME: Define ${ZESHY_MACRO_EXPANSION_UNSET} above.

            # Initialize such global to a placeholder, permitting detection of such
            # command reporting success but failing to set such global to such
            # macro's expansion. In such case, such global will remain set to such 
            # uniquely identifiable placeholder.
            ZESHY_MACRO_EXPANSION=${ZESHY_MACRO_EXPANSION_UNSET}

            #FIXME: Consider wrapping exceptions thrown by and/or failure exit
            #status reported by such command with human-readable exception messages
            #listing the macro that failed, the line number on which it failed, and
            #so on. We'll definitely want some common error handling routines for
            #similar functionality elsewhere in this parcel.

            # Set such string to such macro's expansion. Specifically:
            #
            # * "${(z)...}", splitting such command into shell words as well as
            #   removing all unquoted line continuations between such words.
            # * "${(qq@)...}", quote-protecting each such word as a single-quoted
            #   string. This ensures that executable code embedded in such words
            #   (e.g., process substitutions) will be preserved as is rather than
            #   insecurely executed in-place.

            #FIXME: Correct, but inefficient. Since macro names need *NOT* be
            #quote-protected (by design) *AND* since all prototype and block
            #arguments are already necessarily quote-protected above, the only
            #arguments that remain to be quote-protected are scalar. If one
            #considers it, however, we could simply:
            #
            #* Explicitly quote-protect scalar arguments above in phase 2.
            #* Refactor the following command to read merely:
            #
            #    ${(z)ZESHY__MACRO_COMMAND}

            ${(qq@)${(z)ZESHY__MACRO_COMMAND}}

            # If such command reported success but failed to set such global to
            # such macro's expansion, throw an exception.
            [[ ${ZESHY_MACRO_EXPANSION} != ${ZESHY_MACRO_EXPANSION_UNSET} ]] ||
                :die 'Macro '${ZESHY__MACRO_NAME}'() function '${ZESHY__MACRO_COMMAND}'() undefined.'

            # Replace such macro in such code by such expansion.
            ZESHY__PREPROCESS_CODE[${ZESHY__MACRO_BYTE_NAME_FIRST},${ZESHY__MACRO_BYTE_LAST}]=${ZESHY_MACRO_EXPANSION}

            # If such expansion is nonempty, append its first and last byte
            # indices to the FIFO queue of such indices, ensuring all top-level
            # (i.e., non-nested) macros in such expansion will be expanded by
            # subsequent preprocessor iteration. Since this phase iterates
            # macros in reverse lexical order, appending onto such list
            # implicitly preserves such order, ensuring the first pair of list
            # items will remain the indices of the last expansion in such code.
            if (( -n ${ZESHY_MACRO_EXPANSION} )) {
                ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST+=(
                    ${ZESHY__MACRO_BYTE_NAME_FIRST}
                    (( ZESHY__MACRO_BYTE_NAME_FIRST + ${#ZESHY_MACRO_EXPANSION} ))
                )
            }
        }
    }
}

# --------------------( WASTELANDS                         )--------------------
        #* +{compile} should set explicitly set ${ZESHY__PCRE_IS_RECOMPILED} to
        #  0 for safety before the first call to :script.source():. Err,
        #  actually, a 

#FUXME: "-n ${ZPCRE_OP[(w)2]}" isn't quite right. We absolutely need to
#record byte indices for macros expanded within block arguments passed to
#top-level macros: e.g.,
#
#    :func :void :outer() {
#        :func :void :inner() {
#            print "Me! It's me!"
#        }
#    }
#
#While we *COULD* force each and every macro to explicitly call this
#function with each and every passed block argument, that seems exceedingly
#ungainly. Instead, since macros are line-bounded, simply start the next
#match at any byte index inclusively between the first and last characters
#of such line. For efficiency, we'd might as well choose the byte index of
#the last character of such macro's name, which we have access to below.
#FUXME: O.K.! The above change is both simple and correct. Unfortunately,
#we need to do more. Alot more. The reason why, of course, is that
#expanding an inner macro invalidates the last (but not first) byte indices
#of all outer macros containing such inner macro. Consider the following
#fairly minimal example:
#
#    :func :void :outer1() {
#         :func :void :inner1() {
#         }
#    }
#
#    :func :void :outer2() {
#         :func :void :mid1() {
#             :func :void :inner2() {
#             }
#         } {
#             :func :void :inner3() {
#             }
#         } <<'/---
#    /---
#         :func :void :mid2() {
#             :func :void :inner4() {
#             }
#         } {
#             :func :void :inner5() {
#             }
#         } <<'/---
#    /---
#    } <<'/---
#    /---
#
#In the third prepreprocessor phase, the order of macro expansion will be
#as follows:
#
#1. :inner2(), invalidating the last byte indices (of both the macro and
#   block arguments) of :mid() and :outer(). For each such macro, note that
#   the last byte indices of such macro and their block arguments differ.
#2. :inner1(), invalidating the same byte indices.
#3. :mid().
#4. :outer(), invalidating the last byte indices (of both the macro and
#   block arguments) of :outer().
#
#Solving such issue (hopefully) only requires changes to the third
#preprocessor phase. To generalize our way into a solution, let's begin at
#the simplest possible example:
#
#    :func :void :outer() {
#        :func :void :inner() {
#            print "Me! It's me!"
#        }
#    } <<'/---
#    /---
#
#If all macros followed such form, the following would suffice:
#
#* Declare two new integer locals:
#  * ${macro_byte_prior_last}, the byte index of the previously expanded
#    macro.
#  * ${macro_byte_offset}, the number of bytes by which to offset all
#    lexically subsequent byte indices of outer macros.
#* After replacing *ANY* macro, unconditionally:
#  * Set ${macro_byte_prior_last} to the original last byte index of the
#    current macro.
#  * Increment ${macro_byte_offset} by the difference of such macro's prior
#    length (as provided by its recorded first and last byte indices) and
#    replacement length.
#* Before converting *ANY* block argument into a string:
#  * If such argument's first byte index is strictly greater than
#    ${macro_byte_prior_last}, increment both such argument's first *AND*
#    last byte indices by ${macro_byte_offset}.
#  * Else if such argument's last byte index is strictly greater than
#    ${macro_byte_prior_last}, increment only such argument's last byte
#    index by ${macro_byte_offset}.
#* After converting all block arguments for the current macro:
#  * If such macro's last byte index is strictly greater than
#    ${macro_byte_prior_last}, increment only such macro's last byte index
#    by ${macro_byte_offset}.
#
#Unfortunately, a quandry remains. (We knew the above was simply too
#simple!) We need to discover when the current macro cannot have embedded
#the prior macro, as, in such case, ${macro_byte_offset} must be reset to
#0. We cannot simply leave such integer as is, as subsequent incrementation
#of such offset must start from 0.
#
#O.K.; so, we probably need two LIFO integer stacks. How about this:
#
#* Declare new integer locals:
#  * ${macro_byte_stack_index}, the current index into both of the two
#    stacks below. Initially set to 1, implying the default index.
#* Declare new list locals:
#  * ${macro_byte_stack_prior_last}, listing the last byte indices of all
#    (?) previously expanded macros.
#  * ${macro_byte_stack_offset}, listing the number of bytes by which to
#    offset all lexically subsequent byte indices of outer macros (defined
#    as macros with byte indices exceeding the corresponding list item of
#    ${macro_byte_stack_prior_last}).
#
#O.K.; it's clear that, while we can extend the current approach to get
#this working, the interleaving of inner macros with multiple block
#arguments passed to an outer macro complicates things. Fortunately, there
#appears to be a simple solution: refactor everything. Just kidding! Well,
#only sort of. We *WILL* need to refactor the current approach we take to
#constructing the macro function call. Rather than appending the arguments
#to such function to a string local, we *REALLY* instead want to convert
#block arguments into single-quoted strings *IN-PLACE* in the code to be
#preprocessed -- in effect, using such code as a temporary storage buffer.
#What's sweet about this approach is that the macro function to be called
#needn't be programmatically constructed! After converting all block
#arguments for such function (and adjusting byte indices accordingly, of
#course), we simply evaluate the substring of such code identified by such
#delimiting byte indices.
#
#That said, is "adjusting byte indices accordingly" after converting a
#block argument as simple as we make it out to be? Probably, but consider.
#
#In any case, this approach requires that we *INTERLEAVE* byte indices for
#both entire macros and block arguments into a single list. For obvious
#reasons, byte indices for all block arguments passed to a macro will need
#to appear *BEFORE* byte indices for such macro in such list. To
#differentiate between byte indices for macros and byte indices for block
#arguments in such list, we'll need to suffix the former (of which there
#will presumably be fewer) with an identifying character -- say, ".".
#
#So, what does this give us? Well, since we've interleaved the two, we no
#probably longer need the excrutiatingly convoluted map-based data
#structures we were contemplating. Minor relief, anyway. We *WILL* still
#need the list-based stacks we were contemplating above, but such is life.
#
#Wait. We'll need to make additional changes to the second preprocessor
#phase to ensure that the byte indices for inner macros are interleaved
#with those of outer block arguments in a strictly increasing sorted
#manner. Ignoring "." suffixes (treat such strings as floats, perhaps?),
#the easiest means of achieving such ordering might be to simply sort such
#list afterwords with a zsh builtin. Although, that strikes us as fairly
#hacky. Or perhaps not? Yeah, probably not. After all, how the heck could
#we manually ensure such sorting during PCRE-based iteration?
#
#Additionally, one nice aspect of sorting after-the-fact is that we can
#retain all of our existing data structures and logic for the first two
#phases (well, don't forget to suffix macro indices by "."!) and simply
#define a new list local ${macro_bytes} into which we sort the contents of
#the existing two lists of byte indices. (Alternately, we could tryp to
#keep a second list of byte indices mostly-sorted by ...
#
#Wait. Conventional sorting doesn't work, as we need to group last and
#first byte indices in a balanced manner. That said, we *COULD* split last
#and first byte indices into two discrete lists and then sort *THOSE*
#lists. We think, anyway. Contemplate further.
#
#Right. We should note that the two lists will be ordered in opposite
#directions. Rather than explicitly reverse one of them, however, simply
#index one with the negation of the indices used to index the other. Done!
#
#Actually, not done. We can't both sorting such lists and expect them to
#remain in sync. Clearly, only the following alternatives then apply:
#
#* Manually sort inner macro indices into outer macro and block argument
#  indices. This would certainly work, but clearly incurs non-negligible
#  time costs -- particularly as we begin employing more macros.
#* Sort all indices after the fact by combining each pair of first and last
#  indices into a single integer using fixed-point math. Somewhat
#  astonishingly, zsh supports the full gamut of bit operations in
#  arithmetic expansions. Moreover, there appears to be no meaningful upper
#  limit on integer values. Sadly, we can't recall if there's a means of
#  determining the maximum integer value programmatically. On my machine,
#  here's what we find:
#
#  >>> :int j; (( j = (10*(10*10000000000000000)) )); print $(( [#10] j ))
#  1000000000000000000
#  >>> :int j; (( j = (10*(10*100000000000000000)) )); print $(( [#10] j ))
#  -8446744073709551616
#
#We've clearly bisected the limit. This implies, however, that we can
#divide the range. Ah; right. So, we're dealing with 64-bit integers (i.e.,
#longs): e.g.,
#
#  >>> :int j; (( j = 1 << 63 )); print $(( [#16] j ))
#  -0x8000000000000000
#  >>> :int j; (( j = 1 << 64 )); print $(( [#16] j ))
#  0x1
#
#This implies that we should reserve the upper 32 bits for the first byte
#index and the lower 32 bits for the last byte index, further implying that
#the maximum line number the zeshy preprocessor can support is:
#
#    4,294,967,295
#
#Clearly, such maximum will effectively guaranteeably *NEVER* be reached,
#so we're actually fine here. (Still, that should be checked.)
#
#Oh, bother. We forgot about the terminating constant. (Ugh.) We'd somehow
#need to ignore such constants for the purposes of sorting -- which isn't
#going to happen, clearly. Alternatively, rather than embedding terminating
#constants into such list, we *COULD* record the byte index pair of the
#last block argument for each macro in a new map local for efficient
#subsequent lookup. That should incur approximately the same costs as the
#terminating integer approach, but allow us to sort the byte indices list.
#
#Oh, wait; that's silly. Simply embed the terminating constant as a boolean
#bit in *EVERY* such integer.
#
#Oh, wait; under the new conception of aggregating macros and block
#arguments together, we don't care *AT ALL* whether an argument is the last
#or not. So, this whole issue goes up in fruitless smoke. Yay!
#
#Right. After contemplation, it's clear that we no longer care *AT ALL*
#where the last block argument is. Instead, what we care about is
#differentiating byte indice pairs for macros from block arguments. We
#can't simply used floating point numbers for one, as floating point has
#"holes" at larger integer ranges and hence behaves only as an
#approximation. Instead, what we need to do is divide the 64-bit integer
#space into an absurdly compact bit representation. While doing so, we must
#take care to ensure that the resulting structure will remain sortable -- a
#delicate balancing act. Can we do it? We believe we can. Consider:
#
#            First         Last        Boolean bits:
#            byte index    byte index  01 if macro; 00 if block arg
# 64 bit    /-----^-----\/-----^-----\/---^----\
# integer =  High 31 bits Mid 31 bits Low 2 bits
#
#For sorting purposes, the first byte index must be allocated the high
#bits. For orthogonality, the last byte index is allocated the middle
#bits. The boolean bits are allocated all remaining space (despite
#technically only requiring one bit of storage).
#
#Wait! Do we actually need boolean bits? Technically, no. Since the first
#and last bytes of *ALL* block arguments are guaranteed to be "{" and "}",
#testing such bytes suffices to distinguish macros from block arguments.
#For simplicity, our first implementation should probably do exactly that.
#Forego the boolean bits, for now! Going forward, however, it might be
#prudent to optimize such testing with boolean bits. Contemplate later.
#
#Ah. Actually, we believe we *DO* need boolean bits -- but for hard
#algorithmic purposes rather than merely optimization. In the algorithm
#below, integer ${macro_byte_prior} records not merely the byte index pair
#of the prior object but such object's *TYPE*, which must be conditionally
#checked at certain points. So, there we have it: boolean bits to go.
#
#Ugh. Actually, we do appear to need at least three boolean values,
#justifying the above allocation of two bits for such values. What's the
#third such value? Here's a full list of all currently required bits:
#
#* "00", signifying a macro. The upper bits signify the byte index pair.
#* "01", signifying a block argument. The upper bits signify the byte
#  index pair.
#* "10", signifying a terminating block argument. The upper bits signify
#  only the byte index of such terminating block argument -- in whichever
#  of the upper or middle 31 bits is more efficient to access. Oh! Right.
#  For sorting purposes, we *MUST* use the upper 31 bits for this. 
#
#Yay! We no longer require boolean bits, as we've excised ${macro_byte_prior}
#entirely. Given that, it's (more or less) trivial to simply test string bytes
#instead to determine object types.
#
#Yes, we definitively need to record the positions of block arguments as a
#"{"- and "}"-delimited conglomerate pair *AND* of only the latter such
#delimiter. We require the former for expansion purposes, as we need to
#know what to convert into a string. We require the latter for detection
#purposes, because we otherwise have no means of detecting the depth to
#which such objects are nested within other objects.
#
#Naturally, this implies that the second parse phase must now also record
#the byte indices of individual terminating "}" delimiters. Simple.
#
#O.K.; integer precision depends on the machine but should always either
#be 64 or 32 bits. As documented under "man zshmisc":
#
#"For integers, the shell is usually compiled to use 8-byte precision
# where this is available, otherwise precision is 4 bytes. This can be
# tested, for example, by giving the command
# `print - $(( 12345678901 ))'; if the number appears unchanged, the
# precision is at least 8 bytes."
#
#This can thus be tested as follows:
#
#    #FUXME: Define new function :int.set_system_bit_width() in the main
#    #codebase implementing such logic. (Yay!)
#    integer int_is_64_bit=$(( (1 << 31) > 0 ))
#
#Now, finally, the generalized solution to be applied in the third phase:
#
#* Declare new integer locals:
#  * ${macro_byte_offset_stack_index}, the current index into the stack
#    below. Initially set to 1, implying the default index. Actually, just
#    name such integer ${macro_depth} -- as that's exactly what it is.
#* Declare new list locals:
#  * ${macro_byte_offset_stack}, listing the number of bytes by which to
#    offset all lexically subsequent byte indices of outer macros (defined
#    as macros with last byte indices exceeding that of
#    ${macro_byte_prior}). Actually, perhaps name such list
#    ${macro_depth_to_byte_offset}. It's *NOT* a stack under current usage.
#    It does exactly what the new name reflects: it maps from deptch
#    indices to byte offsets. Simple.
#* On iterating to the next object:
#  * If such object is a terminating "}" delimiter, then:
#    * Increment ${macro_byte_offset_stack_index} by 1. (This is basically
#      the only purpose of recording such delimiters.)
#    * Immediately continue to the next object. Do *NOT* set
#      ${macro_byte_prior} to such object's byte index pair, please.
#  * Else if such object is a block argument, then (in order):
#    . Set ${macro_byte_offset_stack[
#          $(( macro_byte_offset_stack_index - 1 ))]} to
#          ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}.
#    . Set ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]} to
#      0.
#    . Increment ${macro_byte_offset_stack_index} by -1. (For very minor
#      efficiency, do this afterwards.)
#    . Continue onward to expansion and glory!
#* To expand either a macro *OR* block argument (in order):
#  . Increment such object's *LAST* byte index by
#    ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}.
#    Its first byte index remains unchanged.
#  . Expand such object in a type-specific manner, using such byte indices.
#  . Increment ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}
#    by the difference of such object's prior length (as provided by its
#    recorded byte index pair) and replacement length.
#
#Hmm. Do we honestly still require ${macro_byte_prior}? That seems
#increasingly superfluous; we only test it in one place above.
#
#O.K.; great! We've excised ${macro_byte_prior} and all (or most, anyway)
#redundancies above, producing what would appear to be a very concise,
#self-consistent algorithm. Let's give her a go, shall we?
#
#That's it. Hurrah, hurrah! The above algorithm appears to smoothly scale to
#*ALL* nesting depths, which is certainly a relief.
#FUXME: O.K.; so, the above solution is actually fairly shoddy for a number of
#reasons. For one, we don't actually need or even want to unconditionally
#expand all nested macros. We only want to expand nested macros that survive
#the expansion process. If one considers the typical macro accepting two or
#more block arguments, it's fair to say that most nested macros will *NOT*
#survive expansion and hence should be ignored rather than unconditionally
#expanded before expansion.
#
#So, how we do effect that? Retain the existing algorithm in full, with the
#following fairly minor tweaks:
#
#* Wrap all *THREE* preprocessor phases in stack-based iteration (a while loop,
#  perhaps?). This should be the *ONLY* stack-based iteration in the entire
#  algorithm, behaving as follows:
#  * Declare a new list local ${ZESHY__MACRO_BYTES_PREPROCESS_FIRST_LAST} (or
#    some such). As with existing list locals, such list consists *ONLY* of
#    consecutive pairs of first and last byte indices for substrings of the
#    passed code to be preprocessed in reverse lexical order. (That last
#    constraint is particularly critical; as elsewhere, iterating in reverse
#    lexical order ensures we don't have to laboriously maintain byte offsets).
#    Note, however, that since we should already be iterating in such order,
#    merely appending to the end of such list suffices to guarantee such order.
#  * Initialize ${ZESHY__MACRO_BYTES_PREPROCESS_FIRST_LAST} before iteration to
#    the byte length of such code: e.g.,
#    ZESHY__MACRO_BYTES_PREPROCESS_FIRST_LAST=( 1 ${#ZESHY__PREPROCESS_CODE} )
#  * Iterate such list in a manner respecting the fact that such list will be
#    updated in-place during iteration: e.g.,
#    integer ZESHY__MACRO_BYTE_PREPROCESS_FIRST
#    integer ZESHY__MACRO_BYTE_PREPROCESS_LAST
#    while (( ${#ZESHY__MACRO_BYTES_PREPROCESS_FIRST_LAST} )) {
#        # At the very beginning of such iteration, pop the top pair of byte
#        # indices off such stack.
#        ZESHY__MACRO_BYTE_PREPROCESS_FIRST=${ZESHY__MACRO_BYTES_PREPROCESS_FIRST_LAST[1]}
#        ZESHY__MACRO_BYTE_PREPROCESS_LAST=${ZESHY__MACRO_BYTES_PREPROCESS_FIRST_LAST[2]}
#        ZESHY__MACRO_BYTES_PREPROCESS_FIRST_LAST[1,2]=()
#
#        # Inject the three existing preprocessor phases here...
#    }
#    Note also that while the above popping scheme should work, it would
#    probably be more efficient to simply keep appending to such list without
#    removing already visited list items. (Not terribly critical, either way.)
#  * Immediately after expanding a macro in-place into the passed code, append
#    the first and last byte indices of the resulting expansion in such code
#    to the end of ${ZESHY__MACRO_BYTES_PREPROCESS_FIRST_LAST}. (O.K.; so we're
#    actually employing such list as a queue rather than stack.) Since the
#    third phase iterates in reverse order, appending onto such list implicitly
#    preserves reverse order, ensuring the first list item will remain the
#    indices of the last expansion in such code. Yay!
#
#And... that's it. Extraordinarily simpler, isn't it? A shame we didn't
#stumble onto this technique sooner, but later is better than never.

            #FUXME: Append the first and last byte indices of the resulting
            #expansion in such code to the end of
            #${ZESHY__PREPROCESS_CODE_BYTES_FIRST_LAST}. Since the third phase
            #iterates in reverse order, appending onto such list implicitly
            #preserves reverse order, ensuring the first pair of list items
            #will remain the indices of the last expansion in such code. Yay!

        #FUXME: Refactor to iterate list indices rather than popping items.
    # while (( ${#ZESHY__MACROS_BYTES_FIRST_LAST} )) {
        # Pop the top pair of byte indices off such stack.
        # ZESHY__MACRO_BYTE_FIRST=${ZESHY__MACROS_BYTES_FIRST_LAST[${ZESHY__MACROS_BYTES_FIRST_LAST_INDEX}]}
        #  ZESHY__MACRO_BYTE_LAST=${ZESHY__MACROS_BYTES_FIRST_LAST[$(( ${ZESHY__MACROS_BYTES_FIRST_LAST_INDEX} + 1 ))]}
 
#FUXME: The current preprocessing approach replaces string slices in-place. We
#should note, however, that ={var/scalar/scalar} takes the alternative approach
#of iteratively constructing the output string from an input string without
#modifying the original. Both approaches have merit. Which is more efficient?
#Since we perform so much slicing split over numerous phases, implementing the
#latter would be quite difficult (if not infeasible). Nonetheless, consider.

        # passed the arguments passed to such macro.

        #FUXME: This could use a bit of massaging. We currently map macro to
        #function names in two maps:
        #${ZESHY__MACRO_NAME_SANS_SEMICOLON_PREFUX_TO_FUNC_NAME} and
        #${ZESHY__MACRO_NAME_WITH_SEMICOLON_PREFUX_TO_FUNC_NAME}. Clearly, this
        #approach is suboptimal here. For all internal uses, we only want a
        #single centralized map ${ZESHY__MACRO_TO_FUNC_NAME}. Since the current
        #distinction between semicolon-prefixed and non-semicolon-prefixed
        #macro names only requires a list, do the following:
        #
        #* Replace ${ZESHY__MACRO_NAME_SANS_SEMICOLON_PREFUX_TO_FUNC_NAME} and
        #  ${ZESHY__MACRO_NAME_WITH_SEMICOLON_PREFUX_TO_FUNC_NAME} with a
        #  single map ${ZESHY__MACRO_TO_FUNC_NAME}.
        #* Create two new list globals
        #  ${ZESHY__MACRO_NAMES_SANS_SEMICOLON_PREFUX} and
        #  ${ZESHY__MACRO_NAMES_WITH_SEMICOLON_PREFUX}, used in lieu of the
        #  prior two maps. Alternately, such lists are probably constructable
        #  on the fly by dynamically grepping ${ZESHY__MACRO_TO_FUNC_NAME}
        #  (e.g., "${ZESHY__MACRO_TO_FUNC_NAME:#:*}" or some such), which may
        #  ultimately prove to be the superior method. Contemplate. Hmm; no.
        #  While such dynamism simplifies logic elsewhere (e.g., on macro
        #  undefinition), it considerably complicates PCRE construction
        #  (consider stripping the prefixing ":" from all such macro names) as
        #  well as reducing the efficiency thereof. Since the latter is much
        #  more of a concern than the former, adopt the 
        #  ${ZESHY__MACRO_NAMES_SANS_SEMICOLON_PREFUX} and
        #  ${ZESHY__MACRO_NAMES_WITH_SEMICOLON_PREFUX} approach.

    #FUXME: Don't forget to strip suffixing line continuations (i.e.,
    #"\"-prefixed newlines) from macro arguments before calling such macros.
    #Efficiently straightforward using a simple global glob replacement. 
    #FUXME: Actually, we pretty much *HAVE* to call eval() to ensure proper
    #expansion of standard arguments. (Consider process substitutions embedded
    #in such arguments, for example.) Simplifies things, anyway. Note that
    #we'll need to apply at least parameter expansion flag "(q)" to each block
    #argument. (Probably "(qq)"). *shrug*

    #FUXME: Any optional horizontal whitespace suffixing a macro expansion
    #should either be:
    #
    #* *NOT* matched. This is the preferable approach, but requires careful
    #  PCRE design.
    #* Explicitly captured into a new match group, *NOT* passed to such macro's
    #  function, and then explicitly appended onto the returned result.
    #* Explicitly matched by the PCRE but ignored here, in which case *ALL*
    #  macro expansions would need to be unconditionally suffixed by one
    #  horizontal whitespace character.
    #
    #Why? *NOT* because of command terminators (e.g., "<", ">", "|"), as such
    #terminators clearly require no prefixing whitespace. Instead, because of
    #zeshy-specific command terminators (e.g., ":or", ":and"), which *DO*
    #require prefixing whitespace. Let's see if we can avoid matching such
    #whitespace as a first attempt, shall we?

#FUXME: Consider dropping pcre_study() everywhere. It's unlikely such call does
#anything for us, either here or elsewhere. Examine the "libpcre" documentation
#closer for confirmation, please.

        #Since we already have PCREs matching strictly supported
        #here-documents, the following should suffice:
        #
        #* Define a new string global (based on existing PCREs)
        #  ${ZESHY__MACRO_HERE_DOC_UNSUPPORTED}. Such PCRE should be prefixed
        #  by "<<[^<]".
        #* If an "invalid" construct is detected by the above approach:
        #  * If the matched substring matches
        #    ${ZESHY__MACRO_HERE_DOC_UNSUPPORTED}, such construct is an
        #    unsupported here-document. Throw an appropriate exception.
        #  * Else, such construct is genuinely invalid. Again, throw an
        #    appropriate exception.

        # If a halting condition was *NOT* matched, a macro must have matched.
        # if [[ -z ${match\
            # [${ZESHY__MACRO_GROUP_INDEX_CODE_VALID_CHAR_LAST_OR_WASTELANDS}]-} ]] {

            #FUXME: Report this bug! This is currently affecting *EVERYONE*
            #leveraging zsh module "zsh/pcre", which is probably *EVERYONE*.

        #FUXME: *ALL* macros are inherently valid, as syntactic invalidities in
        #zsh-specific code are simply ignored now.
        #FUXME: Insufficient, as this fails to distinguish syntactically valid
        #macros accepting no arguments from syntactically invalid macros
        #accepting arguments. Can we actually distinguish between the two? The
        #answer is a resounding "yup." By fortuitous design, PCRE actually
        #captures groups matched in positive lookahead! This means that we can
        #amend the current PCRE with positive lookahead immediately following
        #the matched macro name. Such lookahead must be made optional, and
        #should attempt to look ahead for one or more horizontal whitespace
        #characters followed by exactly one non-whitespace character: e.g.,
        #    local pcre_='(?=('${pcre_spaces}'\S)?)'
        #If such captured group is nonempty *AND* the empty group signifying
        #such macro to be syntactically valid was *NOT* matched, then and only
        #then have we positively identified such macro to be invalid. Sweet!
        #FUXME: Actually, we've now solved this *WITHOUT* lookahead of any
        #sort. Testing for the nonemptiness of the last match group (if such
        #group exists) now suffices. Though, due to intermediate here-document
        #matches, how do we test for such index? We may have no choice but to
        #segregate here-documents to a subroutine. Ah! Right. We can *FORCE* a
        #group to be created for syntactically invalid macros even in the
        #absence of such macros by appending the entire PCRE by an empty group
        #that always unconditionally matches. Reasonably clever, no?
        #FUXME: The above is substantially correct but fails to take into
        #account the fact that we prematurely return on matching syntactically
        #valid macros, implying that, in such case, even such empty group will
        #fail to be matched. Fortunately, this is easily circumventable with
        #logic. The necessary and sufficient conditions for a syntactically
        #invalid macro are as follows:
        #
        #* A macro name was matched. (Uhh; that's *ALWAYS* the case, right?)
        #* ...and, erhm. No, we guess this approach won't work as defined. Hm.

    #phase 2 iteration currently only matches 
    #Alternately, do we *REALLY* need to split such arguments? Would it instead
    #suffice to merely quote protect the entire thing with "(qq)" without the
    #"(z)"-driven word splitting? Unsure. Hmmm; while macro functions could be
    #required to do such work, of course, it seems likely that they'd pretty
    #much *ALWAYS* want word-split arguments, right? Contemplate.
    #
    #Yes, absolutely. We need word-split arguments. This is a fairly hard
    #requirement. Fortunately, as the above example demonstrates, this is
    #easily and efficiently doable.

#${pcre_space_horizontal_or_backslash_or_newline_or_string_end}
            # ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST=$(( ${ZPCRE_OP[(w)1]} + ${#match[1]} ))
            #Naturally, ${pcre_space_horizontal_or_line_continuation_or_newline_or_string_end}
            #is a terrible name; but you get the idea. Whatever the name,
            #concocting such PCRE should be trivial. Note that while capturing
            #positive lookahead *SHOULD* work, we'll probably want to verify
            #this externally at the CLI.

            # The last character preceding such match is either the last
            # character of such macro's name or last standard argument passed to
            # such macro *OR* the "}" delimiting the last block argument passed to such macro. In the former two cases, such character may be UTF-8-encoded and hence multibyte. Given that,

            #FUXME: Right. It's *NOT* simply macro names that may be
            #UTF-8-encoded; it's unquoted standard arguments as well. Hence, we
            #have little alternative but to reimplement this in a
            #multibyte-aware manner. See ={var/scalar}.

            #FUXME: Avoid iterating past the end of such list here! That should
            #probably be the first check, we should think.

            # Byte index of the first character of the next block argument if any
            # or such character constant otherwise.

        #FUXME: Drop the use of terminating character constant ";". Instead,
        #simply iterate block arguments until the first byte index of the
        #current block argument exceeds the last byte index of the current
        #macro, in which case such iteration should continue to the next macro.

        # Append an arbitrary non-positive integer constant terminating the
        # sublist of block argument byte indices for such macro. (Hacks "R"
        # Us.)
        # ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST+=0

            # If such index is *NOT* the terminating constant (i.e., zero),
            # proceed.
            # (( ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST ))

        #FUXME: This and the following check are *GREAT*, but not really
        #appropriate here. Instead, shift such logic down to phase 3, where we
        #actually call such function.

    # Name of the function expanding such macro.
    # local ZESHY__MACRO_FUNC_NAME

        # Names of the function expanding such macro.
        # Function call replacing such macro in such code by its expansion.
        # ZESHY__MACRO_COMMAND=${ZESHY__MACRO_FUNC_NAME}

        # Since such function's name is such macro's name, begin such call with
        # the latter.

    #FUXME: Actually, couldn't we apply the presumably less expensive parameter
    #expansion flag "(e)" *AFTER* stripping line continuations and performing
    #shell word splitting with parameter expansion flag "(z)"? Although, at
    #that point, simply calling eval() is probably more efficient.
    #
    #Note that, if we do take the eval() route, we'll need to apply at least
    #parameter expansion flag "(q)" to each block argument. (Probably "(qq)").

            # 'Macro function '${ZESHY__MACRO_FUNC_NAME}'() undefined.'

    # List of byte indices of the first characters of parsed constructs in
    # reverse lexical order (i.e., reverse of the order such constructs
    # appear in the passed code). There exist three types of constructs:
    #
    # * Macros, comprising such macros' name and all arguments passed to such
    #   macros.
    # * Block arguments, comprising all text inclusive between such arguments'
    #   prefixing "{" and suffixing "}" delimiters.
    # * Block arguments' "}" delimiters, comprising only such characters.
    #
    # The second phase creates such list; the third phase iterates such list,
    # circumventing the current inability to nest PCRE-based iterations in zsh.
    #
    # Listing byte indices for both block arguments as a whole *AND* the "}"
    # characters delimiting such arguments permits the code iterating such list
    # to detect and respond to changes in macro nesting depth (i.e., the number
    # of macros in which the current construct is nested).
    #
    # For each byte index of the first character of a parsed macro or block
    # argument, the corresponding byte index of the last character of such
    # construct is available from map ${ZESHY__MACRO_BYTE_FIRST_TO_LAST}.
    # local -a ZESHY__MACRO_BYTES_FIRST

    # Map from the byte index of the first character of each parsed macro and
    # block argument to the byte index of the last character of such construct.
    #
    # If such construct is a:
    #
    # * Macro:
    #   * The first byte index is the index of the first character of such
    #     macro's name.
    #   * The last byte index is:
    #     * If one or more arguments were passed to such macro, the last
    #       character of the last such argument.
    #     * Else, the last character of such macro's name.
    # * Block argument:
    #   * The first byte index is the index of the "{" prefixing such argument.
    #   * The last byte index is the index of the "}" suffixing such argument.
    # * Block argument "}" delimiter, the first *AND* last byte indices are the
    #   index of such delimiter.
    #
    # Since block argument "}" delimiters comprise only a single ASCII
    # character and hence byte, the first and last byte indices for such
    # delimiters are guaranteed to be the same. For efficiency, this map omits
    # such identity mappings.
    # local -A ZESHY__MACRO_BYTE_FIRST_TO_LAST

    # 1-based index into list ${ZESHY__MACRO_DEPTH_TO_BYTE_OFFSET}, signifying
    # the current block argument depth (i.e., number of block arguments in
    # which the currently parsed construct is nested).
    # integer ZESHY__MACRO_DEPTH=1

    # Map from block argument depth to parent byte offset (i.e., number of
    # bytes by which the last byte index of constructs containing the current
    # construct should be offset). Since such depth is guaranteed to be
    # 1-based, such map is implemented as a list for efficiency.
    # local -a ZESHY__MACRO_DEPTH_TO_BYTE_OFFSET

    #FUXME: Guard against reentrancy; that is, throw an exception if the
    #current function is already on the function call stack. See :die().

#FUXME: Arguably shift to a new parcel ={code} devoted to such operation.

#FUXME: There exists one possible large-scale optimization remaining -- but
#it's only an optimization and not at all required for proper algorithm
#iteration. Rather than expanding in-place into the passed code, it could
#conceivably be more efficient to 

    #FUXME: The current approach basically doesn't work on systems providing
    #only 32-bit integers. Why? Because subdividing 32 bits in half only
    #permits us to index zeshy scripts at most 16KB in size. (Think
    #"WASTELANDS" on old zeshy scripts.)
    #
    #For now, this is more-or-less fine. As "man zshmisc" documents:
    #
    # "For integers, the shell is usually compiled to use 8-byte precision
    #  where this is available, otherwise precision is 4 bytes."
    #
    # It's difficult to envision a modern Linux system providing integer
    # precision of at most 4 bytes. That said, when the first bug reports
    # inevitably hit, we're going to need an alternate edge-case approach *NOT*
    # employing the current bit-packing approach. In other words, on systems
    # supporting only 32-bit integers, we're basically going to have to
    # reimplement the second and third phases below with a very different
    # approach. In reflection, actually, perhaps we just want to do the
    # following straight-away? Consider:
    #
    # * Define a new list local ${ZESHY__MACRO_BYTES_FIRST}, containing only
    #   first byte indices and sorted in descending order.
    # * Define a new map local ${ZESHY__MACRO_BYTE_FIRST_TO_LAST}, mapping from
    #   the former to the latter for macros and block arguments but *NOT* "}"
    #   delimiters. (Which, incidentally, is an efficient test of whether a
    #   construct is such a delimiter or not.)
    #
    # That's it. Fairly obvious, as well. In hindsight, it seems unfortunate we
    # didn't come up with this first. Always have to leap to the clever, hacky
    # solution first, don't we? *shakes head dolefully*

    # First and last byte indices are as follows:
    #
    # * For macros, 

# Bit masks for obtaining the first and last byte indices from a single integer
# into which such indices have been bit-packed.
# integer -g\
#     ZESHY__MACRO_BYTE_INDICES_FIRST_MASK\
#     ZESHY__MACRO_BYTE_INDICES_LAST_MASK

# Number of bits by which to shift first byte indices right by to bit-pack such
# indices into a single integer.
# integer -g ZESHY__MACRO_BYTE_INDICES_FIRST_SHIFT_COUNT

# ....................{ MAIN                               }....................
# Define system-specific globals required by the above preprocessor.
# () {
#     # If the current system supports 64-bit integers, set such globals
#     # accordingly. See :int.set_system_bit_width().
#     if (( (1 << 31) > 0 )) {
#         ZESHY__MACRO_BYTE_INDICES_LAST_MASK=0xFFFFFFFF
#         ZESHY__MACRO_BYTE_INDICES_FIRST_SHIFT_COUNT=32
#     # Else, zsh guarantees the current system to support 32-bit integers.
#     # Again, set such globals accordingly.
#     } else {
#         ZESHY__MACRO_BYTE_INDICES_LAST_MASK=0xFFFF
#         ZESHY__MACRO_BYTE_INDICES_FIRST_SHIFT_COUNT=16
#     }
#
#     # Implicitly compute rather than explicitly specify such mask (e.g.,
#     # "0xFFFFFFFF00000000"). Unfortunately, zsh truncates explicitly
#     # specified 16-digit hexadecimal integers to 15 digits with an error
#     # resembling:
#     #
#     #     >>> ZESHY__MACRO_BYTE_INDICES_FIRST_MASK=0xFFFFFFFF00000000
#     #       zsh: number truncated after 15 digits: 0xFFFFFFFF0000000
#     #
#     # Fortunately, such questionable behaviour is circumventable by
#     # implicitly computing such values.
#     ZESHY__MACRO_BYTE_INDICES_FIRST_MASK=$((
#         ZESHY__MACRO_BYTE_INDICES_LAST_MASK <<\
#         ZESHY__MACRO_BYTE_INDICES_FIRST_SHIFT_COUNT ))
# }

    # List of bit-packed byte index pairs for all parsed constructs, of which
    # there exist three types:
    #
    # * Macros, comprising such macros' name and all arguments passed to such
    #   macros.
    # * Block arguments, comprising all text between such arguments' prefixing "{"
    #   and suffixing "}" delimiters.
    # * Block arguments' "}" delimiters, comprising only such arguments'
    #   suffixing "}" delimiters.
    #
    # Listing byte indices for both block arguments as a whole *AND* for only
    # such arguments' "}" delimiters permits the code iterating such list to
    # detect and respond to changes in the macro nesting depth (i.e., the
    # number of macros in which the current construct is nested).
    #
    # Each list item is an integer whose bits are subdivided such that:
    #
    # * The upper half of such bits signify the first byte index at which such
    #   construct begins in the passed code. If such construct is a:
    #   * Macro, this is the index of the first character of such macro's name.
    #   * Block argument, this is the index of the "{" prefixing such argument.
    #   * Block argument's "}" delimiter, this is the index of such delimiter.
    # * The lower half of such bits signify the last byte index at which such
    #   construct ends in the passed code.
    #   * Macro, this is the index of:
    #     * If one or more arguments were passed to such macro, the last
    #       character of the last such argument.
    #     * Else, the last character of such macro's name.
    #   * Block argument, this is the index of the "{" prefixing such argument.
    #   * Block argument's "}" delimiter, this is 0. (Since such construct
    #     consists of only a single ASCII character, there exists no compelling
    #     reason to repeat such index in both halves of such integer). 
    #
    # The second phase creates such list; the third phase iterates such list,
    # circumventing the current inability to nest PCRE-based iterations in zsh.
    #
    # To sanitize algorithm behaviour, each such pair of byte indices should
    # be iterated in reverse lexical order (i.e., the reverse of the order
    # that the corresponding constructs appear in such code). Hence, such list
    # must be sorted in descending order while still preserving the pairing
    # between the first and last byte indices of all parsed constructs.
    #
    # In a conventional high-level language, such indices would be subsumed
    # by a class, structure, or related composite datatype. Since zsh does
    # *NOT* support type composition, alternative approaches must be
    # considered. If such list did *NOT* require sorting, simply appending each
    # first and byte index (e.g., as alternating integer list items or
    # ":"-delimited string items) would suffice to preserve such pairings.
    # Since such list does require sorting, the only seemingly feasible
    # solution is low-level bit packing: that is, packing each pair of first
    # and last byte indices into a single integer list item. To sort each pair
    # by its first byte index, ensure such index takes arithmetic precedence
    # over the last byte index of such pair by bit-packing the former into the
    # upper half of each list item.
    #
    # Unfortunately, there exists a caveat: finite limits. Subdividing a 64-bit
    # integer into two 32-bit halves still permits indexing of zeshy scripts at
    # most 4GB in size -- exceeding most sane size limits. Subdividing a 32-bit
    # integer into two 16-bit halves, however, only permits indexing of zeshy
    # scripts at most 16KB in size -- a considerably less sane size limit.
    # local -a ZESHY__MACRO_BYTES

#. by offering the
    # expected sort order
    # In a conventional high-level language, each pair of such indices would
    # be subsumed by a class, structure, or similar composite datatype. Since
    # zsh does *NOT* 

    # However, sorting  list
    # while preserving the pairing between the first and last byte indices for
    # parsed constructs
    #
    # To guarantee the third phase iterates

#    :func :void :outer() {
#    } {
#        :func :void :inner() {
#            print "Me! It's me!"
#        }
#    } <<'/---
#    /---
#
#In this case, such inner macro invalidates only the last byte index of the
#last block argument of the outer macro. As generalization:
#
#* 
#
#Specifically:
#
#* Declare a new list local ${stack} serving as a LIFO stack.

    #* After converting all block arguments for the current macro:
    #  * If such macro's last byte index is strictly greater than
    #    ${macro_byte_prior_last}, increment only such macro's last byte index
    #    by ${macro_byte_offset}.
    #    . Increment such object's *LAST* byte index by
    #      ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}.
    #  . Add ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]} to
    #    such object's *LAST* byte index to obtain its new last byte index.
    #  . Set ${macro_byte_prior} to such object's original byte index pair.
    #  * Else if such object's *LAST* byte index is strictly greater than the
    #    last byte index of ${macro_byte_prior} (which, in consideration,
    #    should imply such object to be a block argument), then (in order):
    #    . Increment such object's *LAST* byte index by
    #      ${macro_byte_offset_stack[${macro_byte_offset_stack_index}]}.

    #  * ${macro_byte_prior}, the byte index pair of the most recently expanded
    #    macro preceding the current macro. A stack should *NOT* be required
    #    here, if our meager analysis is correct.

# Before expanding either a macro *OR* block argument (...although, :
    #So, we make tests of whether a byte index pair refers to a block argument
    #or not tests against 0 and hence more efficient than

#    . Prefix *ALL* such contents by the following line:
#      "${general_alias_definitions}
#      "func ::script.code__${random_integer}() {"
#    . Suffix *ALL* such contents by the following lines:
#      "}
#      ::script.code__${random_integer}", where:
#      * ${random_integer} is a random integer guaranteeing the uniqueness of
#        such

#FUXME: Or perhaps not? The prior solution appears to work quite well, now.

 # Since the former is a non-integer
    # character constant on iterating past the last block argument passed to
    # such macro, declare such indices to be strings rather than integers.
    #Solving such issue can be split into two parts:
    #
    #1. Detecting such 

            # ZESHY__MACRO_BYTE_ARG_BLOCK_LAST=${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST[$((
            #     ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST_INDEX + 1 ))]}

        # for ZESHY__MACRO_BYTE_ARG_BLOCK_FIRST ZESHY__MACRO_BYTE_ARG_BLOCK_LAST (
        #     "${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST[,-1]}") {

        #FUXME: Implement me.

        # If such macro was passed a here-document or -string on standard
        # input, pass such input to such call as a single-quoted here-string.
        #FUXME: Correct me. We probably want to embed new byte indices
        #corresponding to the first and last byte indices of such here string.
        #The latter is simply ${ZPCRE_OP[(w)2]}; the former is the latter minus
        #the byte length of such match.
        #FUXME: Alternatively, perhaps we should simply copy such match into a
        #string list item of the same list. Since empty standard input is
        #indistinguishable from no standard input, perhaps this is the ideal
        #approach. It certainly seems simpler, if slightly less efficient. (Or
        #perhaps not, since we'll be taking a string slice and hence copy
        #anyway later if not here?)
        #FUXME: This necessitates modifying ${ZESHY__MACRO_BYTE_LAST} above to
        #strictly precede such byte indices, implying such integer should be
        #renamed to ${macro_byte_args_last}.

        # If such macro was passed a here-document or -string on standard
        # input, record such string for the third preprocessor phase.
        # if [[ -n ${ZESHY__PREPROCESS_CODE[
        #     ${ZESHY__MACRO_GROUP_INDEX_HERE_DOC_OR_STRING}]} ]] {
        #     macro_byte_here_first=
        #     macro_byte_here_last=
        # }

        # If such macro was passed a here-document or -string on standard
        # input, record such string for the third preprocessor phase.
        # if [[ -n ${ZESHY__PREPROCESS_CODE[
        #     ${ZESHY__MACRO_GROUP_INDEX_HERE_DOC_OR_STRING}]} ]] {
        #     ZESHY__MACRO_BYTES_NAME_FIRST_LAST[1]+=( )
        # }

  # delimited by
            # "'" characters and properly
            # internally escaped).
# Hence, we need not add such
            # whitespace manually.
    # Byte indices of the first and last characters of the previously matched block argument
    # passed to such macro  if any or the first character following the last
    # character of such macro's name otherwise.

    #FUXME: Still require this? Excise us!

    # Byte index into such code.
    # integer macro_byte

            # Since the character preceding such match is guaranteed to be a non-UTF-8 whitespace character, incrementing such byte
            # index by 1 is safe.

            # Since the character preceding such block argument is guaranteed
            # to be a non-UTF-8 whitespace character, incrementing such byte
            # index by 1 is safe.

        # to the last character of the last argument passed to such macro.
#FUXME: O.K.; so, implementing a zsh-capable in pure-PCRE is rather difficult
#if not infeasible. For example, consider the complexity of merely
#differentiating the following edge cases:
#
#:func ':void yum()' {
#    print ${:-
#    :func
#    }
#    { print $(:stdin.get) } <<'o''k'
#:func
#o'k
#}
#
#Things get *VERY* tricky, *VERY* rapidly. Note the fact, for example, that
#quoted here-doc words may contain quote escapes that then need to be matched
#as their unescaped equivalents, which isn't really feasible to accomplish with
#simplistic PCRE-based group capturing and back referencing.
#
#So, here's what we're going to do for now (and possibly always): we're going
#to implement a genuine preprocessor in the sense that preprocessing will be a
#distinct phase performed strictly *BEFORE* zsh interpretation. In the above
#examples, such preprocessor would treat all macros ":func" regardless of zsh
#context the same. While non-ideal, there's just no feasible means of
#reimplementing robust zsh parsing with PCRE-based iteration.

#FUXME: Uh-oh! How do we perform nested PCRE-based matching? We need to
#perform iterative PCRE matching here to reliably match macros and, for each
#such macro, call such macro's preprocessor -- which itself could (and at
#least in the case of :func() absolutely will) also perform iterative PCRE
#matching. In a conventional language, this would be no problem. Here, however,
#we have a problem.
#
#While we *COULD* probably mitigate this by performing glob-based iteration
#here instead, globs are unwieldy, inefficient, and ultimately infeasible.
#Consider matching here-documents without back references, for example.
#
#Technically, we could simply call pcre_compile() *AFTER* calling each and
#every macro preprocessor.
#FUXME: O.K.; the solution, clearly, is to split preprocessing into two phases:
#
#1. In the first phase, PCRE-based iteration finds *ALL* macros to be replaced
#   in the second phase. Specifically, for each macro, such iteration appends
#   the byte indices of (in order):
#   * The first character of such macro (e.g., the ":" prefixing such macro).
#   * For each block argument passed to such macro (i.e., "{"- and
#     "}"-delimited block of user-defined code):
#     * The first character of such argument (i.e., such "{" delimiter).
#     * The last character of such argument (i.e., such "}" delimiter).
#   * The last character of such macro (e.g., the "}" suffixing such macro),
#     prefixed by "e". Since the number of block arguments is variable, the
#     second phase needs to differentiate this index from block
#     argument-specific indices. This seems a simple way.
#2. In the second phase, list-based iteration iterates over the previously
#   appended byte indices in *REVERSE* order, ensuring macros appearing later
#   in such code will be replaced before macros appearing earlier. Rather than
#   explicitly reverse such list (which introduces other complications),
#   implicitly reverse such list by ensuring the first item of such list is
#   always the empty string and only appending immediately after such item.
#   Assuming a linked list implementation, this is presumably efficient.

    # Phase 2: call and replace all such macros by their output in such code.

    # If such list size is *NOT* a factor of three, such list does *NOT*
    # consist of byte index triples. In such case, throw an exception. (While
    # this should never be the case, an ounce of prevention...)
    # (( ${#ZESHY__MACRO_BYTES_NAME_FIRST_LAST} % 3 == 0 )) || :die
    #     'List ${ZESHY__MACRO_BYTES_NAME_FIRST_LAST} size '${#ZESHY__MACRO_BYTES_NAME_FIRST_LAST}' indivisible by 3.'

    #FUXME: Ugh. We really need the macro order of both
    #${ZESHY__MACRO_BYTES_NAME_FIRST_LAST} and
    #${ZESHY__MACRO_BYTES_ARG_BLOCK_FIRST_LAST} to be synchronized, implying we
    #should probably implement such reversal above where we append to the
    #former. (Fairly clear, in hindsight.)

    # For each previously matched macro, match each block argument passed to
    # such macro. The next preprocessor phase replaces such macros by their
    # expansions "in place." To prevent these expansions from invalidating the
    # byte indices of block arguments passed to subsequent macros, the next
    # preprocessor phase must iterate macros in reverse order. To ensure this,
    # iterate such macros in reverse order here.
    # for (( macro_byte=$(( ${#ZESHY__MACRO_BYTES_NAME_FIRST_LAST} - 2 ));
    #        macro_byte > 1;
    #        macro_byte+=-3 )) {

    #FUXME: Not quite right. We should note that zsh's inability to nest PCRE
    #compilations is an excrutiating handicap. Basically, we really need to
    #explicitly match a sufficiently large finite number of block arguments --
    #say, 16? (This can always be artificially increased as needed.) The reason
    #why, of course, is that we have no way of efficiently matching individual
    #block macro arguments with the ideal "((?:...)*)" approach permitting a
    #countably infinite number of such arguments. Hmm; actually, we *COULD* do
    #it, and probably should. To do so, however, we'll need a new phase 1.75.
    #
    #In phase 1, simply append the following integers to a list local for each
    #macro match:
    #
    #* The first byte of such macro name.
    #* The last byte of the last argument passed to such macro.
    #
    #Since the current PCRE structure readily provides such bytes, no
    #additional work should be required here.
    #
    #In phase 1.75, we'll need to iteratively find the first and last bytes of
    #each block argument passed to such macro (and append such bytes to yet
    #another list local in a manner also detailed elsewhere).
    #
    #It's hardly ideal, of course, but there is no ideal solution here. This
    #has the benefit of scaling to an arbitrary number of arguments, which is
    #blatantly preferable to the alternative -- even if it does incur minor
    #inefficiencies and code complexities along the way.
    #FUXME: Unfortunately, the above approach is complicated by the current
    #need to match macro-specific indentation prefixing the "}" delimiting such
    #block arguments, clearly infeasible with a single universal PCRE. Given
    #that, it's fairly clear that we either need to:
    #
    #1. Only match a finite number of block arguments at the same time as
    #   macros themselves are matched.
    #2. Match a countably infinite number of block arguments after macros are
    #   matched by matching the former with genuine context-free recursion.
    #
    #Obviously, one of these approaches has a bounded shelf-life and the other
    #does not. Given that, we'd might as well implement it right the first
    #time. (You know what to do.)

    #FUXME: We arguably need an intermediary validation phase -- call it
    #"Phase 1.5", perhaps. This phase's sole purpose is to validate the passed
    #code with respect to preprocessing -- that is, to ensure that all macros
    #present in such code were syntactically valid and hence preprocessed.
    #Naturally, such phase should be optimized away under optimized builds.
    #
    #To support such phase, phase 1 should append the starting byte index of
    #each syntactically valid macro to a new list local -- say,
    #${macro_valid_bytes_start}.
    #
    #In all non-optimized builds, phase 1.5 should then compile
    #${ZESHY__MACRO_NAMES_PCRE} and iterate over all substrings matching such
    #PCRE in such code. For each such match, such iteration should compare the
    #starting byte index of such match with the next list item in
    #${macro_valid_bytes_start}. If the two differ, the current macro must
    #necessarily be syntactically invalid. In such case, throw an exception and
    #immediately terminate preprocessing.
    #
    #Such validation does unavoidably introduce inefficiencies but must
    #nonetheless be performed. Due to the complexities of such PCREs, such
    #validation will be essential even for core debugging.
    #FUXME: Actually, we can combine phases 1 and 1.5 via clever PCRE
    #construction. How? By simply appending an alternative to the main
    #macro-matching PCRE matching syntactically invalid macros as any substring
    #beginning with a valid macro name. By listing such alternative *AFTER* the
    #initial PCRE matching only syntactically valid macros, we effectively
    #guarantee the former to only match invalid macros. (At least, that's the
    #theory.) Well, this puts a brighter spin on validation.

        # Begin searching for block arguments passed to the current macro from
        # the first character following the last character of such macro's name
        # to the last character of the last argument passed to such macro.
        # ZPCRE_OP=$((
        #     ${ZESHY__MACRO_BYTES_NAME_FIRST_LAST[$(( ${macro_byte} + 1 ))]} + 1
        # ))' '$((
        #     ${ZESHY__MACRO_BYTES_NAME_FIRST_LAST[$(( ${macro_byte} + 2 ))]}
        # ))

           # macro_byte < ({${#ZESHY__MACRO_BYTES_NAME_FIRST_LAST}..1}) {

    # triple of
    # previously recorderd byte indices, in
    # reverse order.

    # In the first phase, iteratively match all preprocessor macros in the
    # passed code and append the corresponding match indices to list local
    # ${ZESHY__MACRO_BYTES_NAME_FIRST_LAST}.

        # If either the byte indices of the last character of such macro and
        # such macro's name are equal, such macro is either a syntactically
        # valid macro accepting no arguments *OR* . the character following the
        # latter
        # is a newline, such macro  *AND* 
        # (( ZESHY__MACRO_BYTE_LAST == ZESHY__MACRO_BYTE_NAME_LAST )) ||
        # ${ZESHY__PREPROCESS_CODE} ]] || 

        #FUXME: Perform crude validation here.
#of the first and last non-whitespace macro characters.
        # Macro name.
        # ZESHY__MACRO_NAME=${}

        #FUXME: We probably also want to append the byte index of the last
        #character of such macro name, to prevent having to match macro names
        #in the next phase as well.

    # For each such match, append the following integers to list local
    # ${ZESHY__MACRO_BYTES_NAME_FIRST_LAST}:
    #
    # * The index of the first non-whitespace byte of such macro (e.g., the ":"
    #   prefixing such macro's name).
    # * The index of the last non-whitespace byte of such macro (e.g., the "}"
    #   suffixing such macro's last block argument).

#and
    # last bytes of such macro to a list local
    # Fortunately, locally disabling such option permits all strings (multibyte
    # or not) and hence such substring to be indexed by such byte indices.

    #   * The first two phases *ALWAYS* perform PCRE-based iteration.
    #   * Phase 2 *MAY* perform PCRE-based iteration, conditionally depending
    #     on whether the macros such phase calls do so. (Notably, the :func()
    #     macro does so.)
    #
    # Why? Subtleties! Consider the following causal chain of logic.
    #
    # * Since the first two phases *ALWAYS* perform PCRE-based iteration.
    #   * zsh currently only permits one PCRE to be compiled at a time,
    #     directly implying PCRE-based iteration cannot be nested.
    # * Such phases cannot be combined into a single phase.

# :void :code.preprocess(^:string code_name)
    # (( # == 1 )) || :die 'Expected one string name.'
    # local code_name__cp=${1} code__cp

    # For efficiency, cache such code.
    # code__cp=${(P)code_name__cp}

    # Set such code.
    # :string.set "${code_name__cp}" "${code__cp}"

#FUXME: Ugh. Shift string setters and type checkers here we suppose. Let's
#assume we've done that, for now.

    #FUXME: For readability, each line of such replacement should be manually
    #by the exact amount of indentation prefixing each such macro. This will
    #probably require matching such indentation into a new list local -- say,
    #${macros_indentation}.
    #FUXME: Wait. Forget this. For what "readability"? zsh forcefully reindents
    #all internal code, anyway.
