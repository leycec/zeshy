#!/usr/bin/env zsh
# --------------------( LICENSE                            )--------------------
# Copyright 2007-2015 by Cecil Curry.
# See "LICENSE" for additional details.

:parcel <<'/---'
Handle core *URL* (i.e., Uniform Resource Locator) functionality.
/---

# ....................{ CANONICALIZERS                     }....................
#FIXME: The "wget"-based implementation is currently broken! See below.

# If:
#
# * "curl" is pathable, prefer such command, which (unsurprisingly) provides a
#   considerably simpler method of canonicalizing URLs than "wget".
# * "wget" is pathable, fallback to such command.
:declare_func_if_pathable_or_noop\
    ':void :canonicalize_url(:str:str string_name)'\
    ':str url_name__cu="${1}"'\
    curl '
        :Str.set "${url_name__cu}" "$(command curl\
            --head\
            --location\
            --show-error\
            --silent\
            --output /dev/null\
            --write-out ''%{url_effective}''\
            "${(P)url_name__cu}")"'\
    wget '
        :str page_headers__cu

        # Capture all pager headers sent by the webservers serving such URL and
        # all . Dismantled, this is:
        #
        # * "--max-redirect=...", resolving nearly arbitrarily many redirections
        #   (rather than failing at the 21st redirection).
        # * "--output-document=-", outputting to standard output (to be captured
        #   by such process substitution).
        # * "--spider", retrieving only page headers (rather than page content).
        # * "--tries=1", failing if the first attempt fails (rather than
        #   retrying 20 times).
        # * ":to_stdout", redirecting standard error to output. Nonsensically,
        #   "wget" always prints such headers to standard error -- and provides
        #   no means of altering such behavior. *sigh*
        page_headers__cu="$({
            command wget\
                --max-redirect=999999\
                --output-document=-\
                --spider\
                --tries=1\
                "${(P)url_name__cu}"
        } :to_stdout)"

        #FIXME: Actually implement
        #:set_string_to_string_line_last_matching_pcre_group_if_found().
        #Wonder how we''ll do that, incidentally? Two methods, as we see it:
        #
        #1. Split such string on newlines into a list and search such list from
        #   the *END* for the first matching line.
        #2. Iteratively apply such PCRE to such list from the *BEGINNING*. The
        #   last captured group if any is the desired group.
        #
        #O.K.; clearly, on arbitrarily large strings, the former solution is
        #(probably) the most efficient. Although the storage costs are
        #significantly higher, we don''t expect to ever apply such function to
        #grossly corpulescent strings. In such case, iteratively searching from
        #the beginning of a large string merely to ignore most such matches
        #seems particularly inefficient. Besides, the former solution is
        #modestly clever... modestly.

        # If such URL redirects to another URL, set such string to the last URL
        # such URL transitively redirects to; else, leave such string unaltered.
        :set_string_to_string_line_last_matching_pcre_group_if_found\
            "${url_name__cu}" "${page_headers__cu}" ''^Location: ([^ ]++) '' or
            ignore_failure' <<'/---'
Canonicalize the value of the passed string variable as a URL. If such URL
points to an HTTP redirection rather than a page, such canonicalization
attempts to iteratively resolve such redirection until the resulting URL either
resolves to a page or exceeds a sensible timeout for doing so. In the latter
case, an exception is thrown: e.g.,

.:canonicalize_url()
==========================================
[source]
------------------------------------------
# If http://dieoff.com redirects to http://holoceneextinction.com
# redirects to http://raiazome.com, then...
>>> :str url='http://dieoff.com'
>>> :canonicalize_url url
>>> :str.output "${url}"
http://raiazome.com
------------------------------------------
==========================================
/---

# --------------------( WASTELAND                          )--------------------
#FUXME: This is also implementable via 'curl --head --show-error --silent "${url}"'. Do so.
#FUXME: Documentation could certainly use sprucing up. Add a proper example.

        #     :Str.set\
        #         "${url_name__cu}" "${url_resolved__cu}"
        # # Else, leave such URL unmodified.
        # }'

# if { is_pathable curl } {
#     function canonicalize_url:() {
#         # Validate sanity.
#         die_unless_arg 'Expected one URL.'
#         string url="${1}"
# 
#         # Canonicalize such URL. Happily, this is quite simple.
#         command curl\
#             --head\
#             --location\
#             --show-error\
#             --silent\
#             --output /dev/null\
#             --write-out '%{url_effective}'\
#             "${url}"
#     }
# # If "wget" is installed, fallback to such command.
# } elif { is_pathable wget } {
#     function canonicalize_url:() {
#         # Validate sanity.
#         die_unless_arg 'Expected one URL.'
#         string url="${1}" page_headers
# 
#         # Capture all pager headers sent by the webservers serving such URL and
#         # all . Dismantled, this is:
#         #
#         # * "--max-redirect=...", resolving nearly arbitrarily many redirections
#         #   (rather than failing at the 21st redirection).
#         # * "--output-document=-", outputting to standard output (to be captured
#         #   by such process substitution).
#         # * "--spider", retrieving only page headers (rather than page content).
#         # * "--tries=1", failing if the first attempt fails (rather than
#         #   retrying 20 times).
#         # * ":to_stdout", redirecting standard error to output. Nonsensically,
#         #   "wget" always prints such headers to standard error -- and provides
#         #   no options for modifying such behavior. *sigh*
#         page_headers="$({
#             command wget\
#                 --max-redirect=999999\
#                 --output-document=-\
#                 --spider\
#                 --tries=1\
#                 "${url}"
#         } :to_stdout)"
# 
#         #FIXME: Actually implement
#         #:set_string_to_string_last_matching_pcre_multiline_group_if_found:().
#         #Wonder how we'll do that, incidentally? Two methods, as we see it:
#         #
#         #1. Split such string on newlines into a list and search such list from
#         #   the *END* for the first matching line.
#         #2. Iteratively apply such PCRE to such list from the *BEGINNING*. The
#         #   last captured group if any is the desired group.
#         #
#         #O.K.; clearly, on arbitrarily large strings, the former solution is
#         #(probably) the most efficient. Although the storage costs are
#         #significantly higher, we don't expect to ever apply such function to
#         #grossly corpulescent strings. In such case, iteratively searching from
#         #the beginning of a large string merely to ignore most such matches
#         #seems particularly inefficient. Besides, the former solution is
#         #modestly clever... modestly.
# 
#         # If such URL redirects to another URL, get the last URL such URL
#         # transitively redirects to.
#         if { :set_string_to_string_last_matching_pcre_multiline_group_if_found:\
#             url_resolved "${page_headers}" '^Location: ([^ ]++) ' } {
#             :str.output "${url_resolved}"
#         # Else, get such URL as is.
#         } else {
#             :str.output "${url}"
#         }
#     }
# }

#FUXME: After doing so, rename to canonicalize_url() and shift to a more
#general-purpose parcel -- say, "always/net/url".

        #FUXME: We need to match in a line-oriented manner and hence use PCREs,
        #instead. In particular, match as follows:
        #   if { :set_string_to_string_text_matching_glob_group_if_found:\
        #       url_resolved "${page_headers}" '^Location: ([^ ]++) ' } {
        #FUXME: Note that "curl" may or may not require the slightly different PCRE:
        #'^Location: (.*)$', as reported by stackexchange. Honestly, one or the
        #other is probably wrong, so verify! Sample URL for verification purposes:
        #
        #* http://raspberrypi.stackexchange.com/a/1521/86 should resolve to
        #  http://raspberrypi.stackexchange.com/questions/1508/how-do-i-access-the-distributions-name-on-the-command-line/1521#1521.

# If "wget" is installed, prefer such command. While "curl" resolves only the
# first such redirection, "wget" recursively resolves all possible redirections.

#Handle `wget`, a CLI utility for mirroring remote websites to local directories.
#. For
#example, if http://dieoff.com redirected to http://holoceneextinction.com
#redirected to http://raiazome.com, then this function prints the final URL,
#"http://raiazome.com".

#Transitively resolve the passed URL of all intermediary redirects

    # Avoid running under "try" as that currently fails to interpret shell
    # sensitive characters properly (e.g., "&"), and redirect stderr to stdout
    #FUXME: This is broken. What did we replace match_multiline_first_group()
    #with, again? Given the simplicity of the PCRE, shift to using a glob instead.
